You
22:55:14

Before we talk about the actual algorithm, let's understand what is the problem. The problem is that after the SFD stage, we get this LMM then this LMM is good at answering questions. Meaning that if you ask a question, like, if I want to learn ML, what should I do? It's going to output something that basically answers your question. For example, it would say take handwritten scores on Coursera. This response is contextually relevant. It's grammatically correct, and it's answering my question. But now let's compare this with a different output. For example, something like a solid foundation by nonlinear algebra, probability, and statistics, then take a beginner friendly ML course like hand rings on Coursera, practice by building a small projects and exploring real datasets. Now both of these answers are quite correct and contextually relevant, but this answer is clearly much better than this answer. It is more detailed. It's also more helpful, and it's, uh, giving me some starting points. The focus of reinforcement learning to go from this kind of output to more detailed, accurate, correct, safe safer outputs. And that's what you want to happen in reinforcement learning stage. I have another example here to better understand. You have a prompt like, what are some objective ways to reduce my stress? The LLM may have different options to respond to. For example, this one can be go by diving for an in rush. Second response is exercise regularly and maintain a healthy diet. Third response is shame on you. Try meditation. And four fourth response can be ignore your problems and hope they go there. Now, again, all these are positive from the SFT output because actually answering questions, and they're not too off in terms of the, uh, the context. For example, um, when it says ignore your problems and hope they go there, it's actually answering to this question. It's not answering to a different question. But the the response is really not helpful. Because by ignoring problems, it's not going to help you. Response three is is a mix. Giving good advice like trying education, but it's not, uh, polite. It's it's not a safe response. And response one can be, uh, not accurate, but still it's responding to the prompt, and this one is the best. So the goal of reinforcement learning is to somehow adapt the SFT model and get a final model that is more likely to produce responses like this. And less responses like this three. So now let's just start about, uh, learning the technicals. So reinforcement learning goal is to generate responses that are preferred by humans. So which responses are preferred by humans? Typically, the ones that are correct, accurate, say, self food, politeness, and so on. So after we complete the RL stage, we go from this SIFT model to this final one. And this final model is basically what eventually we would use and deploy as a chatbot. But how do we do that? How does this reinforcement learning happens? It's basically the training algorithm at a very high level just to get an intuition. It's basically a practicing algorithm. We allow the SFT model to practice. So what practice means is that we give it some inputs, and it ask the SFT model to produce different, um, responses. So each of these arrows here is basically showing one response to this prompt, which is two plus two via a safety model. So here we have one, two, three, four, five, six, seven responses. And then based on these responses, we say, hey. This response and this response was better, and this, uh, five responses that are in red are not good. So then the training algorithm goes back to SRT model, updates its parameters. So that next time, can we pass a prompt like this? It is it would be be more likely to produce responses that are green that are more similar to this green responses. So that's the high level idea. The model practices. It generates responses for the same prompt. And then we somehow, um, let the provide be shared with the model that these responses are better, which responses are worse. And then the model tries to reinforce its its basically reinforces such that, uh, it generates more responses similar similar to the ones that we like. So how does this happen? Basically, the main question how can we really once the model generates these, uh, variations of responses, how do we determine which responses are better than others? To answer this question, there are you can split all the tasks in the board into two, uh, groups. Verifiable and unverifiable. Unverifiable sorry. Verify the tasks are the ones that, uh, they can easily verify the response if it's correct or not. For example, if you think of math problems and then, um, we have something like this, what is two plus two, it's a math problem. We know that the answer is four. Answer four is correct. And then we can just look at all these responses and look at their final answer at the very end and see which of those are four. If they are four, they are green. They're they're good. We want more answers similar to those. And if they are anything but not four, they are wrong. It doesn't matter if they're, um, logic or um, you know, the reasoning is correct. It's just we don't like it. Coding is also another example of verifiable because we can easily verify code if it's we can run it or if it, uh, produces output that we expect or not. But there are also unverifiable For example, if you think of writing or brainstorming, it's not really easy to identify or say if output is, um, correct or not. It's very subjective, and, um, it's also relative. So, uh, for example, for business, I mean, if the input is, you know, help me choose a good name for my start up, might be various responses, and then some of them are better than others. Uh, but there's there's no really easy way to define if they are

You
22:55:24

first start with verifiable. We understand how, um, training works in verifiable problems, and then you switch to unverifiable. Problems.

You
22:57:42

Again, verifiable is easier because we can have some logic here, some software, some Python code. To look at the responses, and then check, basically, check if their final answer is is similar to what we expected or not. So, basically, all we need is a dataset. And for, um, things like what is two plus two? If you know the answer is four, can just let the SRT model generate various responses, apply this code or, um, logic to all these responses, and then automatically label them as correct versus incorrect. And then once we have this, the second step is now the actual training port, updating the model. So during the updating, we use some algorithms, some reinforcement learning algorithm, their various algorithms, um, like PTO, uh, more recent GRP, and so on. And then this algorithm is basically given the prompt as well as the which one the labels, which ones are correct, which ones are not. And then the PPO is responsible for updating the SST model parameter so that it reinforcement reinforces correct answers. So there are lots of wonderful resources for learning in detail about, um, parallel and PPU and how they differ. But at the high level, all they do is they're responsible updating the parameters of the model, in a way that reinforces the different answers. And then after we update the model, again, this is the final model. It's it's already model that is reinforced to produce, uh, card cancers. So when you ask like, what is two plus two? It's more likely that generates something that leads to four. So this is verifiable task, and, uh, at the core of it is this PPU algorithm. And then the verification here happens automatically and easily because the response the the task is verified. Now let's switch to unverifiable tasks. So now in unverifiable tasks, the challenge is we no longer can automatically label these responses. We really don't know which ones are better than others. For a prompt like how should I learn ML, it's really not easy to say, um, if they're very close, it's not really easy to say which ones are preferred. So because of that, we need a way to first score these responses. We need to score each response. And then after we have those scores, then we can follow the same process as the verified We can just train a machine learning model and basically use reinforcement learning to continue the SFG model based on those inputs. So now the next thing that we would learn is how are we going to score these responses. And for that, we have to train a separate one. So, um, and this is called RLHF, reinforcement learning from human feedback. Feedback. Basically, we get we use human feedback to train a model that can score these responses. And then once we have that separate model, we'll use it to automatically score responses, and then we can use reinforcement learning to update SIFT model. So basically, our two steps in part of the lecture. Training every one of them using human feedback.

ByteByte
22:57:43

Hey. We talked about the SFDSH. And in this lecture, we'll switch to the second step of post training, which is reinforcement learning. Before we talk about the actual algorithm, let's understand what is the problem. The problem is that after the SFS stage, we get this LMM, and then this LMM is good answering questions. Meaning that if you ask a question like, I want to learn ML, what should I do? It's going to output something that basically answers your question. For example, it would say take handrails course on Courser. Now this response is contextually relevant, is grammatically correct, and is answering my question. Now let's compare this with a different output. For example, something like a start with a solid foundation by nonlinear algebra probability and statistics, then take a beginner friendly ML like Andrei's on Coursera, practice by building a small project and explore exploring real datasets. Now both of these answers are correct correct and contextually relevant, but this answer is clearly much much better than this answer. It is more detailed. It's also more helpful. And it's, um, giving you, uh, some starting points. The of reinforcement learning to go from this kind of output to more detailed, accurate, correct safe safer outputs. And that's what we want to happen in reinforcement learning stage. I have another example here to better understand. Then we have a prompt like, what are some effective ways to reduce my stress? The LLM may have different options to respond to it. For example, response one go sky diving for an adrenaline rush. Second response is exercise regularly and maintain a healthy diet. Third response is shame on you. Try meditation. And four first response can be ignore your problems and hope they go away. Now, again, all these are possible from the output because you're actually answering questions and they're not too off in terms of the, uh, the context. For example, um, when it says ignore your problems and hope it go away, it's actually answering to these questions. It's not answering to a different question. But the the response is really not helpful because by ignoring the problems, it's not going to help you. Response three is is a mix. It's giving good advice, like trying medication, but it's not, uh, polite. It's a, uh, it's not a safe response. And response one can be, um, not accurate, but still it's responding to the prompt, and this one is the best one. So the goal of reinforcement learning is to somehow adapt the SIFT model and get the final model that is more likely to produce responses like this. And less responses like this three. So now let's just start about, uh, learning the technicals. So reinforcement learning goal is to generate responses that are preferred by humans. So which responses are preferred by humans? Typically, the ones that are correct, accurate, say, fail, full, and so on. So after we complete the RNN stage, we go from this SFE model to this final model. And this final model is basically what eventually we would use and deploy as a chatbot. But how do we do that? How does this reinforcement learning happens? The training algorithm at a very high level, just to get an intuition, is basically a practicing algorithm. We allow the SFE model to practice. So what practice means is that we give it some inputs. And we ask the SFT model to produce different, um, responses. So each of these arrows here is basically showing one response to this prompt, what used to be plus two by a SIFT model. So here we have one, two, three, four, five, six, seven responses. And then based on these responses, we say, hey. This response and this response was better. And these, uh, five responses that are in red are not good. So then the training algorithm goes back to SFML that updates its parameters. So that next time, when we pass a prompt like this, it is it would be be more likely to produce responses that are green that are more similar to these green responses. So that's the high level idea. The model practices. It generates responses for the same prompt, and then we somehow, um, let the provide we share with the model that which responses are the term, which responses are worse. And then the model tries to reinforce it itself. Basically reinforces such that, um, it generates more responses similar similar to the ones that we like. So how does this happen? Basically, the main question is how can we really the model generates these, uh, variations of responses, how do we determine which responses are better than others? To answer this question, there we can split all the tasks in the world into two, um, groups, verifiable and unverifiable. Unverifiable sorry. Verifiable tasks are the ones that uh, we can easily verify the response if it's correct or not. For example, if you think of math problems and when, uh, we have something like this, one is two plus two, it's a math problem. We know that the answer is four. Answer four is correct. And then we can just look at all these responses, and look at their final answer at the very end and see which of those are four. If they are four, they are green. They are they are good. We want more answers similar to those. And if they are anything but not four, they are wrong. It doesn't matter if they're, uh, logic or you know, the reasoning is correct. It's just we don't like it. And coding is also another example of verifiable because we can easily verify code if it's we can't run it or if it, uh, produces the output that we expect or not. But there are also unverifiable tasks. Example, if you think of writing or brainstorming, it's not really easy to identify or say if output is, um, correct or not. It's very subjective. And, um, it's also relative. So, um, for example, for brainstorming, if the input is, you know, help me my start up, there might be various responses, and then some of them are better than others. Um, but there's there's no really easy way to verify if they are correct or not. And there are it was on VFiber. Tough problems are handled differently in the reinforcement learning stage. We first start with verifiable. We understand how, um, training works in verifiable problems, and then use issue unverifiable problems. So, again, verifiable is easier because we can have some logic here. Some software, some Python code to look at the responses. And then check, basically, check if their final answer is is similar to what we expected or not. So, basically, all we need is a dataset and for, um, things like what is two plus two. If you know the answer is four, we can just select the effective model generated by these responses. Apply this code or, um, logic to all these responses, and then automatically label them as correct versus incorrect. And then once you have this, the second step is now that actual training or updating the model. So given the updating, we some algorithms, some reinforcement learning algorithm, there are various algorithms, um, like PPO more recent GRPO, and so on. And then this paralal algorithm is basically given the prompt as well as the one the latest, which ones are correct, which ones are not. And that is responsible for updating the SFT model parameters. So that it reinforce reinforces correct answers. There are lots of wonderful resources for learning in detail about, um, r l and p d o and how they differ. But at a high level, all they do is they're responsible for updating the parameters of the model. In a way, that reinforces the better answers. And then after we update the model, again, this is the final model. It's it's already a model that is reinforced to produce, uh, correct answers. When you ask a question like, what is two plus two? It's more likely that generates something that leads to four. So this is verifiable task, and, uh, at the core of it is this PBO algorithm. And then the verification here happens automatically and easily because the respond the the task is verifiable. Now let's switch to unverifiable tasks. So now in unverifiable tasks, the challenge is we no longer can't automatically label these responses. We really don't know uh, which ones are better and others. For a prompt like how should I learn ML? It's really not easy to say, um, if they're very close, it's not really easy to say which ones are preferred. So because of that, we need a way to first score these responses. We need to score each response And then after we have all the scores, then you can follow the same verifier. We can just train a machine learning model and basically use reinforcement learning to continue the SFU model based on those scores. So now the next thing that we will learn is how are we going to score these responses. And for that, we have to train a separate model. So, um, and this is called RLHF, reinforcement learning from human feedback. Basically, we get to use human feedback to train a model that can score these responses. Once we have that separate model, we'll use it to automatically, uh, score responses. And then we can use reinforcement learning to update SFT model. So basically, our two steps in parallel chip. Training your reward model using human feedbacks and then optimizing the model

You
22:57:53

And then optimizing the model, the the 50 model with the reinforcement learning algorithm using this train model. So this is a step one, training every part one. The data preparation part, is basically the initially collect some, um, prompts. These are some initial prompts

You
22:59:53

things like what is the capital of France, name of famous physicist, and so on. Use the SFT model to generate multiple responses for each prompt. For example, for the first prompt, which is where is the what is the capital of France. We have response one can be Paris. Response two, 10 d, it's in Europe, and response three is it's, uh, Now then we need actual humans. We need to hire annotators to rank these responses. So here, basically, actual humans could have all these responses and rate rank them. Example, in the first example, Paris is obviously better than it's in Europe because it's it's more accurate. And then it's in Europe is better than Eisenhower Tower. So this is the ranking. Then after we have all these rankings from their prompts, you can finally generate our training bid. And this is how training data will look. What it would have each example would have, um, a prompt is the frontier, which is the capital of France. And then you'll have winning response and losing response. For example, based on these rankings, we can say, Paris is better than it's in Europe. So one example, we can formulate by, um, something like this. Freedom response is Paris. Using responses that, uh, based in Europe. And then we can also construct more examples based on, uh, these rankings. We can go and say, hey. What is two plus two? For pending response, Math is hard. Losing response. Just because from the annotators, we know that this response is ranked lower than this response. This is how we get the training data to train a reward model. Again, in the reward modeling stage, only purpose is to train a model that can reward answers, that can basically score answers. After we have this data, the second part is training the reward model, and this is a very typical problem in machine learning. Basically, we have a model that takes prompt and response and score them. So this is the input and output of the reward model. We have one prompt and one response, and the mother outputs a score. Now in order to train this using these training data examples from meaning with losing response, this is how we would train it. We pass the prompt. We pass the prompt. What is two plus two and getting response to this model to get some score. Also pass the prompt and the losing response, we get another score. And then the we apply some, um, common losses. Such as margin ranking loss. And what this loss does is basically tries to maximize the difference between these scores. So what it does is it tries to increase the score of feeding response, and then it tries to minimize or lower the score of losing response so that their difference be maximized. This is the the objective.

You
23:01:36

Um, so that's it. It it simply uses this, uh, the loss function like margin ranking was for this purpose. And then after we, uh, train this model on this on our training data, on this data, eventually, we would have a model that maybe passed a prompt time response. It would predict this score that and this score would align with humans, uh, preferences or feedbacks. Which we obtained here. So now this reward model will be a proxy for humans. Instead of humans going and actually ranking, um, these responses, the reward model would now score. So we talked about the first step of our elective, training a reward model. The second step is almost identical with what we saw in the verified and task here. It's it's very similar to this. The only difference is that now we don't have an automatically, uh, verifiable component, which is, um, easy to implement. Instead of this, we have now a required model. So let's go to the optimizing the model with wireless stage. Again, very same process. Some reinforcement learning algorithm to update the safety model's parameters using this course. And these scores are obtained by the reward model. So for a prompt like how should I learn ML, it pass to the safety model, they get multiple responses. Reward model scores each response, and these scores are the aim the hope is that these scores would align with humans would have scored, um, because the reward model is trained on human preferences. And then the PTO algorithm would, um, take these scores and the prompt, and, uh, it basically updates the parameters of the system model. So it if so next time, it generates responses with higher score. So it reinforces reinforces responses like this and this with higher score. So that's the second stage. And then after we apply this reinforcement learning, we would end up with a model that produces, um, answers that are more aligned with humans. And that means that they are more correct, they are more detailed, and useful, and safe. For a question that, uh, like, if I want to learn machine learning, what should I do? This is an example. They would have put something like a solid foundation in Python and so on. So this is post training, and, um, we talked about both the stages of post training. We talked about let me go here.

ByteByte
23:01:57

the SSE model with the reinforcement learning algorithm using this train model. So this is step one, training a robot model. The data preparation part is basically we initially collect some, um, prompts. These are some initial prompts like things like what is capital France, name of famous physicist, and so on. Use the SFT model to generate multiple responses. For each prompt. For example, for the first one, which is where is the is the capital of France. We have response one can be Paris. Response two can be it's in Europe, and response three is some Python tower. Now then we need actual humans. We need to hire annotators. To rank his responses. So here, basically, actual humans look at all these responses and rate rank them. For example, in the first example, Paris is obviously better than it's in Europe because it's it's more accurate. And then it's in Europe is better than Eiffel Tower. So this is the ranking. And then after we have all these rankings from their prompts, you'll can finally generate our training data. And this is how training data will be look like. What it would have each example would have, uh, a prompt, which is the prompt here. What is the capital of France? And then we'll have winning response and losing response. For example, based on these rankings, we can say Paris is better than it's in Europe. So one example, we can form it by um, something like this winning response is Paris. Losing response is that, um, it's in Europe. And then we can also construct more examples based on, uh, these rankings. We can go and say, hey. What is the two for winning response, Matt is hard losing response? Just because from the annotator, so you know that this response is ranked lower than this response. So this is how we get the training data to train a reward model. Again, in the reward modeling stage, the only purpose is to train a model that can reward answers. Like the score answers. So after we have this data, the second part is train the reward And this is a very, uh, typical problem in machine learning. Basically, we have a model that takes prompt and response and score them. So this is the input and output of the reward model. We have one prompt and one response, and the model outputs a score. Now in order to train this using these training data examples, prompt meaning re losing response, this is how we would train it. We pass the prompt We pass the prompt like what is two plus two, and getting response to this model, we get some score. We also has the prompt and the losing response. We get another score. And then the we apply some, um, common losses. As margin ranking loss. And what this loss does is basically tries to maximize the difference between these scores. So what it does is it tries to increase the, uh, score of winning response, and then it tries to minimize or lower the score of losing response so that their difference be maximized. Um, Basically, this is the the objective. Um, so that's it. It it simply uses this, uh, plus function like merging ranking plus. For this purpose. And then after we, uh, train this model on this on our training data, on this data, eventually, we would have a model that's when we pass a prompt response. It would predict a score that and this score would align with humans, uh, preferences or feedback. Which we obtained here. So now this reward model will be a proxy for humans. Instead of humans going and actually ranking, um, these responses, the reward model would now score. So we talked about the first step of our, uh, training your reward model. The second step is almost identical with what we signed in the verifiable tasks. Here. It's it's very similar to this. The only difference is that now don't have an automatically, uh, verify the component, which is, um, easy to implement. Instead of this, we have now a reward model. So let's go to the optimizing the model, the RL stage. Again, very same process. We use some reinforcement learning algorithm to update the safety model's parameters. Using this course. And these course are obtained by the reward model. So for a prompt like how should I learn ML, we have safety model to get multiple responses. The reward model scores each response, and these scores are the aim the hope is that these scores would align with what humans would have a score. Um, because the reward model is trained on human references. And then the p d algorithm would take these scores and the prompt, and, uh, it basically updates their parameters on a safety model. So it so next time, it generates responses as the buyer score. So it reinforces reinforces responses like this and this with higher score. So that's the second stage. And then after we apply this reinforcement learning, we will end up with a model that produces, um, answers that are more aligned with humans. And that means that they're more correct. They're more detailed and useful and safe. So for a question uh, like, I want to learn machine learning. What should I do? We saw this example. It would ask for something like a solid solid foundation in Python and so on. So this is post training and, um, we talk about both the stages of, uh, post training. We talked about let me go here. So we talked about both SFDSH. Is responsible for just adapting the the format of the output to answer questions instead of continuing it. And then we apply reinforcement learning to the model practices to to produce a more accurate correct responses. And then during the reinforcement learning, there are two types of tasks, verifiable and unverifiable. Verifiable tasks are the ones that we can easily learn them.

You
23:01:57

So we talked about those SFC stage. She's responsible for just adapting the format of the output to answer questions instead of continuing. And then we apply reinforcement learning so the model practice to to produce a more accurate correct responses. Then during the reinforcement learning, there are two types of tasks, verifiable and unverifiable. Verifiable tasks are the ones that we can easily learn there.

ByteByte
23:02:39

That can easily verify them, such as math problems and coding. And unverifiable are the ones that are difficult to unver to verify. For verifiable, we can just use simply have a component rate is the score, these responses, and then we use reinforcement learning to update parameters. For unverifiable, we would, uh, train a new model called reward modeling to score these responses automatically. And then once we have that, we can then apply, um, reinforcement learning to from the SFF model to the final model. So that's it. That's all I wanted to cover for post training. And one last thing be before we wrap up post training is showing this as a real example. In the last lecture, we saw that how a base model would simply continue Let me rerun again to see if it's going to work now. I want to learn ML. What should I do?

You
23:02:40

That we can easily verify them, such as math, coding, and unverifiable are the ones are difficult to to verify. For verifiable, we can just use simply have a component to rate his score, his responses, and then we use reinforcement learning to update parameters. For unverifiable, we would, uh, train a new model called reward modeling to score these responses automatically. And then once we have that, we can then apply, um, reinforcement learning to from the SIFT model to the final model. So that's it. That's all I wanted to cover for post training. And one last thing before before we wrap up post learning is showing just a real example. The last lecture, we saw that how a base model would simply continue. Uh, I didn't rerun it in to see if going to work now. Um, I want to learn ML What should I do?

ByteByte
23:03:48

Alright. So again, for the smaller, just asking more questions. What are their resources I should follow? And so on. Now what I'm going to do is I'm going to switch to a model that was is not a base model. It's a post stream model. So here we can choose, uh, one of these models. I'll see if I can find the laminate 3.1. Alright. I'll choose laminate three point one hundred five billing, so it's the exact same model, but not the base model, the post train model. And I'll ask the exact same question. Here. So the UI is a little different. I'll ask my question here and then send it. Then I'll keep max tokens to five dev so it produces more tokens. And then let me send this. Alright. Now this is the response, and it says learning machine learning design exciting journey. Here's a step I'd like to help you get started. Step one. And, you know, uh, math programming is step two. Also takes care of the formatting, like, I'm showing this, um, like reset item itemized, uh, sentences, and some of the words are bold. So basically, it's just answering my question in a very detailed and helpful way. And this is what happens when we apply post training to a base model. It becomes a really useful model. So that's it now. Now I guess I can wrap up the post training.

ByteByte
23:05:20

Hey. Let's just start by reviewing a summary of element training stages. And then we focus on how to evaluate the trained LMS. So these four, um, stages are basically summarizing all the everything we discussed over the last couple of, uh, lectures. There is a pre training stage. It's trained on internal data, typically trillions of trillions of tokens. They're quality large quantity. And then they require a lot of compute. They have they need thousands of GPUs. And a month of training. And then it's, uh, the language modeling, uh, algorithm and objective is just next token ready So we train them to predict the next token. And then the outcome is the base model. And these are some examples like lemmat base and g p k two and three. And then the second stage is supervised fine tuning, which is part of the post training. In this stage, we we build and curate our demonstration data on pairs of prompt responses. They typically range in the in in the in the range of 10 to tens or hundreds of thousands of pairs. And then they are low quantity low quantity, high quality. And DSS still requires only, um, hundreds of GPUs or even less and days of training. And the, uh, ML objective is the same. We still have the same model, same training algorithm to predict the next step. And then the model is, uh, initialized from the base model, meaning that we continue training the base model, and then the outcome becomes SFE model. And then the next part is creating the reward model. If the tasks are not verifiable. And then reward modeling, we need comparison data. So we hire annotators to, um, to rank possible responses generated by this SF model. And then this is data also requires of GPUs or less and days of training. And then here, the goal is to train a reward model that can predict the score. So output is just a score. This is the general objective. And then we have the reward model, we'll start the final reinforcement learning stage. It's initialized around this SFT model

ByteByte
23:09:30

and it's relying on reward model to score reports. And then for reinforcement learning, we start with tens or thousands of hundreds of thousands of prompts. We use solid number of GPUs and days of training. And then we, um, use a reinforcement learning algorithm to, um, so the model practice to generate responses that are reward scored higher by the reward model. And then the outcome of this stage is the final, uh, model, which is typically used to deploy and serve, uh, users as a chatbot. So this is the summary of the training stages. And after we, uh, complete all these stages, we will end up with the final model. And there are different companies having different models. And if you see here, again, there are lots of, um, models, like, the ones from DIPC and Quinn and they're basically a lot, and the lama, and there are a lot more examples, uh, small, large, and so on. So, um, the question is how should we evaluate them? How can we determine which models are better and which are worse? And this is what we are going to talk about in LMM evaluation. There are two parts in the evaluation. The first part is optimization and then the second part is online evaluation. In offline evaluation, we evaluate the model in an offline environment on some evaluation data. And then we measure their performance, and they can compare different elements. In online evaluation, we have elements already deployed in a production environment, and then we use different phase to compare them. While they are in production and selling users. So we'll just start with offline evaluation under are different base for offline evaluation. We'll start with traditional, uh, way of evaluating and then pass specific and human evaluation. And then for online evaluation, we'll talk about human feedback and crowdsourcing platforms. So the first one is traditional evaluation. What traditional means here is we use some traditional metrics. Um, to evaluate elements. And what common metrics to evaluate elements, and in general, a text generation model, is to, uh, preprocessing. And it's basically a metric that measure measures how accurately the model predicts an exact sequence of tokens present in the text data often in the evaluation data. So there is a formula, and then, um, you can look on Wikipedia to see the details of the formula. But at the very high level and intuitively, what this formula is doing is we'll have some, um, sequence of data. Let's say, evaluation data. For example, how are you doing? Then we use the model to see how to produce this exact how are you doing sequence. So we start with how. We get the probability. Then we pass how are. We get the probability of you. And then, um, we have basically, we we practice like, we have to multiply them or we can basically take the log of these values and then sum it. And then this would give us a number representing how likely the model is to produce this exact sequence of how are you doing. And this way, basically, we measure how good the model remembers evaluation or can exactly reproduce evaluation data. And in this way, we can compare different models and the model for the LMM. That is better at producing or reproducing the evaluation data is better. However, this method, this traditional way of evaluating elements is no longer, um, you too helpful or meaningful because this metric is not really, um, telling us much. What it says is that the model can reproduce a certain sequence of tokens, but that's what humans want really in practice. So what they what we want is a model that can really answer our question or its correct or its useful. So that brings us to the second way of evaluating evidence, which is task specific evaluations. So the purpose of this kind of evaluation is to assess the performance of element across diverse tasks. That we really care about. For example, mathematics, code generation, common support knowledge, and so on. So these are some of the key areas that we want to evaluate the model and see how the model performs. So we'll just briefly review some of these, um, and they are great benchmarks, meaning that they're benchmarks are basically just a a a dataset of different examples in that particular domain. And then we can use those and run them on uh, as they pumps the LLM and see if the LLM response is, uh, similar to the current answer. For example, common sense is just common sense questions if the model ask answer, uh, can answer those questions or not. Uh, this is one real example. Um, the pro the trophy does fit in their brown suitcase because it's too large, but it's too large. The trophy a, b, the suitcase. This is the prompt. This goes into the element. And then the correct answer from this dataset, from this example, is the trophy. So we pass this to the element and we see if the element really generates the trophy or it generates the swithes or or anything else, like any random outputs. And in this way, we can compare different elements and see how well they are doing in, uh, let's say, comments that we see. And there are a bunch of benchmarks for each of these domains. We have, uh, we set three benchmarks here. Then for word knowledge, for example, who wrote this, who wrote that, and this is the current and these are the benchmarks. And we can use telemeds and evaluate them on their capability. And then we have mathematical reasoning. For example, if a train traveled 60 miles or hour for three hours, how far does it travel? And then we see if the model can really answer it correctly or not. And then we have also code generation, something like write a Python function to, um, if a number is prime. And then this is some possible card cancer. And then model auto generates an answer, and then we can check and see if, uh, that other

ByteByte
23:10:05

is very common because we really care about, um, these numbers, and we want to know how good different models are compared in different domains. So this is a common way of, uh, evaluating candidates. And then the last part, which we have here is human evaluation. Basically, in this side of evaluation, we just ask higher experts and, um, uh, expert humans And then we ask them to ask challenging questions from the element and see if their answers are, um, correct or not. So this time we're treating. In in in one sense, it's good because humans are, uh, more capable of evaluating and, um, understanding the responses and verifying them. But also, it can be tricky because humans can be biased and a lot of times depending on who is evaluating and evaluating in which domain.

You
23:10:41

Alright. So, again, the base model, just keep asking more questions. What are their resources I should follow? And so on. Now what I'm going to do is I'm going to switch to a model that was is not a base model. It's a posturing model. So here we can choose, uh, one of these models. I'll see if I can find the lambda 3.1. Correct. I choose lama three point one four hundred five billion. So it's the exact same model, but not the base model, the post train model. And I'll ask the exact same question here. So the UI is literally different. I'll ask my question here and then send it. And then I'll keep max tokens to five left, so it produces more tokens. And then let me send this. Alright. Now this is the response. And it says learning machine learning design exciting journey. Here is a step by step guide to help you get started, step one, and, you know, uh, math programming is step two. It also takes care of the formatting, like, I'm showing this like, list that item itemized, uh, sentences, and some of the words are bold. So, basically, it's just answering my question in a very detailed and helpful way. And this is what happens when we apply post training to a base model. It becomes a a really useful model. So that's it now. Now I guess I can wrap up the post training. Hey. Let's just start by reviewing a summary of relevant training messages. And then we focus on how to evaluate the trained element. So these four, um, stages are basically summarizing all the everything we discussed over the last couple of lectures. There is a pretraining stage. It's trained on Internet data, typically trillions of trillions of tokens. They're low quality, large quantity. And then they require a lot of compute. They have been need thousands of GPUs and month of training. And then it's, uh, the language modeling, uh, algorithm and objective is just next prediction. So we train them to predict the next token. And then the outcome is the base model. And we saw some examples like gamma base and g p t two and three. And then the second stage is supervised fine tuning, which is part of the post training. In this stage, we'd be build and curate our demonstration data on pairs of prompt responses. They typically range in the in in in the range of 10 to tens or hundreds of thousands of pairs. And then there are low quantity low quantity, high quality. And this is it requires only, um, hundreds of GPUs or even less than days of training. And the, uh, ML objective is the same. We still have the same model, same training algorithm to predict the next step. And then the model is, uh, initialized from the base model, meaning that we continue training the base model, and then the outcome becomes SIFT model. And then the next part is creating the reward model. If tasks are not verifiable. And then reward modeling we need comparison data, so we hire annotators to, um, to rank possible responses generated by this SFF model. And then this is stage also requires, um, hundreds of GPUs or less and days of training. And then here, the goal is to train a report model that and predict this code. So the output is just this code. This is the NL object. And then once you have the reward model, it'll start the final reinforcement learning stage. It's initialized from this SSE model here, and it relies on reward model to score rewards. And then for reinforcement learning, we start with tens or thousands of or hundreds of thousands of prompts. We use solid number of GPUs and days of training, and then we, uh, use a reinforcement learning algorithm to Um, so the model practices to generate responses. That's our reward score higher by the reward model. Then the outcome of this stage is the final, uh, model, which is typically used to deploy and serve users as a channel. So this is the summary I would say. Uh, it stages. And after we, uh, complete all these stages, we will end up with the final model. And there are different companies having different models. And if you see here again, there are lots of, um, models, like, the ones from Deep Sea and PoEEN and GRPCP a lot and LaMDA, and there are a lot more examples, um, small, large, and so on. So, um, the question is how should we evaluate? How can we determine which models are better and which are worse? So And this is what we're going to talk about in data and evaluation. So there are two parts in the evaluation. First part is offline evaluation, and then the second part is online evaluation. In offline evaluation, we evaluate the model in an offline environment on some evaluation day. And then we measure the performance, and then we compare different elements. All line evaluation, we have the elements already deployed in a production environment and then we use different ways to compare them while they are in production and serving users. We just start with offline evaluation, and there are different ways. For offline evaluation, we'll start with traditional way of evaluating and then pass the specific human evaluation. And then for online evaluation, we'll talk about human feedback and crowd sourcing platforms. So the first one is traditional evaluation. What traditional means here is we use some traditional metrics. Um, to evaluate elements. And one common metrics to evaluate elements and in general, a text generation is to, uh, prepare exit. And it's basically a metric measures how accurately the model predicts an exact sequence of tokens present in the text data. Of finding the evaluation data. So there is a formula, and then, um, you can look on Wikipedia to to see the details of the formula. But at the very high level and intuitively, what this formula is doing is you'll have some, um, sequence of data, your evaluation data, for example, how are you doing? And then we use the model to see how likely is it to produce this exact how are you doing sequence. So we started how, get the probability. Then we pass how are we get the probability of you. And then, um, we have basically the in practice, like, theoretically, we have to multiply them, or we can basically basically take a log of these values and then sum them. And then this would give us a number representing how likely the model is to produce this exact sequence of how far you're doing. This way, basically, we measure how good the model remembers evaluation date or can exactly reproduce evaluation And in this state, we can compare different models and the model or the element that is better at producing or reproducing the evaluation data is better. However, this method is traditional of evaluating elements is no longer, um, you you too helpful or meaningful. Because this metric is not really, um, telling us much. It says is that the model can reproduce a certain sequence of tokens, but that's not what humans want really. In fact, What we want is the model that can really answer our question or it's correct or it's used. So that brings us to the second way of evaluating evidence, which is task specific evaluations. So the purpose of this kind of evaluation is to assess the performance of element across layers as that we really care about. For example, mathematics, code generation, common sense reasoning, word knowledge, and so on. So these are some of the key areas that we want to evaluate the model and see how the model performs. So we'll just briefly review some of these, um, under our great benchmarks, meaning that you're benchmarks are basically just a a a dataset of certain examples in that particular domain. And then you can use those and run them on, um, they bounce the LLM and see if the LLM response is, uh, similar to the prior cancel. For example, common sense is just common sense questions if the model ask answer can answer those questions or not. Um, this is one real example. Um, the the trophy doesn't fit in their brown suitcases because it's too large, but it's too large. The trophy a, b, the suitcase. This is the prompt. This goes into the LNO. And then the correct answer from this dataset, from this example, is the trophy. So we pass this to the element, and we see if the element really, um, generates the trophy or it generates the submits or or anything else, like any random apples. And then this way, we can compare different elements and see how well they are doing in, uh, let's say, comments and season. And there are a bunch of benchmarks for each of these domains. We have, uh, we said three benchmarks here. And then for word knowledge, for example, who wrote this, who wrote that, and this is the correct answer, and these are the benchmarks And we can use algorithms and evaluate them under word knowledge, uh, capability. And then we have mathematical reasoning. For example, if a train travels 60 miles per hour for three hours, how far does it travel? And then we see if the model can really answer, uh, correctly or not. And then we have also quad generation, something like write a Python function to, um, check if a number is prime. This is some possible prior answer. And then the model also generates an answer, and then we can check and see if uh, the outputs are correct. And these are some benchmarks for for generation. This way of evaluating elements is very common because we really care about numbers, and we want to know how good different models are compared in different domains. So this is a common way of, uh, evaluating analytics. And then the last part, which we have here is human evaluation. Basically, in this type of evaluation, we just ask higher experts and, um, uh, expert humans. And then we ask them to ask challenging questions from the element and see if their answers are um, correct or not. So this can be tricky. In in in one sense, it's good because humans are uh, more capable of evaluating and, um, understanding the responses and verifying them. But also, it can be tricky because humans can be biased, and a lot of times, depending on who is evaluating, enable them in which domain. The responses for their evaluations might be slightly biased and also subjective. But this is also another way to evaluate elements, and then they switch to the online evaluation. In online evaluation, you have two ways of doing. One is human feedback, and another is crowd sourcing So human feedback is simply, um, just then the model in the UI, in, let's say, touch it with the UI in response. Asks the user to rate it with a thumbs up or thumbs down. And this way, the, um, the team behind Tagivity can get the feedback from real users whether they're happy with answers or not. And this would give them a signal whether how good the model is doing compared to previous models. Um, and this signal can also use for other purposes, like, for further fine tuning or doing reinforcement learning because these

ByteByte
23:12:34

Responses or their evaluations might be slightly biased and also subjective. But this is also another way to evaluate elements. And then we'll switch to the online evaluation. In online evaluation, we have two ways of doing it. One is human feedback and another is sourcing platforms. So human feedback is simply, um, just when the model in the UI, in, let's say, HIBB UI, it responds. It asks the user to rate it with a thumbs up or a thumbs down. And this way, the, um, the team in behind tagivity can get a feedback from real users whether they're happy with dancers or not. And this would give them a feedback on whether how good the model is doing compared to previous models. Um, and this signal can be also used for other purposes like for further fine tuning or doing reinforcement learning because the speed are coming from real humans, so they can be valuable. But they can also they serve as a evaluation, uh, purpose as well. And then the final part of other evaluation is crowdsourcing. So there are different websites that they use crowdsourcing to rank and rate elements. And one of them is LMRN. And it's initially developed by, um, graduate graduates. And if I search Google, it it says that Ele Mariner is a public web based platform that evaluates LLMs through anonymous grabs on viruses. Users interprompts for two anonymous models. Respond to and then both on the model that gave the better response. Image the model's identities are revealed. So basically, just ask, uh, real real humans to it shows them different responses from two different models, and then the the users can vote. And then it uses those voting to rank elements. And then this is their website. There are different tabs here. I select a text, and text means basically the model like, the models that are, um, ranked, um, for text generation capability. And this is the ranking as of today. For example, the the first ranking is Gemini 2.5 pro developed by Google. These are the number of votes, and this is the score so we can see the the gap between, uh, different models. And then this is the license. Some of these are open source. Some are closed. Um, the second rank is o three, and then we have Chats GT four o, 4.5, cloud model, cloud, uh, plus four. And so on. And this is DeepCreek, which has this MIT license, which basically is open source and you can use it. And it's also open beta so the beta are available to be used. You can see there are lots of different models and they rank, and it this this data keeps changing. And this company is keep releasing new models and just usually newer models are better, so they rank higher. And again, it keeps changing and changing. So I think it has, like, lots of elements now. 211 elements are, um, are, uh, included in this table, and then lemma 13,000,000,000 is 209. And these are relatively older models, and that's why they are, uh, lower on this list. So yeah. So that's it. That's the, uh, sourcing platform. So these are all different ways that they can evaluate elements, and they are using package to see a newly trained element is better than the previous ones or not.

ByteByte
23:17:26

Hello again. This is, uh, finally the last lecture of deep one of element foundations. So we learned everything about, uh, the foundations of Elements, how the data preparation looks like. How the model architecture is model training, both the stages between and post training, and also evaluation. One last topic that we want to cover in this lecture is overall system and how the what is the holistic view of, um, like, chatbot systems. So as you can see here, trained 11. This is basically the post train model that we finished training. Is here. And then, you know, it's just very small part of the entire system. Then there are a lot of other components that are necessary and used in practice. To power a service like Chasmiki. And this is simplified visualization. In practice there might be even more components. Handling different things. Um, but this would give a a good overview of what are different things that are important and needs to happen aside from the actual training data. So let's just walk over them one by one and understand what are their purposes. So when we have the text prompt coming from the user, the first component usually have is a some guard card rates, uh, for input card rates, and we can also call them safety filtering. So all these comp all this component does is it ensures that, um, our text font, the text prompt that was provided by the user is a safe prompt, meaning that it's the same safe request or safe For example, if the user requests something that is, uh, violent or could be harmful, this step, this component would identify it and just, um, not answering it. So basically, if the result of the safety filtering is that it's not safe, if it determines it's not safe, then it would use some other smaller models to generate some rejection response. This generated response would be shown to the user. And this is something like Sorry. We cannot assist you with that or or things like that that we've seen on various platforms. Now if the piece component identifies that the exponent is safe, to process an answer, the second part is usually a prompting answer. And what this component does, it's it's most of the times it's a machine learning based model. And what it does is it improves the text. A lot of times in the input text, there is ambiguity. There can be some missed spellings or primary issues and all those kind of, uh, different prob problems in the initial x one. So this component is responsible to fix all of them. It's responsible to make sure punctuation punctuations are well and it's they are there in the round. There is no misspellings. There is no typos. If there are typos, it's they get fixed. And, also, if there are ambiguity or the grammar is not corrected, they they all get fixed. And it can be a combination of heuristics as well as machine learning models. Then after the prompt in answer, we would have a prompt that actually is correct and, uh, is understandable. And it's not also weak. Now this enhanced prompt goes into this response generator This response generator is just the text generation decoding algorithm that is been true. And it, uh, interacts with the trained LML and generate the tokens one by one. And, um, we could use, uh, some, um, like, copy, sampling method. We talked about session management in a bit, but, um, having that aside, after text and response and a code process, there would be some response to the prompt. And then that response goes into this response safety evaluator. Or sometimes it's called output guardrail. So what it does, it, again, ensures that the generated output is also safe. Meaning that it's not really, um, sharing something dangerous or harmful or biased. And then if the result of this is that it's not safe, it would go again to the rejection response. Uh, generator and should generate a response that, hey. Somehow, we couldn't process your request or something like that. And if it determines that it's safe, it would just show the generated response to the user. So every time a user enters something like this, it would go through all these processes and components and then user would see the generated response. Now if you, um, if you see in chatbot services like Chargebee, when you ask questions, you get responses. You can follow-up questions. Um, and then the model would remember all the previous conversation and interactions. For example, you can ask something like, hey. Help me come up with a name for my start up. And then the model would share something, and then you would say, hey. These are not good. Uh, make it more or informal or something like that. And the model would remember everything from before, and then they adapt the model would adapt it response and show you another response. And this is handled by by a session management. And all it does is basically it just keeps track of all the, uh, previous messages that are interactive between the user and the LLM. And it would append those into, um, and it's all chat history. It will append those to a new prompt each time. So each time you enter a text prompt, and it goes here. Before that, it just happens here all the previous, um, you know, chats in that particular session. And all of this goes into the response generator. So when the response generator interacts with LLM using puppies and padding, the LLM takes into account the entire chat history when generating the new, uh, response. So this is how the follow ups and chat and, uh, follow-up questions are handled using a session management. So that's it. That's, um, about overall system design. Again, in practice, there are a lot of other, um, components for different reasons. And again, it's something there is not a single solution. Different companies use different components for different purposes. But this can be, uh, rough. You know, it gives an overall impression of how a system design of a chatbot can look like. Like. So with that, we can wrap up week one to, uh, for Eleventh We've learned everything we wanted to cover. And let's let me find this. So, basically, we got LLMs. We saw free training, data data collection, model architecture, training, text generation. They'll learn about post training, in particular SFT and reinforcement learning. As well as reinforcement learning from human feedback for unverifiable tasks. And then we discuss evaluation. And, um, so design. So with that, now we build a strong foundation, and now we can switch to more advanced use cases and extensions of LN that powers some of these applications.

You
23:17:27

coming from real humans, so they can be valuable. But they can also they serve as a evaluation, uh, purpose as well. And then the final part of online evaluation is crowd sourcing. So there are different websites that they use craft sourcing to rank and rate elements. And one common example is, uh, this LMRM. And it's initially developed by, um, graduate students. And if I search Google, it's it says that telemarinar is a public the base platform that developers LLMs through anonymous crowdsource pay wise comparisons. Users intercoms for two anonymous models to respond to, and then both on the models that give the better response. In which the model's identities are real. So basically, just ask the real humans it shows them two different responses from two different models, and then the users can vote. And then it uses those voting to rank elements. And then this is their website. There are different tabs here. I select the text, and text means basically the models for, like, teleemps, the models that are um, ranked. Um, basically, the models that are ranked for their text generation capability. And this is their ranking as of today. For example, the Line is Gemini 2.5 pro developed by Google. And these are the number of votes, and this is the score so we can see the the gap between, uh, different models. And then this is the license. Some of these are open source, some are closed. Uh, the second rank is o three, and then we have chat g p four o, 4.5, card model, uh, score, and and so on. This is TIPSY, which has this MIT license, which basically is open source, and you can use it. And it's also open rate, so the base are available to be used. You can see there are lots of different models and they rank, and it this this table keeps changing. This company is keep releasing new models and just, um, usually, newer models are better. So they rank higher. And, again, it's changing and changing. So I think it has, like, lots of elements now. Yeah. 211 elements are, um, are, uh, included in this table, and then 13,000,000,000 is 209 And these are relatively older models, and that's why they are, uh, lower on this list. So yeah. So that's it. That's the, uh, crowd sourcing platform. So these are all different ways that it can evaluate LLMs, and they're using practice to see if a newly trained LLM is better than the previous ones or not. Hello again. This is the finally, the last lecture of deep one. Of elements. So we learned everything about, uh, the foundations of elements, how the data preparation looks like. How the model architecture is, model training, both the stages, pretraining and post training, and also evaluation. One last topic that we want to cover in this lecture is the overall system design and how the what is the holistic view of, um, like, chatbot systems. As you can see here, trained LMM, this is basically the post train model that we finished training is here. And then, you know, it's just very small part of the entire system. And then there are a lot of other components that are necessary and used in practice to cover a service like Chachi. And this is simplified visualization. In practice, there might be even more components handling different things. But this would give a a good overview of what are different things that are important and needs to happen aside from the actual training. So let's just walk over them one by one and understand what are their purposes. So let me have the text prompt coming from the user. The first component usually we have is a some guard card rates, um, for input card rates, and we can also call them safety filtering. So all this comp all this component does is it ensures that, um, our x one the x one that was provided by the user is a safe prompt, meaning that it's the same safe request or safe example, if a user request something that is, um, violent or could be harmful, at this step, this component would identify and just, um, not answering it. So, basically, if the result of the safety filtering is that it's not safe, if it determines it's not safe, then it would use some other smaller models to generate some rejection response. This generated response would be shown to the user, and this is something like, hey. Sorry. They cannot assist you with that or things like that that we've seen on various platforms. Now if the if this component identifies that the x prompt is safe to process an answer, The second part is usually a prompt enhancer. And what this component does is it it most of the time is a machine learning based model. And what it does is it improves the text. So a lot of times in the input text, there is ambiguity. There can be some or grammar issues and all those kind of, uh, different problems in the initial text font. So this component is responsible to fix all of it. It's responsible to make sure punctuations are well, and it's there there in the prompt. There is no misspelling. There is no typos. If there are typos, it's they get fixed. And, also, if there are ambiguity or the grammar is not corrected, they they all get fixed. It can be a combination of heuristics as well as machine learning models are. Then after the prompt enhancement, we would have a prompt that actually is correct and, uh, is understandable. And it's not also weak. Now this enhanced prompt goes into this response generator This response generator is just a text generation decoding algorithm that is venture. And it, uh, interacts with the Trend LML and generate the tokens one by one. And, um, it could use some, um, like, topic sampling method. We'll talk about session management in a bit, but having that aside, after text response generator completes its process, there will be some response to the prompt. And then that response goes into this response safety evaluator. Or sometimes it's called output quadrates. So what it does, it, again, ensures that the generated output is also safe, meaning that it's not really, um, sharing something dangerous or harmful or biased. And then if the result of this is that it's not safe, it would go to the rejection response, uh, generator and to generate a response that, hey. Somehow, we couldn't process your request or something like that. And if it determines that it's safe, it would just show the generated response to the user. So every time a user enters something like this, it would go through all these processes and components, and then the user would see the generated response. Now if you, um, if you see in services like charity, when you ask questions, you get responses. You can ask follow-up questions. Um, and then their model would remember all the previous conversation and interactions. For example, you can ask something like, hey. Help me come up with a name for my start up. And then the model would share something, and then you would say, hey. These are not good, um, make it more formal going something like that. And the model would remember everything from before, and then they adapt the model would adapt its response and show you another response. And this is handled by by a session management. And all it does is basically it just keeps track of all the, uh, previous messages that you're between the user and the element, and it would append those into, um, and it's called chat history. It would append those to the new prompt each time. So each time you enter a text prompt, and it goes here, Right before that, it just happens here all the previous, um, uh, chats in that particular session. And all of these goes into the response generator. So when the response generator interacts with LMM using copy sampling, The LLM takes into account the entire track record when generating the new, uh, response. So this is how the follow ups and chat and, uh, follow-up questions are handled using a session ID. So that's it. That's, um, about overall system design. Again, in practice, there are a lot of other, um, components for different reasons. And, again, it's something there is not a single solution. Different companies use different components for different purposes. But this can be, uh, rough. You know, it gives an overall impression of how a system design of a chatbot can do back. So with that, we can wrap up week one to, uh, for Kellen and Foundations. We've learned everything we wanted to cover. And let's let me find this So, basically, we learned about LLMs, we saw free training, data data collection, model architecture, training, text generation, then we learn about post training, in particular, SFT and reinforcement learning. Well as reinforcement learning from human feedback for unverifiable tasks. Then we discuss evaluation and, um, system design. So with that, now we build a strong foundation. Now we can switch to more advanced use cases and extensions of that powers some of these applications.

ByteByte
23:18:17

Alright. So we can officially start. It's it's great to kick off our third cohort. Um, thanks a lot for joining the cohort. It's so great to meet all of you, and I'm really looking forward to the cohort and learn from all of you and the, um, we I hope you have a wonderful experience. Um, so again, the court the call

You
23:18:17

