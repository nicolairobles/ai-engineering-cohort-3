ByteByte
22:17:55

And how it works? And then after they data preparation, they talk about, um, model architecture. So they start with lambda three uses a standard dense transformer architecture. So as we mentioned before, most elements are all based on decoder only transformer. And after model architecture is the training. And then here, basically, this part talks about infrastructure, scaling, and efficiency. And if you just read it and go over it, there are a lot of details about the storage and compute that is required and they are for instance, they are training this model on, um, 16,000 g h 100 GPUs, and h 100 is one of the more powerful GPUs that are available in the market. So they are using 16,000 of those GPUs. And then here also, they're talking about how storage biggest storage is required and, you know, network, um, challenges. And then after that, they talk about all their optimization and techniques that they have to use. So they can only fit everything in memory. And they're talking about all different parleys, uh, parleys and techniques for model scaling and, um, you know, pipeline parallelism. So what I want to show is that there are and and CP section is very long. So they're doing a lot of, uh, tricks and a lot of engineering work just to make training possible on such a big model. So just engineering wise, I just wanted to say it's difficult. But in theory, it just gives us steps. If you look at the code, the code is just, um, calculating the loss using a standard cross entropy loss function. And then using some standard optimizer to keep updating the model parameters. And then keep doing this on the entire Internet data, and then the outcome would be a model that is really good at predicting dynamics token. Based on the statistics of Internet data. So I call all I wanted to cover for the model training.

You
22:18:26

And how it works. And then after the data preparation, they talk about, um, model architecture. So they'll start with Nana three uses a standard dense transformer architecture. So as we mentioned before, most elements are all based on the core only transformer. And after model architecture is the training. And then here, basically, this part talks about infrastructure scaling and efficiency. And if you just read it and go there, there are a lot of details about storage and compute that is required. And they are for instance, they are training this model on, um, 16,000 gig hitch 100 GPUs, and hitch 100 is one of their more powerful GPUs that are available in the market. So they are using 16,000 of those GPUs. And then here also, they're talking about how storage biggest storage is required and, you know, network, um, challenges. And then after that, they talk about all the optimization and techniques that they have to use. So they can only fit everything in memory. And they're talking about all these and parallel parallelism techniques for model scaling and, um, you know, pipeline parallelism. So what I want to show is that they are and city section is very long. So they're doing a lot of, um, tricks and a lot of engineering work just to make training possible on such a big model. So just engineering wise, I just wanted to say it's difficult. But in theory, it's just pieces of steps. If you look at the code, the code is just um, calculating the loss using a standard first entropy, um, loss function. Then using some standard optimizer to keep updating the model parameters. And then you keep doing this on the entire Internet data. And then the outcome would be a model that is really good at predicting the next token based on the statistics of Internet data. So I covered all I wanted to cover for the model training. Hello, everyone. In the last lecture, we wrapped up model architecture of elements. This lecture, we want to talk about text generation. Given that we haven't trained LMS, how can we use it to generate text? From the last lecture, we learned that if we use LMM and we pass some input rate like other times time, the LLM would produce probabilities for the next token. So it's going to be a vector, and each value would represent the probability of that particular, um, token. Now for text generation, this is not enough. What we want is we want actually some more text given input.

You
22:21:35

Text in a meaningful and contextually relevant way. So that's the purpose of the step for machine text generation. So to do that, text generation is an iterative process. We iteratively generate tokens, and we continue doing that. It's basically a for loop. So to better understand that, let's just watch to a real example. Let's say we pass Albert to an LMM, and then the LMM produces probabilities. Now in order to go from these probabilities to an actual token, we need some strategy for some algorithm to pick one token. From these probabilities. Let's assume that there is some algorithm that does that for us. So it takes Einstein from these probabilities. Then now we have Albert Einstein. So we pass we can pass it to the LMM. And the LMM produces probabilities for the next token after Albert Einstein. Times. And then the algorithm picks was, and then we can repeat this process. So we can just, uh, repeat this process and keep getting new tokens until, uh, one of two things happen. So the generation ends when, one, we reach end of sentence, a special token. This is a special token that we can use when training the LLM. So that whenever the LLM outputs that, it means that the sentence is, uh, complete, and we no longer need to generate models. So this is one way that the generation can end. The other way is that the LLM never produces end of sentence token and reach the maximum desired length. So, um, as soon as one of these two things happen, we can stop the syntactic generation, and we would end up with a continuation of the initial sentence that we have. So I have a more compact uh, visualization of this, um, thing here, and then it's here. So, basically, we started something. Let's say, hi. And then the LLM produces probabilities and algorithm picks the next token, which is how, let's say. And then how comes here. Now the new input is I, how goes to the LLM, then you pick r. We keep doing this. In this case, we reach end of sentence, meaning that the LLM, at some point, which believes that the sentence is completed, it produces end of sentence special token. So we learned about this iterative, um, generation process. Now the only thing that we need to answer is the algorithm here. How can we really pick token given their probability distribution? And that's what we are going to talk next. So, basically, there are different algorithms and different strategies. We often call them decoding algorithms. Or sampling algorithm. And these algorithms specify how to choose a token from a probability distribution. So given that the LMM output something like this, it can be interpreted as this probability distribution. The algorithm specifies how to pick the next token from this probability. There are different algorithms for that purpose, and they can be categorized in different ways. Ways. So here, we have, uh, at the very high level, all the text generation methods or sampling methods. Categories include deterministic and the stochastic. Deterministic are the algorithms that the, um, there is no randomness in the generation process. So if you repeat the generation using the same input and same algorithm, it would always reduce the same output. The stochastic algorithms, on the other hand, has some random meaning that using the exact same LMM and exact same input, if you run it twice, run text generation twice, get two different continuations or two different, uh, generations. And each of them has their, um, pros and cons. So we'll start with deterministic, and then you'll switch to as a gas. Deterministic, there are two popular algorithms. One is greedy search, and the other is beam search. Again, we'll briefly talk about this to better understand the evolution of different, uh, text generation algorithms. But I want to also point out that in most autonomous elements, we they no longer using, like, greedy search or beam search. And they use this copy sampling here, which we will talk about. But it's it's very good to just have some ideas about how these, um, search algorithms work and what are the limitations. We'll start with three d search.

ByteByte
22:21:36

Hello, everyone. In the last lecture, we wrapped up model architecture of LMS. In this lecture, we want to talk about text generation. Given that we have a trained LMS, how can we use it to generate text Next lecture, we learned that if we use LMM and we have some input to it like Albert Einstein, the LMM would produce probabilities for the next token. So it's going to be a vector, and each value would uh, represent the probability of that particular, um, token. Now for text generation, this is not enough. What we want is we want actually some more text, given, uh, input text. For example, if you are given Albert Einstein, we want the LMM to generate more text. In a meaningful and contextually relevant way. So that's the purpose of the step four, which is text generation. So to do that, text generation is an iterative process. We iteratively generate tokens, and we continue doing that. It's basically a for loop. So to better understand that, let's just watch through a real example. Let's simply pass Albert to an element, and then the element produces probabilities. Now in order to go from this probabilities to an actual token, we need some strategy or some algorithm. To pick one token from these probabilities. Let's assume that there is some algorithm that does that for us. So it picks Einstein from these probabilities. And then now we have Albert Einstein. So we pass we can pass it to the LMM again. And the LMM produces probabilities for the next token. After other time stamp. And then the algorithm picks one. Then we can repeat so we can just, um, repeat this process and keep getting new tokens. Until, uh, one of two things happen. So the generation ends when one we reach end of sentence, a special token. This is a special token that we can use when training the LMM. That whenever the LLM outputs that, it means that the sentence is, uh, complete and we no longer need to generate more tokens. So this is one way that the generation can end. The other way is that the LLM never produces end of sentence token and reach the maximum desired length. So, um, as soon as one of these two things happen, we can stop this with trigger generation. And we would end up with a, um, continuation of the initial sentence that we have. So I have a more compact, uh, visualization of this thing here, and then it's here. So, basically, we started something. Let's say hi. And then the LLM produces probabilities and algorithm picks the next token, which is how, let's say. And then how comes here. Now the new input is hi, how. Goes to the LMM, then we pick r, and we keep doing this. In this case, we reach end of token, meaning that LLM at some point, which believes that the sentence is completed, it produces end of sentence special token. So we learned about this iterative uh, generation process. Now that we need to answer is the algorithm here. How can we really pick token given their probability distribution? And that's what we are going to talk next. So, basically, there are different algorithms and strategies. We often call them decoding algorithms or assembling algorithms. And these algorithms specify how to choose a token from a probability distribution. So given that the LMM output something like this, which can be interpreted as this, uh, probability distribution, the algorithm specifies how to pick the next token from this probabilities. There are different algorithms for that purpose, and they can be categorized in different ways. So here, we have, uh, at a very high level, all the text generation methods or sampling methods. They can be categorized into deterministic and the stochastic. Deterministic are the algorithms that the, um, there is no randomness in the generation process. So if you repeat the generation using the same input and same element, it would always produce the same output. The stochastic algorithm, on the other hand, has some randomness. Meaning that using the exact same element and exact same input, if you run it twice run text generation twice, we would get two different continuations or two different, uh, generations. And each of them has their, um, pros and cons. So we'll stop with the terminologies, and then we'll switch to stochastic. Deterministic, there are two popular algorithms. One is greedy search, and the other is beam search. Again, we'll briefly talk about this to better understand the evolution of of different, uh, uh, next generation algorithms. But I want to also point out that in most advanced elements, we they no longer use any, like, research or beam search. And they use this copy sampling here, which we will talk about. But it's very good to just have some ideas about how these, um, search algorithms work and what are their limitations. So we'll start with three d search.

ByteByte
22:24:15

Greedy search is basically the simplest form of, um, picking a token from the probability distribution. It as the name suggests, it it really picks the highest probability token then use it as a next token. So it's just, um, very simple. So let's say, for instance, this is the, uh, predicted probabilities. Given the input token, how? So the model has predicted these values for different tokens. Uh, in this example, the previous search simplifies r because it has the highest highest chance of being the next token. It has 37% chance, and it's more than all other tokens. So another way to visualize this is, uh, like a tree. So we start from this input, um, again, which is how. It's passed to the LMM, and then the LMM uh, outputs the probabilities for all different tokens in the vocabulary. For simplicity, we visualize three outputs here. But in practice, it's equal to the number of tokens in the vocabulary. So let's say it's 50,000 values. So at this step, the algorithm picks the highest probability token, which is r with 56%. And now we have how r. It's passed to the LLM. The LLM again produces 50,000 probabilities in the next step. And the greedy algorithm picks u because it has highest probability. And it just keeps repeating this. So at the end, we will end up with a path in this tree. Uh, like, how are you, um, doing? So the advantage of research is that it's very simple and efficient. It simply picks the highest probability at each step, and it can be easily implemented. However, there are lots of limitations with presearch, and it's not and it's not really used in, uh, in practice. Practice for text generation in elements. The key limitation of video search is that it's not really looking ahead. So at each step, it's only picking the highest probability and without looking, um, the probabilities at the, uh, future steps. So this may lead to suboptimal generations because there might be better, um, sequence of tokens. In other branches. For example, here, even though probability, there might be a different branch that if the model the text generation explores that, it may need to override, may need to something with higher probability. If we just consider the probability of all the tokens in the sequence. But greedy search does not that. Greedy search just picks the highest probability at each looking ahead or considering any other branches. The other key limitation of grid search, it may lead to repetitive outputs. What that means is that you would see a sequence of tokens that keeps repeated, um, in the generated text. And the reason for that is because there might be some cases where a sequence tokens has very high probability and are very common and also commonly, um, appeared on the Internet. So the greedy search keeps picking those sequence of tokens because it's just preferred. They have higher highest probability. Statistically, and the algorithm just, uh, prefers to deterministically just keeps, um, picking them. So that's greedy search before we switch to the next algorithm, which is beam search. Let's see greedy search in example in in practice. So have this quote here in my JupyterLab. Um, and then there are some different libraries. Again, the purpose is not to, uh, be super familiar with some of these libraries and how they work. But there is this transformers library, which easily gives us access to various pre trained models and also digital organizers. So I'm using that to get a model, g p t two model. And, also, it's corresponding tokenizer. So let me run this.

ByteByte
22:30:46

And then the next thing I want to do is I want to start with some, uh, input sentence. For example, I enjoy walking with my cute dog. Now what needs to happen is that this text first needs to be converted to a sequence of IDs using the tokenizer. So I apply I run the tokenizer on this sentence, and it would give me model inputs. And model inputs is simply a sequence of IDs. Or a sequence of, uh, IDs corresponding to the these tokens. So let me run this. Now if I print model inputs, we would see a sequence of numbers. Forty twenty eight, 83, and so on. And these are corresponding to these individual, um, sub words or tokens. So now we have model inputs. We have the model. We have everything ready. The next thing is we have to now run text generation using greedy search. And we don't have to implement greedy search from scratch. It's already implemented, and there is this, uh, method called generate with some inputs. Again, there are some good documentations to understand what are the inputs, but my purpose here is to only show it means to run previous search on some input using GPT-two model. So here, I'm passing all the, um, the and also I set max new tokens to 40. What that means is that do not generate more tokens after you generate 40 new tokens. And um, this is necessary because otherwise, the model may generate a lot of tokens, and it may never end. So now I run this and let's see what is output. So Alright. Here this is output. It starts with enjoy I enjoy walking with cute dog. This is my original sentence. And then everything after this is what, uh, the previous search algorithm generated. And it starts with comma. So the continuation is comma, but I'm not sure if I'll ever be able to walk with my dog. I'm not sure if I'll ever be able to walk with my dog. And also here, I'm not sure. This is the the continuation is still probably the same token. So the same sequence of tokens. So this is a good example of showing repetition. This this sequence of tokens, I'm not if I'll ever be able to work with my job. It's apparently very common, um, sequence of tokens. They have high probability. The LMM, the g p two model generates very high probabilities for this sequence. And then the greedy search algorithm is deterministic. So each time it just prefers to just, um, pick sequence of tokens over and over and over again. So this is greedy search and this is why it's not ideal to be using practice for text generation in elements. Now let's get back to our, um, lecture and switch to the next algorithm. Which is in search. So the idea behind bean search is to, um, basically fix some of the limitations of greedy search. Remember, greedy search always picks the, um, token, What beam search does is it keeps track of the top k, uh, path industry. So for instance, instead of picking the highest probability NICE token, it would always keeps track of the highest um, three highest probability, like, three passes with the highest, uh, cumulative probability. So for example, if maybe pass half to LNM and we get all those 50,000 probabilities, the beam search would pick top three probabilities. For instance, let's say they are cum or do. With these, um, numbers. And then it just discards everything else. So at the next for the next step, we have three paths. Three, um, possible uh, continuations or three possible sequence of tokens. And they are how come, how are, how do. Now LMM, again, for each of these, it's it's, uh, we pass to the LMM. And the LMM generate the next the probabilities, 50,000 for each of these. And then out of all those 50,000, I think it's going to be overall 100. Assuming that the output of the element has 50,000 numbers. So out of all of them, the beam search comes and look at the the cumulative probability. For example, how calm animals has, um, 24% here and 36% here. How are you as um, 31% here, 91% here. And then how do you has also 26% here, um, 63% here. So these three paths are better than all other passes. Here. So the BIM search discards all these, um, possible continuations. And only keeps, uh, these three, uh, sequence of tokens which we have in both, uh, lines. So at second step, we would end up with again three d path. How come animals, how are you, and who how do you? And then it goes to the next step. So it keeps doing that again until, um, a certain number of iterations it reaches. And then finally, it just picks the highest probability path out of the three candidates. So this is in search. Again, if if you can think of it as an extension of real research by by keeping track of top k, um, possibilities, I'd teach you step. Um, and the advantage of this is that it it may sometimes discover continuations compared to the degree search. For example, here in this case, it may at some point, it may decide that how do you is and if they're contained in the continuations of that has higher probability or is more likely to be a better con continuation compared how are you. So by just having keeping track of top okay, uh, path, it most of the times it outperform really search. So this is Bing search. And BIM search also has the similar issues of repetitions. A lot of times you may see repetitions when you run it on the element. Um, but overall, you would see now more relevant continuations in most examples. Now this is beam search, and then we covered deterministic algorithms. And without green search, beam search are deterministic, meaning that there there is no randomness. If you if you run it multiple times on the same input, you will get the same output. For example, here, if you if I run this again, I would the same output. I enjoy working with my cute dog and the result. Now the next category of, um, algorithms are stochastic bid randomness. And we'll start with multinomial sampling. That's the simplest way of sampling. Um, among all these three algorithms. So let's just start with that. Multinald sampling. Basically, what it does is that instead of picking the highest probability token it just samples according to their probabilities. So, for example, if this is a, uh, predicted probability, by the model, the algorithm at this step would pick r with 37 chance. It would pick 21 sorry, is with 21% chance. And so on. So this this randomness basically allows you to discover other continuations and not always pick r as the next token. So that's multinomial. But there is a major limitation with multinomial. Because it sometimes according to their probability distributions. Sometimes, if they run enough times, it may pick tokens that are less likely or or very unlikely. For example, if you run it a 100 times, at some point, the next token that it may pick after half might be hard. Imagine there are some more tokens here that can be even they can be grammatically incorrect to be to come after how. But the multinomial sampling may occasionally pick some of those tokens. So that's why multinomial sampling is not really used in practice in evidence. And, um, some variations of multinomial sampling is used. And top k and top d are both improvements over multinomial sampling to fix that I just mentioned. So the next thing we can be, uh, talk about is top k sampling. So top case handling, basically, instead of picking according to the probability distribution of autocents, it first picks the top k tokens based on their probabilities, and then it samples according to the probability distribution. So this way, it discards the unlikely tokens, and it it does not even consider those tokens. For example, in this case, if topic is let's say k is equal to three and k is a hyperparameter, we can define it. But let's assume k is three, and we want to run top k top three, um, sampling from this property distribution. First, we keep the first three highest probability tokens, which are, um, are easy to do.

ByteByte
22:37:53

We discard everything else. And then out of these three, now we sample one token. So this is where the randomness is introduced. So topic is clear clearly better than matinomial sampling because it simply discards what is very unlikely to come next and only keep, uh, likely words. And their sample from those. So this is Copy. But topic but topic has also one limitation that, uh, we'll talk about. And that's what, uh, top piece handling is trying to fix. So let's go here. Imagine these two different probability distributions. In this example, we have probability of next location for tank's a, and then these are the probabilities. You can see tanks a lot Lot has the highest probability, 89%. And then everything else is very low. Now if you run top k algorithm on this, and k is, let's say, equal to three, it means that we are going to keep the first three tokens and consider all these three, um, lots much high. As a, um, as a possible as a candidate for the next token. However, the model is somewhat confident that this is the token. We don't these are very unlikely to come next. But up k, since k is fixed, would always consider top three, um, tokens. Now imagine this uh, probability distribution, which is a probability of tokens coming after half. For this model, top k, again, you would consider top uh, top three, let's say. It would consider the first three tokens. Or is due, and it would ignore the rest. However, in this example, according to the probabilities, the model is confident about some other, um, tokens. So it's it's basically it's more uniform, the predictive probability. So there are more tokens that can be good candidates to come next to come after how. So all these explanations, what what it means is that top k, because k is fixed, is not a good, um, is not an ideal algorithm. Because regardless of the shape of this probability distribution, it would always consider the top k tokens. When the model is very confident about, let's say, the next token, it would still consider top k. And then the model is not too confident and there is a huge possibility of candidates, the model would still pick, okay. And that's why what TOPP sampling is trying to address. So what TOPP does is basically it it it does not fix k. What it does is that it picks the top k tokens that their cumulative probability exceeds value p. Now this p is a hyperparameter that we can provide and we can pass to this, uh, we can specify in this sampling algorithm. For example, we can say top p p equal to 90%. Or in this example, we have 88%. P is equal to eighty eight. In the it means that peak, the first few tokens, such that their cumulative probabilities exceed 88%. And in this example, because the model is relatively confident about next token lot and it has already 89%, it exceeds our threshold. And topizampling would only consider this. In this example, since the model is less confident and there are more, uh, possibilities, the model would pick these four tokens because the first one is 31%, and it's not exceeding 88% threshold. The next one is, uh, 29%, so they're they're sum is, um, 60, and still it's below eighty eight. So at this point, the sum of these tokens, we call it exceeds eighty eight. And then the model discards the rest of the token and sample from this. So basically, top peak is just an improvement over top k with a more dynamic k. It dynamically changes k, and, um, it basically um, it basically calculates it basically dynamically decides on k based on the cumulative probabilities. And copy is what is commonly used in practice in LMS to generate text. Let's go back to my Jupyter Notebook and see if I copy on the same input, what's going to be the output. Again, I'm going to use the same I enjoy working with my QDOT. Then I have copy sample here. Again, model dot generate works with that. We just need to assist at, uh, spell through. And provide the top p value. In this case, I'm just I'm saying 92, meaning that I'm saying that use the tokens at accumulated probability probability are 92. And discard the rest of the tokens at each step. Now if I run this, I get this output. I enjoy working with my q doc. This is the original sentence. And now the rest is, um, comma. And then says RM, um, whatever 65 who is on a leash at home. Um, so, again, it's not the best continuations, and that's because two is not the best model out there. If we switch to some better models, we would say you would see a lot better continuations. But at least we are not suffering from, like, radiations or things like that. And also there's some randomness. Meaning that if I rerun this, I will see a different continuations. Let me rerun. Okay. Now I'm clearly seeing something different. Uh, I enjoy working with my cute dog. I don't know what that is. And then it's saying I love with my baby dog and having an empty apartment without dogs and other kind of noises. These combinations are not the best, but it's not because our sampling. Algorithm is bad. It's because g p two is not too good. If it's simply switch g p t two with something more powerful like g p p p or more even more powerful models, we would see a lot better continuations. And in future lectures, we would do that to see how how different the output can be. So this is top desampling, and, um, there are some great, um, are some great, uh, like, tutorials about these algorithms and strategies and how different they are, what is going to be the output, uh, you know, the details of the arguments to use this generate and everything. So it's an interesting Greek. I've included the links in the lecture. And also here is basically another very interesting, um, tutorial how to generate text Both of them are published by Hugging Face. So it basically starts with the introduction and then it starts on beauty search. And it just goes and switches to our more advanced, um, algorithms. And then I think at the end, it talks about, uh, top k and top b and some examples. So it's it's an interesting read, but basically just explains what we already discussed in the the lecture. So the next thing I want to share is that top e has this, um, p hyperparameter. Some hyperparameters that we can specify or introduce in our sampling algorithm. For example, we can always, um, we can have a, um, a hyperparameter to adjust the raw probabilities. So for example, when the model output these probabilities, we can just make it a little bit smoother. Or we can make it sharper. So these are some of the things that are basically there is no best best or optimal values. It just depends on the task and depends on, uh, our model, what are the best values to afford the p or some other hyperparameters. You would see some of those values when you try to call elements. For instance, here, you saw that how we specify top And there are potentially more, um, more arguments here that we can't specify. And not going to go too deep into that. Um, just one thing I want to show here is that again, as I mentioned, it it's very empirical. We have to try an experiment based on the model and based on the task. And also for different tasks, there might be, uh, different p values. That are optimal. For example, this is just a very empirical, um, temperature and top view ranges for different tasks. Temperature, again, this refers to a type of parameter that smooth the raw probability. So we can use it to make the raw probabilities, uh, sharper or more uniform. So, um, for example, for generation, it's been uh, seen that copy of 0.1 is a good number. And what that means is that basically, you're saying the algorithm to only consider, uh, very few tokens, not not, uh, many tokens as candidates. And that's because code generation is tricky. It's important. We don't want to really we don't care about novelty. What we want is that we pick the tokens in the code that are, um, um, um, syntax wise, they were correct. And they can be run. So we don't really want to consider all those uncertain tokens that the model predicted. However, in more, um, like, creative tasks, like creative writing, we want to increase p. Because now we want more novel continuations as well. So we always want the model the, uh, text generation algorithm. To consider, uh, less likely continuations that might be more interesting or exciting. So that's all I wanted to cover for text generation. And in the next lecture, we'll switch to post training.

You
22:37:54

Greedy search is basically the simplest form of, um, picking a token from the probability distribution. It as the name suggests, it it readily picks the highest probability token then use it as the next one. So it's just, um, very similar. Let's say, for instance, this is the, uh, predicted probabilities given the input token how. The model has predicted these values for different tokens. And in this example, the previous search simply gets r because it has the highest chance of being the next token. It has 37% chance, and it's more than all other tokens. Tools. So another way to visualize this is, um, like a tree. This start from this input, um, okay, which is how it's passed the LLM, and then the LLM, uh, outputs the probabilities for all different tokens in the vocabulary. For simplicity, we visualize three outputs here, but in practice, it's equal to the number of tokens in the vocabulary. So let's say it's 50,000 values. So at this step, the algorithm picks the highest probability token, which is r, at 56%. And now we have how r, it's passed to the LLM. The LLM again produces 50,000 probabilities in the next step, and the greedy algorithm picks you because it passed by its probability. And it just keeps repeating this. So at the end, they would end up with a path in this tree. Uh, like, how are you, um, doing? So the advantage of research is that it's very simple and efficient. It it simply picks the highest probability at each step, and it can be easily implemented. However, there are lots of limitations with with this strategy. It's not and it's not really used in, uh, impact score text generation in elements. The key limitation of research is that it's not really looking ahead. So at each step, it's only picking the highest probability okay, without looking, um, the probabilities at the future steps. So this may lead to stop optimal generations because there might be better, um, sequence of tokens. In other branches. For example, here, even though r has the highest probability, there might be a different branch that if model detects generation explores that, it may lead to overall, it may lead to something with higher probability. If we just consider the probability of all the tokens in the sequence. But greedy search does not that. Greedy search just picks the highest probability that is just step without looking at the or considering any other branches. The other key limitation of critic search it may lead to repetitive outputs. So what that means is that you would see a sequence of tokens that keeps repeated, um, in the generated text. And the reason for that is because there might be some cases where the sequence of tokens has high probability and are very common and also commonly, um, appeared on the Internet. So the previous search keeps picking those sequence of tokens because it's just preferred. They have higher highest probability. Statistically, and the algorithm just, uh, prefers to deterministically just keeps, uh, picking it. So that's greedy search before we switch to the next algorithm, which is beam search. Let's see pretty search in example in in practice. So I have this code here in my JupyterLab. Um, and then there's some different libraries. Again, the purpose is not to, uh, be super familiar with some of these libraries and how they work. But there is this transformers library, which easily gives us to various between models and also different organizers. So I'm using that to get the model, g p t two model, and also its corresponding organizer. So let me run this. And then the next thing I want to do is I want to start with some, uh, input sentence. For example, I enjoy working with my QA. Now what needs to happen is that this case needs to be converted to a sequence of IDs using the tokenizer. So I applied I run the tokenizer on the sentence. And it would give me model inputs. And model inputs is simply a sequence of IDs or a sequence of, uh, IDs corresponding to the these tokens. Let me run this. Now if I print model inputs, we would see a sequence of numbers. 40, 20, and 83, and so on. And these are corresponding to these individual, um, sub codes or tokens. So now we have model inputs. We have the model. We have everything ready. The next thing is we have to now run text generation using three d search. And we don't have to implement three d search from scratch. It's already implemented, and there is this, uh, method called generate. With some inputs. Again, there are some good documentations to understand whether the inputs, but my purpose here is to only show it means to run previous search on some input using GPT-two model. So here, I'm passing all the, um, the sequence of IDs, and also I set max new tokens to 40. What that means is that you'll not generate more tokens after you generate 40 new tokens and this is necessary because otherwise, the model may generate a lot of tokens and it may never end. So now I run this, and let's see what is output. So alright. Here, this is output. It just starts with pin I enjoy walking in get my QTAP. This is my original sentence, and then everything after this is what, uh, the pre research algorithm generate. And this starts with comma. So the continuation is comma, but I'm not sure if I'd ever be able to walk with my dog. I'm not sure if I'll ever be able to walk with my dog. And also here, I'm not sure, which is the continuation is still probably the same token. So the same sequence of tokens. So is a good example of showing repetition. This this sequence of tokens I'm not sure if I'll ever be able to work with my dog. It's apparently very common, um, sequence of tokens So And then the previous search algorithm is deterministic. So each time, it just prefers to just just, um, pick the same sequence of tokens over and over and over again. So this is pretty safe, and this is why it's not ideal to be using Rakis for text generation in PLL elements. Now let's get back to our, um, lecture and switch to the next algorithm, which is beam search. So the idea behind beam search is to, um, basically, fix some of the limitations of greedy search. Remember, greedy search always fix the, um, token, but beam search does it it keeps track of the top k, uh, path industry. So for instance, instead of picking the highest probability next token, it would always keeps track of the highest, um, highest product, like, three passes with the highest, uh, cumulative probability. For example, if then you pass out to the LMM and we get all those 50,000 probabilities, the in search would pick top three probabilities. For instance, let's say they are cum or two. It is, uh, numbers, and then it just discards everything else. So at the next for the next step, we have three paths. Three, um, possible continuations or three possible sequence of tokens. And they are how come, how are, how do, Now LLM, again, for each of these, is it's, uh, passed to the LLM. LMM generate the next up to probabilities, 50,000 for each of these. And then out of all those 50,000, I think it's going to be overall 150,000. Assuming that the output of LLM has 50,000 numbers. So out of all of them, the bin search comes and look at the the cumulative probability. For example, how come animals has, um, 24% here and 36% here. How are you as uh, 31% here, 91% here? And then how do you, as also 26% here, um, 63% here. So these three paths are better than all other passes. So the beam search discards all these, um, possible continuations, and only keeps, um, these three, uh, sequence of the which we have in both, um, lines. So at second step, we will end up with again three path. How come animals? How are you? And who? How do you? And then it goes to the next So it keeps doing that again until, uh, a certain number of iterations it reaches. And then finally, it just hits the highest probability path out of the three candidates. So this is Immersed. Again, it it think of it as an extension of research by by keeping track of top k, uh, possibilities that you just said. Um, and the advantage of this is that may sometimes discover better, um, continuations compared to the degrees For example, here in this case, it may at some point, it may decide that how do you is if they're content and the the continuations of that has higher probability or is more likely to be a better con continuation compared to how far you. So by just having keeping track of top top k path, it most of the times, you calculate from previous edge. So this is beam search, and beam search also has the similar issues of repetition A lot of times, you may see repetitions when you run it on the element. Um, but overall, you would see, you know, more relevant continuations in most examples. So now this is beam search, and then we cover deterministic algorithms. And both of grid search and beam search are deterministic, meaning that there is no randomness if you if you run it much would get the same output. For example, here, we if you if I run this again, I would get the same app. I enjoy watching it by two dot and the rest of it. Now the next category of, um, algorithms are stochastic. It's randomness. And we'll start with multinomial sampling. That's the simplest way of sampling. Um, among all these three algorithms. So let's just start it back. Multinomial sampling. Basically, what it does is that instead of picking the highest probability token, it just samples according to their probabilities. So for example, if this is a predicted probabilities by the model, the algorithm at this step would pick r with 37 chance. It would pick 21 sorry, please with 21% chance. And so on. So this this randomness basically allows you to discover audio continuations and not always r as the next token. So that's Montinovion. But there is a major limitation with Multinomial because it sometimes according to their probability distributions, sometimes if they run enough times, it may pick tokens that are less likely or or very unlikely. Example, if you run it 100 times, at some point, the next token that it may pick after how might be hard. Imagine there are some more tokens here that can be even they can be grammatically incorrect to be to come after half. But the multinomial sampling may occasionally pick some of those tokens. So that's why multinomial sampling is not really used in practice in LMS. And, um, some variations of multinomial sampling issues. And top j and top j are both improvements over matronomials sampling to fix the issue that I just mentioned. So the next thing we can be, uh, talk about is top case handling. So So top case handling, basically, instead of picking according to their probability distribution of all tokens, it first picks the top k tokens based on their probabilities, and then it samples according to their probability distribution. So this way, it discards the unlucky tokens, and it does not even consider those tokens. For example, in this case, if top k is let's say k is equal to three and k is a hyperparameter, we can define it it. But let's assume k is three and we want to run top k top three, um, sampling from this probability distribution. First, we keep the first three highest probability tokens, which are, um, are is due. Discard everything else. And then out of these three, now we sample one token. So this is where the randomness is introduced. So k is clear clearly better than monotonous sampling because it simply starts what is very unlikely to come next and only keep, uh, lucky words and then samples from those. So this is top b, but top k but top k has also one limitation. That, uh, we talked about, and that's what, uh, top k sampling is trying to fix. So let's go here. So imagine these two different probability distributions. In this example, we have probability of next token for tanks head, and then these are the probabilities. You can see tanks a lot. Lot has the highest probability to 89%. And then everything else is very low. Now if you run top k algorithm on this and k is, let's say, equal to three, it means that we are going to keep the first three tokens and consider all these three, uh, lot much high. As a, um, as a passive passive candidate for the next token. However, the model is somewhat somewhat confident that this is the token. You don't these are very unlikely to come next. But top k, since k is fixed, we'd always consider top three, um, tokens. Now imagine this, uh, probability distribution, which is the probability of tokens coming after half. For this model, top k, again, it would consider top uh, top three, let's say. You would consider the first three tokens. Or these two, and it would ignore the rest. However, in this example, according to the probabilities, the model is also confident about some other, um, tokens. So it's it's basically it's more uniform, the predicted probability. So there are more tokens that can be good candidates to come next to come after half. So all these explanations, what it what it means is that top k, because k is fixed, is not a good, um, is not an ideal algorithm. Regardless of the shape of this probability distribution, it would always consider the top k tokens. When the model is very confident about, let's say, the next token, it would still consider top k. Then the model is not too confident and there is a huge possibility of candidates, the model would still pick top k. And that's why what copy sampling is trying to address. So what copy does is basically it it it does not fix k. What it does is that it picks the topk tokens that their cumulative probability exceeds value p. This p is a hyperparameter that we can provide and we can pass to this, uh, we can specify this algorithm. For example, we can say copy p equal to 90%. Or in this example, we have 88% key is equal to eighty eight. And when it's 88, it means that peak, the first few tokens, such that their cumulative probabilities exceed 88%. And in this example, because the model is relatively confident about next token dot and it has already 89%, it exceeds our threshold. And topic sampling would only consider this. This example, since the model is less confident and there are more, uh, possibilities, the model would pick these four tokens because first one is 31%, and it's not still exceeding 88% threshold. The next one is, uh, 29%, so they're they're zombies, um, 60, and the state is below 88. So at this point, sum of these tokens, we call x six eight eight, and then the model discard the rest of the tokens and sample from this. So basically, top k is just an improvement over top k with a more dynamic decay. Dynamically changes k, and, um, it basically it basically calculates it basically dynamically decides on k based on the cumulative probabilities. And copy is what is commonly used in practice in telelamps to generate text. Let's go back to my Twikitr notebook and see if I run copy on the same input, what's going to be output. Again, I'm going to use the same. I enjoy working with my view doc. And then I have topic sampling here. Again, model dot generate works with that. I you just need to add, uh, still true. And provide the top key value. In this case, I'm just I'm saying 92, meaning that I'm saying that use the tokens at the accumulative probability or 92, and then discard the rest of the tokens at each step. Now if I run this, I get this output. I enjoy what they did my cue card. This is the original sentence. And now the rest is, um, and then says r m, 65 who is on the leash at home. Um, so again, it's not the best continuations, and that's because GPT two is not the best model out there. If we switch to some better models, we would say you would see a lot better continuations. But at least we are not suffering from, like, repetitions or things like that. And also there is some randomness, meaning that if I rerun this, I would see a different continuations. Let me rerun. Okay. Now I'm clearly seeing something different. Uh, I enjoyed working with my QTAP. Um, I don't know what that is. And then it's saying, I love walking with my baby dog and having an empty apartment without dogs and other companions. Again, these continuations are not the best. It's not because our sampling, uh, algorithm is done. It's because g p two is not too good. If we if we simply switch GPT two with something more powerful like GPT three, more even more powerful models, we would see a lot better continuations. And in future lectures, we would do that. To see how how different the output can be. So this is top piece handling, and, um, there are some great, um, there are some great, uh, like, tutorials about these algorithms and the strategies and how different they are, what is going to be the house food, uh, you know, the details of the, um, arguments we use this generate and everything. So it's interesting brief. I've included the links in the lecture. And also here is basically another very interesting tutorial on how to generate text. Both of them are published by Agnew Trace. So it basically starts with the introduction and then it starts from brief research. And it just goes and switches to a more advanced, um, algorithms. Then I think at the end, it talks about, uh, top k and top p and some examples. So it's it's an interesting read, but basically just explains what we already discussed in there. So the next thing I want to share is that Topghee has this, uh, p hyperparameter, and there are also some other hyperparameters that we kinda specify or introduce in our sampling algorithm. For example, we can always, um, we can have a, um, a hyperparameter to adjust the raw probabilities. So for example, when the model output these probabilities, we can just make it a little bit smoother. Or we can make it sharper. So these are some of the things that are basically, there is no best or optimal values. It just depends on the task and depends on, uh, our model, what are the best values to for p or some other type of parameters. Um, so you would see some of those values when you try to LMS. For instance, here, you saw that how we specify topic, and there are potentially more uh, more arguments here that the time is specified. And not going to go too deep into that. Um, just one thing I want to show here is that again, as I mentioned, it's very important. We have to try an experiment based on the model and based on the task. And also for different tasks, there might be, uh, different p values. Uh, that are optimal. For example, this is just a very empirical temperature and top few ranges for different tasks. Temperature, again, this refers to a hyperparameter that smooth the raw probabilities. So we can use it to make the raw probabilities, uh, sharper or more uniform. So, um, for example, for code generation, it's been uh, seeing that copy of 0.1 is a good number. And what that means is that, basically, you're saying the algorithm to only consider very few tokens, not not, uh, many tokens, as 10. And that's because code generation is tricky. It's important. We don't want to really we don't care about novelty. What we want is that we pick the tokens in the code that are, um, um, um, syntax syntax wise, they are correct. And they can be run. So we don't really want to consider all those uncertain tokens that the model predicted. However, in more, um, like, creative tasks, like creative writing, we want to increase speed because now we want more novel continuations as well. So we always want the text generation algorithm to consider, uh, less likely continuations that might be more interesting or exciting. So that's all I wanted to cover for next generation. And in the next lecture, we'll switch to post training.

You
22:40:19

Base model, that is really good at predicting the next token. And then we can run text generation to continue text, um, given some input, uh, sentence. Now that base model is not useful because it's going to just continue whatever input sentence is given to you, and it's not necessarily going to answer your question. Now the purpose of post training is to adapt that base model that it answer questions and also be more helpful and safe. And post training has two steps, supervised fine tuning or SFE, and reinforcement learning, and you're going to talk about both of these in this lecture. Before talking about the safety, I want to go back to our Jupyter Notebook. And see what we mean by, uh, when we say that pre trained model is add continuing sentences and not answering questions. So this is, again, the same library transformers. I'm going to pull g p t two model and its tokenizer.

ByteByte
22:40:19

Base model, that is really good at predicting the next token. And then we can run text generation to continue text, um, given some input, uh, sentence. Now that base model is not useful because it's going to just continue whatever input sentence is given to it, and it's not necessarily going to answer your questions. Now the purpose of post training is to adapt that base model so that it answer questions and also be more helpful and safe. And post learning has two steps, supervised learning or SFT and reinforcement learning, and we are going going to talk about both of these in this lecture. Before talking about SFT, I want to go back to our Jupyter notebook and see what we mean by, um, when we say that pre trained model is good at continuing sentences and not answering questions. So this is, again, the same library transformers. I'm going to call g p two model and its tokenizer.

You
22:41:09

Correct. And we'll use the same sentence. I enjoy working with my tutor. Um, I use copy center link with with some optimal parameters. Then we run it to see what the output is. And remember, copy is the best sampling method that is available and most used in these days. So the output is I enjoy working with my few dog. My wife, and my friend. They always have me covered with a blanket as well. So the the input. Now, um, also remember that pre training a stage, we we previously said that it's a very expensive stage, and it's most important part because it's not from Internet data. So this base model, even though it's just a, um, next token predator or just continues the inputs, it is a still very powerful model. It's basically it has all the implicit knowledge that it have seen in on the Internet. So what that means is that if we change it to, for instance, I like machine learning because

You
22:42:22

would continue this, but now it would continue and use words and terminologies that are very relevant to machine learning and why you like it as one. So what that means is that it has some implicit knowledge about machine learning, what's are typical words, um, how they follow each other, and so on. It can make this model very powerful. So it says I like machine learning because it's easier to understand, and you can use it to predict what people are going to say. Um, it's not like you can do that in an elearning system. So, again, it's not the best combination, and that's because the model is not too, uh, take a look. It's just g p t two. But just the fact that it's using relevantly, uh, reasonable words, for example, it uses predict or system or elearning, those are all things around machine learning. Now if I change this to a more complex, um, people's sense, let's say, uh, in quantum computing, we let's see what it answers. In quantum computing, we can take an equation and generate equation with the result, and you can divide by the number of different elements in the equation. Again, it's it's not, um, it's not a useful continuation, but still it's using some, um, kind of relevant, um, terminologies or keywords. So, again, all I wanted to show here is that the base model is capable. It has implicit knowledge of the word because it has the Internet data learned from it. It has seen lots of different domains and areas, and it knows how different words and terminologies appear or colloquial, uh, after each other in different domains. But the model is not really good at at answering questions. For example, if I change this to, uh, something like um, like, how is the weather?

ByteByte
22:42:24

Correct. And we'll use the same sentence. I enjoyed working with tutor, um, and then use copy sampling with, uh, some optimal parameters. And then we'd run it to see what the output is. And remember, copy is the best sampling method that is available and mostly used in elements these days. So the output is enjoy walking with my cute dog. My wife, and my friend. They always have me covered with a blanket and so So the sentence is basically just a continuation of what we, um, provided as the input. Now, um, also remember that pre training stage, we we we previously said that it's a very expensive stage, and it's most important part because it's not from Internet data. So this base model, even though it's just a, um, next token predictor or just continues the inputs, it is a still very powerful model. It's basically it has all the implicit knowledge that it could have seen on the Internet. So what that means is that if we change it to, for instance, like, machine learning because it will continue this. But now it will continue and use words and terminologies that are very relevant to machine learning and why we like one. So what that means is that it has some implicit knowledge about machine learning, what our typical words, um, how they follow each other, and so on. It can make this model very powerful. So it says I like machine learning because it's easier to understand, and you can use it to predict what people are going to say. Um, it's not like you can do that in an elearning system. So again, it's not the best continuation. And that's because the model is not too, um, capable. It's just g p two. But just the fact that it's using relevantly, um, reasonable words, for example, it uses predict, or system or elearning, those are all things around machine learning. Now if I change this to a more complex, um, equal sentence, let's say, uh, in, um, quantum computing, we let's see why it answers. In quantum computing, we can take an equation and generate an equation with the result that we can divide by the number of different elements in the equation. Again it's it's not, um, it's not a useful continuation, but still it's using some, um, kind of relevant um, terminology or keywords. So again, all I wanted to show here is that the base model is it has implicit knowledge of the word because it has seen internal data and it has learned from has seen lots of different domains and areas, and it knows how different words and terminologies are accurate or colloquy, um, after each other in different domains. But the model is not really good at at answering questions. For example, if I change this to, uh, something like um, like, how is David there?

You
22:43:59

It's going to probably just how is the better better is one of the most common problems for people who are tired and stressed. So it's not really answering my question just because it's the baseline. Now we also discussed that more Jacobite models are bigger. They have more parameters. Have billions of parameters. For instance, g p three had, uh, 175,000,000,000 parameters. And then more recent models like LAMA, the base model has four hundred five four four zero five billion parameters, so it's not even practical to load it on a on a single machine. And I'm not able to load that here locally and show you how it responds. But there are different services, online services that you can use to load those models and, um, interact with them. And they can terminate examples. One example is hyperpoly. And I just wanna show that if I switch GPT two to something more capable and bigger, how the response would be different. So here we can, uh, select the model. I have selected lemmas 3.145 b base, and name is clear what it means. LAMA is basically meta's LMM. Point one is the version of one of their recent, uh, models. Four zero five b showing the number of parameters. It has 400,000,000,000 parameters. And this means that this model the outcome of the pre training data. So this is the base model. It just continues the sentences. It's not going to answer questions. Now, like, this is also the max of tens. I'm going to reduce this, um, to 60. And then you'll keep this default. These are top key temperature. These are the hyperparameters to control the text generation process. And what I'm going to enter here so in the notebook, we saw that for something like I like machine learning because doubt was not really too useful. Because it's easy to understand, and I don't have to worry about that, he says. And this is just because the model is not too It's not good. Now here, if I enter the exact same prompt, I like machine learning, because let's see what, um, lemma would, um, respond. I like machine learning because it's a set of tools that can be applied by a variety of problems. You can use machine learning to predict

ByteByte
22:44:42

It's going to probably just have is a better better is one of the most common problems for people who are tired and stressed. So it's not really answering my question just because it's a base model. Now also discussed that more checkable models are bigger. They have more parameters. They have billions of parameters. For instance, g p three had, uh, 175,000,000,000 parameters. And then more recent models like LAMA, the base model has 405 four four zero five billion parameters. So it's not even practical to load it on a on a single machine. And I'm not able to load that here locally and show you how it responds. But there are different services, online services that we can use to load those models and, um, interact with them. And there are many examples. One example is hyperbolic. And just wanna show that if I switch g p t two to something more capable and bigger, how the response would be different. So here we can up select the model. I have selected NAMA 3.1405 b base. And name is clear. What it means, lemmah is basically meant as, uh, LML. Point one is the version, one of the recent, uh, models. Four zero five b showing their number of parameters as 400,000,000,000 parameters. And basis means that model is the outcome of the pre trained unit test. So this is the base model. We just continue the sentences. It's not going to answer. Questions. Now let this is also the max locator. I'm going to reduce this, um, to 60. Then we click this default. These are top heat and pressure. These are the hyperparameters to control the text generation process. And what I'm going to enter here So in the notebook we saw that for something like I like machine learning because Talbot was not really too useful. Because it's easy to understand and I don't have to worry about that, he says. And this is just because the model is not It is not good enough. Now here, if I enter the exact same prompt, I like machine learning because let's see what, um, LAMA would, um, respond. I like mentioning because it's a set of tools that can be applied to a variety of problems. Learning to predict the price of a house or the probability that someone will click on an ad. Ad. You can also use it to identify faces and so on. So again, as you see, now the continuation is a lot more meaningful and relevant to the input tenants. So that's why beer models are base models basically has an implicit knowledge about the word. Because they are really good at understanding text and continuing in a relevancy. So now if I also change it in the other example in quantum computing v, see how it responds. We are going to construct a circuit perform a particular operation. One way to do this is to design circuit directly Again, I'm not an expert in quantum computing, but when I read this, it makes sense. So, again, the model seems to be doing well. And then the last example is I want to now, um, enter a a question. For example, I want to see, um, I want to write what

You
22:44:42

probability that someone will click on an ad. You can also use it to identify faces and so on. So, again, as you can see, now the continuation is a lot more meaningful. And also relevant to the input sentence. So that's why bigger models are base models, basically, has an implicit knowledge about the word because they are really good at understanding text and continuing in in a relevant way. So now if I also change it in other example in quantum computing, we let's see how it responds. We often want to construct a circuit to perform a particular operation. One way to do this to design a circuit directly. Again, I'm not an expert in quantum computing, but when I read this, it makes sense. So, again, the model seems to be doing well. And then the last example is I want to know, um, enter a a question. For example, I want to see, um, I want to write what

ByteByte
22:44:50

um, what is the best way to learn machine learning? Originally appearing, uh, appeared on Quora, the another chain networker. Again, it's just

ByteByte
22:45:49

continuing. It's not really answering my questions. So now back to the lecture. Default is self post training as we mentioned. Is that to adapt this base model which is very powerful and has increasing knowledge. To adapt it. So it answer questions instead of continuing. And we will start with the first step, which is supervised by tuning. So again, we have supervised point tuning as the first step. Also sometimes it's called instruction fine tuning because you're fine fine tuning the base model so it follows instructions. And the goal of this stage is to, um, adapt the model from a completion model to following instructions. So for example, if from the base model, they ask them a question like, I want to learn ML. What should I The model may continue with things like, um, with more questions maybe, um, something like is it given easy? I'm so confident as one. But after this SFTS stage, you'll get the SFP model, And then, um, if we ask the same question, it would actually answer the question. It would say something like, take Android scores on Courser. Now just out of curiosity, let me ask the exact same question from lemmatry. And see if it really answer if it really adds more question to it. Or just answered it.

You
22:45:49

what is the best way to learn machine learning. Originally appearing, uh, appeared on Quora, the knowledge sharing network pair it's just continuing. It's not really answering my questions. So now back to the lecture. The focus of post training, as we mentioned, is that to adapt the space model, which is very powerful and has increased knowledge. Adapt it so it can support instead of continuing. And we will start with the first step, which is supervised finding it. So, again, we have supervised fine tuning as the first step. It's also sometimes it's called instruction fine tuning because we are fine fine tuning the base model, so it follows instructions. The goal of this stage is to, um, adapt the model from a completion model to following instructions. So for example, if from the base model, we ask them a question like, I want to learn ML. What should I do? The model may continue with things like, uh, more questions maybe. Like, is it even easy? I'm not so confident as one. But after this SFTS state, you get the SFT model. And then, um, if we ask the same question, it would actually answer. The question. It would say something like take Andrei's course on Coursera. Now just out of curiosity, let me ask the exact same question from Gamma three. And see if it really answer if it really adds more question to it. Or just answers it.

You
22:45:51

Um, I don't know what happened. Let me try again.

ByteByte
22:45:53

Um, I don't know what happened. Let me try again.

ByteByte
22:50:45

Maybe maybe it's down. We we can try later, but, um, I'm sure it's going to answer. Going to continue with more maybe questions or it's not for sure gonna answer it. So back here. In order to complete this SFE, we'll follow the same process as we discussed in the pretrained message. First, we will prepare data, and then we'll, um, train it. So let's just start with the data preparation. The data preparation is basically the goal here is instead of having the random text sampled from Internet, you'll have uh, data in a particular format because now we want to show to the model examples of prompt and response or questions and responses. So basically, we have to, um, curate, uh, a dataset that has this format. For example, it has prompts with some special token. And then it has the prompt here, if three tips for staying healthy. And then special token for response and the actual response. Response, um, with three tips. Now if you have a lot of examples in this format, then we will use it to continue training the base model and use data. So this data is also called demonstration data because basically we are demonstrating to the model that follow this order and from this format and follow this format. Now are different datasets, and there are different examples of this, um, dataset. But main thing is that this data has to be curated manually. In practice, it's it's it's often, um, usual to some experts or annotators and then ask them to create these pairs. So one example, for instance, is, um, Alkaqa dataset just to get a sense of how the data look like. I have the link open here, and this is the data. It's basically very similar to the pre training data, but now the difference is that it has a instruction column and output column. So these are things like deep three tips for stanza three and output. And then there are more examples like, um, how can we reduce air pollution describe a time when you have to make a difficult decision. So again, it's basically a bunch of questions. And ans responses. Now, um, this is AltaCorp. It's available online. It's, uh, open source. But there are also some other datasets. For example, OpenAI has instruct GPT dataset. So what happened was that they trained GPT two and a later GPT three but both of these models were only pretrained. So they were not, uh, doing any post training on those models. And then later, they published a paper, this paper, training language models to follow instructions with human feedback. And what they did was they continue continue training g t three model. And running post training. So, basically, they run they run SFT and then reinforcement learning on the g t three. And then the outcome of that model the outcome of this process was that, uh, we basically, uh, fine tune model. And that model, they call it, I think, I don't know, GPT 3.5 or Instructure GPT t. And also, this paper was released on, um, in March 22. And then chat g p t was released and launched a few months later. So I believe that probably this model was used and then probably some additional optimizations was happening on top of that. That to use as their initial model, you know, 14,500 examples of such pairs. And again, there are a lot other examples that are open source now, like Alpaca, um, early plan by Google. And all of instruction or demonstration data usually ranges between, um, uh, uh, like, tens of thousands or maybe hundreds of thousands of examples. So it's a lot smaller than the pretrained data, which is the entire Internet. However, the quality is very high. The pretrained data, it's just all the data on the Internet. And many models, even after we clean them, might still be noisy and not too helpful. But this data seems it's, um, carefully curated by experts. It's usually very high quality. And, um, again, nowadays, if you search in phase four instruction tuning datasets, there are lots of examples, and some of these are for different datasets. For example, this has named, you know, robots. And I'm sure some of these are, um, focused on math and mathematics, math instruct and so on. So a lot of examples. Um, some of these can be used to just post train, uh, to just run SFT and experiment and see how the model learns from these examples. This is data preparation. Basically, it's going to be, uh, thousands of, uh, these examples, prompt and response. Now after this dataset is prepared, the next step is run training. Training is, um, very simple because it's identical to what you saw in the pretraining. We don't have to change a single line of the algorithm and, uh, anything. So all we need to do is just replace the pre trained data, which is, um, SFD data or demonstration data. And then we continue training the base model. And then after, uh, enough iterations, we would get the SFT model. So everything is identical to the pretraining, uh, passage. And that means that we'll use the same plus function. We'll use the same optimization. And then build a sample from the demonstration data. It seems like what is the capital of France, and then we ask the model to predict, uh, next occurrence. So this way, basically, the model would learn from the SFT data. To, uh, answer questions. So that's training. It's very straightforward. We don't have to change it in the line. And after training the outcome of SFD, stage is the SFD model. And the SFD model will not answer So if you ask, I want to learn a mails, what should I do? It would re answer, uh, something like k and ring scores on Coursera. So we talked about SFDS stage. Now this model is not still ready to be deployed. And we'll talk in the next lecture why this model is not ready and why you have to have another stage, and that's called reinforcement learning. To make this model ready for deployment.

You
22:50:56

Alright. Maybe maybe it's sound. We we can try later, but, um, sure it's going to answer. It's going to come continue with more maybe questions or it's not for sure gonna answer. So, Akil, in order to complete this SFTP, we'll follow the same process as we discussed with the training usage. First, we will pick our data, and then we'll, um, train it. So let's just start with the data preparation. The data preparation is basically the goal here is that instead of having the random text sample from Internet, you'll have, um, data in a particular format. Because now we want to show the model examples of prompt and responses or questions and responses. So basically, we have to, um, curate, uh, a dataset that has this format. For example, it has pump with some special token, and then it has a pump here. If three tips for staying healthy, and then special token for response, and the actual response, um, the three tips. Now if we have a lot of examples in this format, then you will use it to continue training the base model on this data. So this data is also called demonstration data because, basically, we are demonstrating to the model that follow this formula, learn from this format, and follow this format. Now there are different datasets and there are different examples of this, um, dataset. The main thing is that this data has to be curated manually. In practice, it's it's it's often, uh, usual to hire some experts or annotators and then ask them to create these papers. So one example, for instance, is, um, albaca dataset, just to get a sense of how the data look like. I have the link open here, and this is the date. It's basically very similar to the training data, but now the difference is that is that it has a construction column and output column. So these are things like if three ticks were staying healthy and the output Um, and then there are more examples like, um, how can we reduce air pollution, describe a time when you had to make a difficult decision. So again, it's basically a bunch of questions. And ask responses. Now, um, is. It's available online. It's open source. But are also some other datas. For example, OpenAI has instruct GPT datas. So what happened was that they chose GPT two and then later GPT three, both of these models were only pre trained. So they were not, uh, doing any post training on those models. And then later, they published a paper, this paper, training language models to follow instructions with human feedback. And what they did was they continue continue training GPT three model and running post training. So basically, they run they run SFT and then reinforcement learning on the GPT three. Then the outcome of that model the outcome of this process was that basically, the fine tune model. And that model, they called it, I think, 3.5 or in stride GPT. And also, this paper was released on um, in March 22, and then short GPT was released and launched a few months later. So I believe that probably this model was used then probably some additional optimizations was happening on top of that to use as their initial model. On Chaggyity UI. So, um, again, this paper is an interesting read, and what I want to, um, focus on is their dataset. So they hire experts, and then they manually create those pairs of response, uh, prompts and responses, and then they call it instructivity dataset. And then they do not release this dataset and it's not open source, Uh, but it has around 14,500 examples of such games. And, again, there are a lot other examples that are open source now, like Hotpepper, Um, Yolked, SLAN by Google. And all of these instruction or demonstration data usually ranges between, um, like, tens of thousands or maybe hundreds of thousands of examples. So it's a lot smaller than the pretraining deck, which is the entire Internet. However, the quality is very high. The pretraining data, it's just all the data on the Internet, and many of those even after you clean them might still be noisy and not too helpful. But this data, since it's carefully curated by experts, it's usually very high quality. And, um, again, nowadays, if you search Python face for instruction training datasets, there are lots of examples. And some of these are for different purposes. For example, this has name, no robots, and I'm sure some of these are, um, more focused on math and mathematics. Math instruction and so on. So lots of examples. Um, some of these can be used to just post your to just run SFD and experiment and see how model learns from these examples. So this is data preparation. Basically, it's going to be, uh, thousands of uh, these examples, round time response. Now after this dataset is prepared, the next step is running training. Training is, um, simple because it's identical to what is in the pretraining. You don't have to change a single line of the algorithm and the, um, anything. So all you need to do is just replace the pretraining data with this, um, SFD or demonstration data. Then we continue training the base model. And then after, uh, in operations, we would get the SFD money. So everything is identical to the pretraining, uh, stage. And that means that we'll use the same plus function. We'll use the same optimization, uh, optimization. And then we just sample from the demonstration data. It things like what is the capital of France, and then we ask the model to predict the next tokens. So this way, basically, the model would learn from the SFC data. To, uh, answer questions. So that's training. It's very, uh, straightforward. We don't have to change it single line. And then after training, the outcome of SFD stage is the SFD model. And the SFD model would not answer questions. So if you what should I do, you would really answer, uh, think something like take Andrei's course on Coursera. So we talked about SFG stage. Now this model is not still ready to be deployed, and we'll talk in the next lecture why model is not ready and why we have to have another stage, and that's called reinforcement learning to make this model ready for deployment. Hey. We talked about the SFTSH. And in this lecture, we'll switch to the second step of post training, which is reinforcement learning.

