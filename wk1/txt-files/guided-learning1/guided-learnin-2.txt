For example, here, I have, uh, some other datasets. For example, we have c four, GoldMine, refine that find that. So what these are? Are they simply different companies or or or researchers? You will see them in a bit. Um, they clean the one data, and then the outcome is just a dataset, and they name it something. For example, they name it c four. For example, if they say c four, you would see it under, um, tensorflow..org. You would see the description of the c four dataset, and it's it's, um, created by Google. So the description is this is a a basically clean version of Common Clouds. Of vector crawls. So and there are more details to it, and you can you can just read them through and see know, how big the data is, for example, how many examples are there in the training suite and so on. But what they are, basically they are the clean version of open crawl of Common crawl. And then this is an example of c four. It's published under, um, it's AI two. And what what this is is basically how this data looks like. This is not exactly the original c four build. Published. It's a clean version of c four again. So c four clean common cloud. And this is here, you can see it's a clean version of common call. Um, and then this is the process version of g l c four dataset. But again, the whole point is that and you can see the sizes here, like, uh, 305 gigabytes, uh, English data. And the non clean is 2.3 terabytes and so on. But basically the data is it's very similar. C four and the here and also the origin on c four. After cleaning, it's going to be looking like this. It's a table, and we have some text here, one column is text. One column is the URL that this text was extracted from. And you can see here, for instance, this URL had this text. And then there are also more, um, examples. Different URLs, different, um, URLs, different links. And some of these are in different languages. And, um, so this is c four after cleaning Common Problems. Now this dataset is relatively old, but it was very important in earlier days of elements because a lot of elements are using this dataset to to start. From, um, to as a starting point to to pre train their Eleum. But then there were also other more recent examples and data pie

You
21:07:16

Hey. In the last picture, we saw various chatbots and elements. And then we discussed the two main stages of training elements. The stage one is pre training, and the stage two is post training. In this lecture, we are going to focus on the pretraining stage. And in particular, we are going to talk about the Internet data. Does it mean to train something on Internet data? How should we collect this data, and how to prepare this data? So let's start. So data preparation has three main steps. Step one is crawling data. So crawling is simply a software that starts from some CDRs or just a single ACR, and then it tries to extract its content and identify the outgoing links within that URL, and then it goes to those outgoing links. And there is a loop where it keeps doing this. It keeps downloading the content, identifying the outgoing wins various URLs, and then visiting those URLs. It keeps doing this until it visits majority of the Internet. So just to better understand how web crawling works, it's I have asked Chagibati to write a simple decoder class if I don't. And I also inspected your kid's code. It's it seems perfect. And before talking about the code, purpose here is not to have a production ready software. The purpose is just to understand the logics and understand what key pieces in a bit color. Because once we learn that, most of the advanced bit colors are all based on the same logic. So, again, any of the product is just a software here we have a simple class, uh, simple web crawler. And what it does is it starts from a page URL, and this is given as the input to this class. And then it also keeps a a set Python set. Time time, named visit. And this visit that is basically keeps track of URLs that this web crawler already discovered, extracted their content, and it's not, um, And then we have this method called prod, and this is the main call the main function the main functionality of this pass. What it does is it starts from this to visit, which is basically a button list and it adds the base URL as a first URL to start exploring it. But then this this list is basically a way to allow the software to keep adding new links as it discovers them so it can later explore. And, again, this whole thing is based on a for loop or while loop And then in the while loop here, we have as long as there are links, unexplored links in the two visits, meaning that there are URLs that you have not we have not explored this content yet. Uh, we extract the URL, If it's previously seen or explored, we state it. Otherwise, we start the product. And here, there's some libraries like request Again, we don't want the focus is not to learn the details of these libraries. There are just lots of different libraries. We can send requests. To different URLs and get the output. And here, we are using this request that did send the URL, get a response back. Then from this response, there is this text, which is the HTML content of that URL. Now, again, Beautiful Soup is just one library under other libraries, but all these libraries are allowing us to once we have the responses, the HTML content, it allows us here to just walk, uh, to go through all the URL links within that HTML content. Here, basically, it says find all, uh, these tags. And then those are links. And then if it fills a full link, full URL, because some of these might be, relative URLs. And then once we have that, it just happens them to the two of these So basically, two visit is just a list that it keeps track of all the discovered URLs that we have not still explored its content, HTML content. And then there is a wild loop back that it continuously tries to, uh, explore unseen, uh, URLs in the tool. And once we are done, it just it just gets added to the visitor. So very simple. So this is the URL again in practice for more details or more arguments, uh, like, maximum number of times to, um, to go deep and keep identifying links and going to those links. But those are details. The purpose is just to, for now, understand the high level logic of building a good crawler. Now back to the lecture, there are two ways now we can plot the Internet. The first option is to crawl our schools, and this is what most big companies like OpenAir and Entropic does. For example, just to validate that Antropic does product, there's this question on and it's on the Antropic website. At whether entropy crawl data from the web. And the short answer, if you read this, it says yes. We do crawl live. And this is also GPT two paper published by. And we're going to talk, uh, in detail about different parts of this g p t two model later in this week. But for now, I just wanna scroll down and get to the training data. Here, training dataset. And if you read this, here, you're saying that in we created a new web scrape, which emphasizes document quality. So, basically, they are also having their own web product. So they have this software, they written that, and then they're using that to call the dev. So this is first option. The second option is to use a public repository of cloud pages. Now there are different research teams and, uh, organizations that regularly crawl the Internet and provide the extracted content public and make it publicly available. And one very common example is CommonCrawl. So I Google CommonCrawl here, and it says if CommonCrawl is profit organization that crawls the debt and freely provides its hard times and data sets to the public. And there's just this website for common crawl. If you go there, um, it's it's how the website look like. We can go to the data. There's this latest call. And, you know, instructions to get started, and here we can choose a call. And it seems that the crawls are posted on history. So here, we can just choose crawl again. There are various options because they regularly crawl under different, um, times. So it's 24 crawls, 23, 22, and goes all the way back to, I think, when since they started 2008. So any of these, once we select, it would just give us some instructions and more details about their Chrome. Now back to the lecture. Most companies prefer to write their own um, product if they're big, so it gives them more flexibility. And for fast experiments for startups, for smaller companies, they prefer to start from, uh, publicly available cloud pages just because it it allows them to iterate faster. And these are some interesting statistics from common crawl. Uh, they crawled in the web since 2007, and then each time the craw is approximately 2,700,000,000 web pages. And as a result of that, they're around um, it's around 200 to four hundred hundred terabytes of HTML text content each time that they crawl the Internet. And then they release a new crawler every month for, uh, two months. Alright. So this is the step one of data compression, calling the Internet, or in other words, it's collecting data. Now after we collect this data, it's the the output of that is HTML text one. Now the second step, which is also very important, is data clean. The mock HTML content has lots of issues. For example, here, I have a screenshot. If I zoom in, to see how this one it'll look like, you can see the HTML text content has lots of, um, irrelevant uh, information. For example, there are lots of tags, HTML, uh, tags, and markdowns, and attributes, and many of these are not really useful to to train a model. Uh, we don't at least for most use cases, we don't want an element to learn about all these, uh, tags and hit HTML, all these things. What we want is the element to learn from the actual content that we see from your screen. The content that we see in the browser. And those are often, um, within certain tags, like h one tags, and here we have the, you know, ping tags. Like, BISTEX, for example, the new MacBook Pro and four features, the latest. These are some of the text data that we really want to extract from this, uh, HTML content and use them for relevant training. So that is the the main goal of Datapin. It's to extract useful information from the raw HTML content But that's not the only, um, the step to clean the data. There are also other issues. In the raw HTML content. For example, a lot of text is, um, duplicated across the Internet. Can think of when there is an important news. Um, there are lots of various websites that publish the exact same news or it is like rewarding of the, uh, same news or original article. And we typically don't want to have lots of duplicated data in our training data because if we expose the telemetry data and LMM see them over and over again, at some point, you may memorize it. So we don't really want LMM to memorize, uh, news. What we want is to learn about different words, different, uh, topics and knowledge. So these are some of the things that are important to take care of during the data cleanup. Again, there are other other things. For example, one important thing is to make sure that we use only useful and safe content. There are lots of websites that we may not want our element to learn from. As part of data cleaning, it's important to just take rid of those websites and those text content. Now there are different companies and researchers that they've tried various phase of cleaning, um, uh, raw HTML content or comment prompt. And then after they apply those cleaning filters or the steps, then they get the clean text, and then they name it something. For example, here, I have, uh, some popular datasets. For example, they have c four, go to find it find it. So what these are are basically different companies or or or researchers. We will see them in a in a bit. Um, they clean the raw data, and then the outcome is just a data set that they name in For example, they name it c four. For example, if we search c four, you would see under, um, tensorflow..rg, you would see the description of the c four data set, and it's it's, uh, created by Google. So the description is this is a, basically, a clean version of CommonCloud. Background. So and then there are more details to it, and you can you can just read them through and see how big the data is. For example, how many examples are there in the training screen and so on. But what they are is basically they are the clean version of open pro or CommonPro. And then this is an example of c four. It's published under, um, it's AI two. And what what this is is basically how this data looks like. This is not exactly the original c four that Google published. It's a clean version of c four again. So c four clean comment run, and this is here you can see it's a clean version of common runs. Um, and then this is the process version of Google c four data. But again, the whole point is that and you can see the sizes here, like, uh, 305 gigabytes. Of English data, and the non clean is 2.3 terabytes as well. But, basically, the data is it's very similar. C four and the year and also the original c four. After cleaning, it's going to be looking like It's a table, and we have some text here. One column is text, and then one column is the URL that text was extracted from. And you can see here, for instance, this URL had this text. And then there are also more, um, examples, different URLs, different, um, URLs, different links. And some of these are in different languages. And, um, so this is before after cleaning common call. Now this dataset is relatively old, but it was very important in career days of LMS because a lot of elements were using this dataset to to start from, um, to as a starting point to to pre train their element. But then there were also other more recent examples, and data pipelines for cleaning common problem. One recent example is. I have the paper here, um, and it says it's an open corpus of 3,000,000,000,000 tokens for language model pre training research. And, um, we'll talk more about tokens in in future lectures. Now, we can just think of tokens as words or subwords. So what that means is that this after cleaning the the code, this total amount dataset has around 3,000,000,000,000 words. English words, let's say, or some words. So this is the scale of the clinic. And then you can also take a look at the paper. It's very interesting, uh, the statistics here. Um, for example, they're not only starting from Common Core, but they're using other sources because all these sources can ultimately be useful for the LMM platform. For example, Deepak has lots of code, um, ready some other places with the video. And then these are some, uh, numbers and, you know, how big the data is and so on. And then you can just read, uh, your pipeline and how they clean this dataset and what what specific filtering steps they apply to clean the You can see they have the quality filtering, content filtering as well. And there's also another very popular dataset. It's called RefinedNet. And, again, the same story. You can you can see the stats here and how big these data are. They are basically just a sequence of filtering steps and cleaning steps to clean, um, raw Internet data or common raw.

You
21:09:56

Now back to our, um, lecture. Beside these three examples, c four will now be finder. So the last one that I wanna share is finder. And finder is, uh, more recent, and it's openly available. It's published by patting face, and it's basically just a a data cleaning pipeline. It you can see all the details of the pipeline. And I have this this screenshot in the lecture as well. It's, um, these are the key steps of, um, cleaning coming up. And again, now most of these should be, uh, familiar. Example, the first step is URL filtering, and this is step a trigger URLs that they are not interested and then pull them from those content. And there's typically a list of lot of these content here. You can see what find the it, uh, used to lot to filter the URLs, and these are some of the categories. For instance, they, uh, they filter URLs that are logged into adult category, and there are some other categories here. And these are pretty, um, subjective. Different companies may want to include or exclude certain, uh, categories. But, again, this is the first, uh, step of cleaning the data. So the first get rid of all the orders that they are not interested to learn from. And then after that, they extract the actual text from the raw HTML content. This is what we saw earlier that the what we want is the actual text content within certain tags. And then after that, we have language filtering here. Filter text data from certain languages that we don't want the LLM to necessarily support. And then there are a bunch of other filtering and duplication here. It tries to duplicate, uh, very similar content. More filters here, and then finally, TIR renewal here, they just, uh, remove content with sensitive information, like bank account, bank numbers, and phone numbers, and those kind of things. And then after they apply all these, uh, sequence of steps to the uh, raw data, they end up with a clean data, which has, uh, 44 terabytes of disk space. This is the storage needed to just store this final clean data. And then approximately, it ends up with 15,000,000,000,000 tokens. We'll talk about tokens later. But for now, we can just think of those as ports or sub ports. So it's around 15,000,000,000,000, um, ports. And this is this is basically what most, um, what data cleaning cleaning is to be. We apply a certain, um, sequence of apply a certain sequence of, um, filters and cleaning steps to go from the raw HTML content to a clean And this screen text is basically just a very huge expander. Or it can be multiple text files. But, again, it's just huge text file that just we have this entire text data from Internet. Just to get an idea, this is the um, finder data after the cleaning, so this is how it looks like. Again, very similar to what we saw earlier. There's this huge table with text column and URL column. You can see from this URL, this text came. And this is the data. This is the final clean data that we want to use. To train out of it. And this is just a strong Internet. And it's very random. Like, did you know um, you have to be the yellow, nine volt, and so on? And if you see another example, five reasons I love Boston. Basically, it's like, uh, very long text from huge range of, um, topics and areas. And domains that we end up having in this context.

You
21:11:16

Back to our lecture, we've covered data cleaning, and it's a, uh, popular pipelines to print data. The next step of data preparation is tokenization. Um, the purpose of organization is to go from the raw clean data to a long sequence of discrete numbers. And this step is also quite important because machine learning models do not expect, um, text drawings. What they expect is numerical inputs. So in this step, we follow a certain procedure or algorithm named tokenization to convert text to sequence of numbers. And that sequence of numbers is now useful for the machine learning model or the algorithm to learn from. So in this lecture, we covered the key various steps of data preparation. We talked about crawling the Internet, and we saw that how we can start from some publicly available, um, crawl pages like CommonCrawl. Then we saw data cleaning and various pipelines and, um, how this raw HTML text content can be cleaned using a sequence of features and, um, algorithms. And then we also saw certain datasets that followed these this recipe and, uh, pipeline to get a clean data. So you don't have to to start from you don't have to necessarily write our own cleaning pipeline. We can always start from an open source clean data, so just find it, and you start from there. Then finally, we would tokenize the data to go from the vertex to the launch sequence of these three numbers. So this is the high level of data preparation. In the next lecture, we would talk more about tokenization, and we understand how it works in detail.

ByteByte
21:11:16

coming from. One recent example is Twilmore. I have the paper here. Um, Twilma. And it says it's an open corpus of 3,000,000,000,000 tokens. For language model pretraining research. And, um, we'll talk more about tokens. In in future lectures. For now, we can just think of tokens as words or subwords. So what that means is that this after cleaning the, uh, web crawd, this dataset has around three three million words, English words, let's say, or sub words. So this is the scale of the clean data. And then you can also take a look at the paper. Very interesting, uh, the statistics here. Um, for example, they're not only starting from common crowd, but they're using other sources because all these sources can ultimately be useful for the element to learn from. For example, GitHub has lots of Um, Reddit and some other places like Wikipedia. And then these are some some numbers and, you know, how big the data is and so on. And then you can just create, uh, their pipeline and how they clean this dataset and what what specific filtering steps they apply to clean data. Can see they have a quality filtering, content filtering, and so on. And there is also another very popular dataset. It's called Refine Beb. And again, the same story. You can you can see the stacks here and how big these data are. They are It's basically just a sequence of completing the steps and cleaning the steps to clean, um, raw Internet data or coming from. Now back to our um, lecture. So beside these three examples, c four does not define it. So the last one that I want to share is find that. And find that is, uh, more recent and it's openly available. It's published by Hugging Face, and it's basically just a a data cleaning pipeline. Bit here, you can see all the details of the pipeline. And I have this this screenshot in the lecture as well. It's, um, these are the key steps of, um, putting time graph. And again, now most of these should be, um, familiar. For example, the first step is you are filtering in this set they filter URLs that they are not interested at the end to learn from those content. And there is typically a list of blog list content here. You can see what find that, uh, used to love to filter the URLs, and these are some of the categories For instance, they, uh, they filter URLs that are belonging to adult category, and there's some other categories here. And these are pretty, um, subjective. Companies may want to include or exclude certain, um, categories. But again, this is the first, uh, step of cleaning the data. So they first just get rid of all the orders that they are not interested to learn from. Then after that, they extract the actual text from the raw HTML content. This is what we saw earlier that the what we want is the actual text content within certain tags. And then after that, we have language return here. We filter text data from certain languages that we don't want the LLM to necessarily support. Then there are a bunch of other filtering and denitification here. It tries to reduplicate, uh, very similar content. Then more filters here. And then finally, PII ring wall here, they just uh, remove content with, uh, sensitive information like bank account, bank numbers, and phone numbers, and those kind of things. And then after they apply all these, uh, sequence of filtering steps to their um, raw data, they end up with, uh, the clean data, which has, uh, 44 terabytes of disk space. This is the storage needed to just store this final clean data. And then approximately, it ends up with 15,000,000,000,000 tokens. Again, we'll talk about tokens later. But for now, we can just think of those as boards or sub boards. So it's around 15,000,000,000,000, um, words. And this is this is basically what most, um, what data cleaning data cleaning is. We apply a certain, um, sequence of we apply a certain sequence of, um, filters and clean steps to go from the raw HTML content to a green text. And this green text is basically just a a very huge text file or it can be multiple text files. But again, it's just huge text that just we have this entire text data from Internet. Just to get an idea, this is the um, find out data after the clinic, so this is how to collect. And very similar to what is earlier. There is this huge table with text column and URL column. You can see from this URL, this text came. And this is the data. This is the final clean data that we want to use to train our data. And this is just like from eternity. It can be it's very random. Like, did you know um, you have two yellow, yellow, nine words, and so on? And if you see another example, five reasons I love Boston. So basically, it's like, uh, very long text from huge range of, um, topics and areas and domains. That we end up having in this screen text. Back to our lecture, we've covered data cleaning, and we saw, uh, popular pipelines to clean data. The next step of data preparation is tokenization. Um, the purpose of tokenization is to go from the raw clean data to a long sequence of discrete numbers. And this step is also quite important because machine learning models do not expect, um, textual inputs. What they expect is numerical inputs. So in this step, we follow a certain procedure or algorithm named tokenization to convert text to sequence of numbers. And that sequence of numbers is now useful for the machine learning model or the element to learn from. Now So in this lecture, we covered the key three steps of data preparation. We talked about crawling the Internet, and we saw that how we're gonna start from some publicly available, um, crawl pages like Common Crawl. And we saw data cleaning and various pipelines, and, um, how is raw HTML text content can be cleaned using a sequence of filters and, um, um, algorithms. And then we also saw certain datasets that followed this this recipe and, uh, pipeline to get the clean data. So we don't have to start from we don't have to necessarily write our own cleaning pipeline. We can always start from an open source clean data such as FindIt, and you start from there. And then finally, we would the data to go from the vertex to a long sequence of discrete numbers. So this is the high level of data preparation. In the next lecture, we would talk more about tokenization, and we understand how it works in detail.

You
21:11:37

Alright. In the last lecture, we learned about the three main steps of data compression, and we saw that the last step, which is also very important, is localization. In this step, we convert the objects into a long sequence of discrete numbers. In this picture, we are going to learn more about it, and we understand how it works under the hood. Hood. How can they really build something that can convert, uh, vertex into numbers. So let's start.

ByteByte
21:11:38

Alright. In the last lecture, we learned about the three main steps of data preparation, and we saw that the last step, which is also very important, is tokenization. In this step, we convert a vertex into long sequence of discrete numbers. In this lecture, we are going to learn more about it, and we understand how it works under the hood. How can we really build something that can convert, uh, raw text into numbers. So let's start.

ByteByte
21:18:16

So in organizations, we are dealing with two phases. The first phase is the training phase, and then the second phase which we have it here is the inference phase. In the training phase, the idea is to expose the tokenizer to this long sequence of text that we've already cleaned. And then we let it learn from it. Understand the statistics of different words and what are the unique words. The purpose of the training phase is to, um, apply these two um, squares that we have here. These are just two, um, algorithms or or logics. Then as a result of that, the tokenizer would end up with a vocabulary. So let's just talk about each of these in detail. The first step the first part is text splitting. So what happens here is that once we this long sequence of text is provided, this applies some logic. It's it's simply a logic to apply to this long sequence of text. To split it. Basically, the the goal here is to split the text into smaller units. And depending what on our logic, it can be, um, implemented in different ways. For example, here we have an example, uh, machine learning, ML is a soft field of artificial intelligence. And then after applying text splitting, we may end up with something like this. It's a list. Of smaller chunks or units of text. Example, we have machine learning, and then we have, like, smaller units and subbuilt artificial and and finally, we have that here. So this is text s s s p team. Now imagine we apply this logic on the entire cleaned Internet data. To get a very huge list of, um, smaller units of text. The next part is building the vocabulary. Now given this huge list, in this step, we create a vocabulary, alternate unique units of text. That we have in this list. And then we also assign an ID. So it's it's very simple. Building vocabulary is nothing but finding all the unique, um, units and refer to this, uh, each of these, uh, smaller unit suffix as tokens. So the building vocabulary is simply responsible for for finding all the unit tokens and just listing them in your table and also assigning an ID to them. Example, we can have a vocabulary like this. And then for tokens, we have a, about, after, all, also, and so on. And these are some of the tokens that we've seen in this list. After we split the the initial the original text. Then these are the IDs which starts from zero and they just increment it, uh, one by one. And here, you can be seen that at the end we have around 200, 7,000 uh, 131, uh, tokens. Depending on our training data, this long sequence of text, we may end up with different vocabularies with different it just depends on how many unique, um, tokens we would end up seeing after we split the text. So this is the training phase. This is we just run this one time on our training data to build this vocabulary. And then from there, the tokenizer is ready to be Basically, the tokenizer, it's just, um, it internally stores this vocabulary. And then anytime we pass a text to it, it can convert it to a sequence of numbers. And the way that it does is first, it runs this, um, text splitting logic It applies it to telemejock, and then it splits telemejock to, uh, the sequence of tokens. And then it would refer to its internal vocabulary and replace each token with its corresponding ID. So for example, tell me a joke might become tell me a joke after a text is blinking. And then after tokens are replaced by their IDs, we may end up with something like this. So this is how a tokenizer can go from any text to a sequence of numbers. In addition to that, it can also go back to an original text given a sequence of numbers. And this is also a very simple logic. So when we pass a of numbers like this to the tokenizer, internally, the tokenizer can also store an inverse table. So instead of mapping from tokens to IDs, it can have a mapping from IDs to tokens. So once this is provided to the tokenizer, and we want to tokenize it, meaning that we want to go from numbers to text, it just simply goes to those, uh, rules in the in the English table and find the corresponding token. And then it applies the inverse of text to to the tokens to output the original text. In this case, tell me a joke. So these are the two main phases of, um, tokenization. 