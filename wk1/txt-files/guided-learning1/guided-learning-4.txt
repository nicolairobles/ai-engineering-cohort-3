You
21:32:05

So for example, uh, is becoming a space r and three o, uh, three a. And then it's because it's a meaningful word, it has its own token. Different tokenizers. We can try different. This one is more, um, more recent version, published by train and published by OpenAir. Uh, for 200 k base. And then if you apply this would just probably see different numbers just because its vocabulary is different. Alright. And then there are a bunch of other models and open source models, so it's just an interesting thing. And there are different encodings and, uh, you know, we can also pick models, um, the actual models to see how input prompt gets translated into sequence of IDs. So this is the visualization of TikTok analyzer. Now the next thing I want to talk about is that we saw all these different algorithms, PPE, and how it works. But in practice, we we usually do not, uh, have to implement it from scratch. There are some great libraries that is already available and open source, and we can use them. And there are also free train tokenizers available. So don't have to even train tokenizers from scratch if if you don't have to. So, um, and one very popular library is TipToken, published by OpenAI. And this is TipToken, and, uh, this is the repo. And, again, you can see the the code and everything is here available. And it's also very simple to use. We can just specific basically, the description is token is a fast PP tokenizer, and they have some, uh, numbers here that why it's faster than other alternatives. It says that it's between three to six x faster than other open source organizers. So it's a very good, um, library to start from and, you know, use it for our organization purposes. And, also, it's very easy to use, um, just to show an example. I have my Jupyter notebook open here, and then I want to show how to use the token. So first thing is we just first install it using the SPP install T token. Then we can just easily import T token here and then get encoding from a certain model. For example, here I have the token dot encoding for model GPGA 3.5. And what that means is that keeping the tokenizer that we're used for training GPT 3.5 model. And then when I run this, I'll get this tokenizer object. It's really good to use. And you can see its name, uh, this so it's equivalent of what we saw in the visualization. This. So after they train, they organize around some training data. They just name it something. So this tokenizer is is ready to use, and it's very simple. To use it, we can just say, organizer that encode um, and then write a lot of machine learning. And this is the sequence you will get back. This should be equivalent to what again we saw earlier in the visualization. And now we can also decode, meaning that it can go from a sequence of text to the it's original sorry, the sequence of numbers with original text. So now if they pass something like this and call decode on it, I'm expecting that it would give us a lot of machine learning. Then we can just add new tokens to see what they are, and then it just keeps changing. So and then another thing I want to show is that we can have we can write in, uh, vocab. And then it would give us the vocabulary size. So this many tokens are available in this organizer. And now if I change this to something else, I can change it to GPT four, let's say, or four o. And, uh, beside that g t four o is the default g t default model can be go to chat g p t. So let's see what organizer it's using. Now if I print organizer, it's anywhere for 200 kb. And from the name, it is probably having a higher number of containers. It's probably having probably around 200. So let's see. Uh, let me also rerun this another machine learning to see if it's trying to output uh, the same numbers or not, which we do not expect to be the same. Uh, if I run this, um, it's the same. So at least for these tokens, it's the same. And then if I run this, I would get um,

ByteByte
21:32:09

So for example, uh, it's becoming the space uh, and three r. Uh, three a. And then it's because it's a meaningful word, it has its own token. And there are different organizers. We can try different. This one is more, um, more recent version. Published by trained by ONA. Uh, 200 k base. And then if you apply it, you would just probably see a different numbers and just because this vocabulary is different. Alright. And then there are a bunch of other models and open source models, so it's just an interesting thing. And different also encodings and, uh, you know, we can also pick models. Um, like, actual models to see how, okay, any good prompt gets translated into sequential IDs. So this is the the visualization of Deep Recognizer. Now the next thing I want to talk about is that we saw all these different algorithms, p p, and how it works. But in practice, we we usually do not, um, have to implement it from scratch. There are some great libraries that is already available and open source and we can use them. And there are also pre trained tokenizers available. So you don't have to even train tokenizers from scratch if if it you don't have to. So, um, and one very popular library is TikToken published by OpenAI. And this is TikToken and, um, this is the repo. And again, you can see the the code and everything is here available. And also very simple to use. Um, we can just specify basically, the description is Ticocan is a fast PPA tokenizer. And they have some, um, numbers here that why it's faster than other alternatives. So it says that it's between three to six x faster than other open source organizers. So it's a very good, um, to start from and, you know, use it for at organization purposes. And also it's very easy to use. Um, just to show an example, I have my Jupyter Notebook open here, and then I want to show how to use the token. So first thing is we just first install it using the SPE install the token. Then we can just easily import the token here and then get any coding from a certain model. Example, here I have tt token dot encoding for model g p t 3.5. And what that means is that give me the tokenizer that we're used for training GPT 3.5 model. And then when I run this I'll get this tokenizer object. It's ready to be used. And you can see it's named, uh, this. So it's equivalent of what we saw the visualization, this tokenizer. So after they train, they tokenize around some training data, they just named something. Now this tokenizer is is ready to use and it's very simple. To use it, we can just say tokenizer dot encode. Um, and then write a lot of machine learning. This is the sequence we will get back. And this should be equivalent to what AM we saw earlier in the visualization. And now we can also decode, meaning that we can go from a sequence of text to, um, its original sorry, a sequence of numbers to its original text. So now if we pass something like this and call decoderate, I'm expecting that it would give us a lot of machine learning. And then we can just add new tokens to see what they are, and then it just keeps changing. So and then another thing I want to show is that we can have we can write in, uh, vocab. And then it would give us the vocabulary size. So this many tokens are available in this organizer. And now if I change this to something else, I can change it to g p p four, let's say, or for whole. And, um, beside the g p t four o is the default g p, uh, default model when we go to chat g p t. So let's see what the mechanism is using. Now if I print tokenizer, it's anywhere of 200 k base. And from the name, it is probably having, uh, higher number of containers. It's probably having it's probably around 200 k. So let's see. Now let me also rerun this. I love machine learning to save if it's going to output, um, the same numbers or not, which we do not expect to be the same. Now if I run this, um, it's the same. So at least for these tokens, it's the same. And then if I run this, I would get, um,

You
21:35:53

let me oh, then it's not the same. So let me alright. So it's not the same as the g p three point is different than much the vocabulary IDs are different. Now if we just copy it here and decode it, you would get the original baseline. And now number of vocabulary is 200,019. So it's just pick a pick a vocabulary, and what that means is that it's just, uh, more advanced than, uh, more 50. So this is how easy it can be just to use organizers and start from some pre trained organizers to train our elements. Now back to our lecture, we talked about tokenizers, and we saw how training works. And after training, how a tokenizer stores its internally and vocabulary. We also discussed the three key categories of organizers for developer level and software level. Work work level is problematic because if we have, uh, one token for this word, our vocabulary would be huge. It's difficult to train on such a huge vocabulary. If we use character level vocabulary is a small, but the problem is that the sequence of tokens after these split the text can be very large And, also, this is going to be problematic when you train element because it has to learn all the dependencies between all different tokens. And if if they're, um, a very long sequence of tokens, it's going to be more expensive and difficult for the element to learn from. And then we discuss software developers, which is balances balances between these two. The vocabulary is more manageable, and the sequence length is also more manageable. There are some algorithms and saw that the most, uh, popular algorithm is probably BPD. And various elements, use some variations of BPD. Um, and then what the reports is that he starts with character. And then it tries to, um, it's basically merge mutual friends, merge tokens, and form mutual friends. And then this merging tokens is based on how like, what is the most frequent pair that they can just merge. And that's how they continue doing this, uh, and continue the algorithm until they reach a ratio for the vocabulary So and then we also saw some visualization and the deep token library and how easy it can be to just use some free chain to organize it. So that's all I wanted to cover for organizations. Then in the next lecture, we would start learning about, uh, model architecture and understand how elements look internally. Hello, everyone. Welcome back. In this lecture, we are going to focus on the model architecture. In the previous lectures, we talked about data preparation and wrap up, and we understood how we can prepare data and tokenizing. Focus of this lecture is to understand now how LMS look like internally. We'll start by revealing around the course and some important layers. And then after that, will talk about transformer and bipolar only transformer architecture. And finally, we will discuss why transformers are so powerful and widely used for gate generation. So let's start. So the first thing is, what is neural network? And before answering that, let's understand the goal of most machine learning models. Most machine learning models are basically trying to learn the mapping from some input to output. I have three examples here. And, um, if you look at them, in all of them, we want to learn a model to learn a map from input file. On the left, I have house price prediction. And then you can think of this problem as given some signals about the houses we want to predict the its price. Those signals can be anything, like location, number of bedrooms, and, uh, house size, and those kind of things. And then somehow, they just convert that to numbers. Their various space doing code is is, uh, signals into numbers, and that's not the main focus of, uh, this lecture. But, um, let's assume that there's some ways to unregister numbers. Then price is also a number. So we can think of the input as which has all these, uh, x one, x two, and x n for different signals. And then the output is y, which is price. We want the machine learning model to learn a mapping from x to y. So look at all the training data of x and y, learn from it. And then when we give a new exploit, should be able to predict y. So it's essentially learning this mapping, this arrow here that I have to go from any x to y. Y. Now in another example, for instance, And the problem here is given an email, we want a model to classify it as a spam or not not a spam. Spam. So we can think of, you know, useful CNOTs, like send their content, subject of email, and so on. And then these gets converted into numbers as the first step. So we can think of the input as x, uh, and then it has all these x one, x one, x And then the output is basically expected to be whether it's a spam or not. So y can be basically one if the email is a spam, and it can be zero if the email is not a spam. So the goal of the machine learning model here is to learn a mapping from x to y. So, again, while it's a different, um, problem,

ByteByte
21:38:08

let me oh, then it's not the same. So let me alright. So it's not the same as the p p two three point Organizer is different and much the vocabulary IDs are different. Now if we just copy it here and decode it, we would get the original text back. And now number of vocabularies is 200,019. So it's just video video vocabulary and what that means is that it's just, uh, more advanced and, uh, more effective. So this is how easy it can be just to use organizers. Start from some pre trained organizers to train our evidence. Now back our lecture, we talked about tokenizers and we saw how training works. And after training, how the tokenizer stores its internal vocabulary. Also discussed the three key categories of organizers, four level, archival, and software level. What what level is problematic because if we have, uh, one token for each work, how which category would be each. And it's difficult to train on such a huge vocabulary. If we use character level vocabulary is small, but the problem is that the the sequence of tokens after we split the text can be very large. And, also, this is going to be problematic when we train other than because it to learn all the dependencies between all these different tokens. And if if they're, um, very expensive and difficult for the header to learn from. Then it is a separate organizers, which is balances balances between these two. So the vocabulary is more manageable. The sequence length is also more manageable. And there are some algorithms and we saw that the uh, popular algorithm is probably PPE. And various elements give a sound variations of PPE. Um, and then what the rate works is actually the structure crackers, and then tries to, um, iteratively merge new tokens merge tokens and form new tokens. And then this, uh, merging tokens is based on how like, what is the most frequent pair that they can just merge. And that's how they continue doing this. Uh, and continue the algorithm until they reach a threshold for the vocabulary size. So and then we also saw some visualization and the typical library and how easy it can be to just use some free training organizers. So that's all I wanted to cover for organizations. And then in the next lecture, we will start learning about, uh, model architecture and understand how elements look internally. Hello, everyone. Welcome back. In this lecture, we are going to focus on the model architecture. In the previous lectures, we talked about data preparation and wrap up, and we understood how we can prepare data and organize The focus of this lecture is to understand now how LMS look like internally. Um, we'll start by networks and some important layers. And then after that, will talk about transformer and decoder on the transformer architecture. And finally, we will discuss why transformers are so powerful and what they use for text generation. So let's start. So the first thing is what is neural network? And before answering that, let's understand the goal of most machine learning models. Most machine learning models are basically trying to learn a mapping from some input to output. I have three examples here. And, um, if you look at them, all of them, we want to learn a model to learn a mapping from input to output. On the left, I have house price prediction. And then we can think of problem as given some signals about the houses. We want to predict its price. And those signals can be anything like location, number of bedrooms, and, um, house size, and those kind of things. And then somehow they get some better numbers. Various ways to encode these these, uh, signals into numbers, and that's not the main focus of, uh, this lecture. But, um, let's assume that there's some phase to convert these numbers. And then price is also a number. So we can think about the input as x, which has all these, uh, x one, x two, and x n for different signals. And then output is y, which is price. Now we machine learning model to learn a mapping from x to y. So uh, look at all the training data of x and y to learn from it. And then when we give a new to it, it should be able to predict y. So it's essentially learning this mapping, this arrow here that I have to go from any x to y. Now in another example, for instance, email is not class And the problem here is given an email, want a model to classify it as a spam or not not a spam. So we can think of, you know, useful signals, like send their content subject of the email and so on. And then these are converted into numbers as a first step. So we can think of the input as x um, and then it has all these x one, x one, x n. And then the output is basically expected to be whether it's a spam or not. So y can be basically one if the email is a spam, and it can be zero if the email is not So the goal of the machine learning model here is to learn a mapping from x to y. So again, while it's a different, um, problem, goal of the machine learning is still the same. You just learn a mapping from a bunch of examples, it takes n And then the third example, like, the tumor detection. Here, the problem is given an image. We want an ML model to find the location of the detected tumor if it exists. And even this example, everything is basically kinda converted to a number and the model's goal is to learn the mapping. Image can be represented by its pixels. And then location of the text here to more, um, to more and we, um, we can think of that as four numbers. And the top left, basically, x and y, and column right x and y. So it's a rectangle, we just include the top left and bottom right. At the end, there are just four numbers that the model is expected to map to. So even two more detection is also mapping. Now neural network is just a sequence of parameter, uh, parameterized transformations that maps an input to an output. So it's just one way that it starts from x. There's just a sequence of transformations. It transforms x into some intermediate, uh, representations. Keeps transforming and transforming until it reaches the final output, which is y. And those transformations are learnable. Meaning that there are some parameters, and then the goal of the machine learning model during training is to is to learn those parameters. Such that for that particular task, it can, um, keep transforming the input in a in a very relevant and effective way. So that it its final output is why. And this is a simple visualization, an example. So let's say you have a neural network and there is some input to it with four numbers, um, zero point five point five and two more. And then this goes into neural network. Again, internally, neural network has some transformations. In this case, let's assume for simplicity, it has one transformation only. And, um, it just transforms it. So one very simple way of transformation you can think of is that it also internally keeps four parameters or weights. And then these weights gets multiplied by the, uh, different x values. And then you just add them up. So we can have something like w one times x one plus w two times x two and so on. So at the end, basically, this neural network is just transforming using a linear, uh, data combination of, uh, its values. And then the output is going to be one number. So this is just a very simple way of transforming something to output using some, um, park internal parameters. But in practice, it can be, uh, more complex In neural network, it can have multiple layers. And each layer is transforming, uh, its input to some output. And, um, for example, layer one transform x one to x two using its internal parameters. And then x two becomes an intermediate representation, which goes to layer two. And layer two again transforms it. And it goes all the way to layer n, and then layer n transforms its

You
21:38:23

the goal of the machine learning is is to listen. We just learn a mapping from a bunch of examples with x and y. And then the third example, like, the tumor detection. Here, the problem is given an image. We want an image model to find the location of the detected tumor if it exists. And even in this example, everything is basically kinda contradicting a number, and then the model's goal is to learn the map. Can be represented by its pixels, and the location of detected tumors, um, tumors can be um, you can think of that as four numbers. And the top left is basically x and y, and bottom right x and y. So if it's in rectangle, you just include the top left and bottom right. I didn't there are just four numbers that the model is expected to match to. So even two more detection is also mapped. Now neural network is just a sequence of parameter, uh, parameterized transformations that match an input one up. So it's just one way that it starts from x, there's just a sequence of transformations. It transforms x into some intermediate, uh, representations, and keeps transforming and transforming until it reaches the final output, which is y. And those transformations are learnable, meaning that there are some parameters then the goal of the machine learning model during training is to learn those parameters. Such that for that particular task, it can, um, keep transforming the input in a in a very relevant and effective way so that it its final output is what? And this is a single visualization, an example. So let's say you have an enrolled effort here. And there is some input to it with four numbers, um, 0.5, one, 0.5, and two more. Then this goes into neural network. Again, internally, neural network has some transformations. In this case, let's assume for simplicity, it has one transformation only. And, um, it just transforms it. So one very simple way of also internally keeps four parameters, four base, And then these bits gets multiplied by the the different text values. And then we just add them up. So we can have something like w one times x one plus w two times x two and so on. So at the end, basically, this neural network is just transforming using a linear, uh, data combination of, uh, its values, and then the output is going to be one number. This is just a very simple way of transforming some input to output using some, uh, part internal parameters. But in practice, it can be, uh, more complex. Neural network, we can have multiple layers, and each layer is transforming its input to somehow And, um, for example, layer one transform x one to x two. Using its internal parameters, and then x two becomes an intermediate representation, which goes to layer two. And layer two again transforms it. And it goes all the way to layer n, and then layer n transforms its input, and its output can be the final output of this run. So, again, all I wanted to show here is that neural network is simply just a sequence of transformations to learn a mapping from inputs x to outputs y. Okay. Now let's zoom in into these layers and understand, uh, what are some common layers in neural networks.

ByteByte
21:38:23

output can be the final output of this neural network. So again, I wanted to show here is that neural network is simply just a sequence of transformations to learn a mapping from inputs x to outputs y. Okay. Now let's zoom in into these layers and understand, uh, what are some common layers in neural networks.

You
21:40:52

So the first layer that is very commonly used in neural network has a Vini layer. And linear layer, again, as the name suggests, it's just a linear transformation of the input. So for example, if I input this x, it internally has this w and b parameters. Rates and bias. And these are the parameters of this linear layer, meaning that it will learn this, uh, during the training. And then it just linear transforms x by using this formula. And then it it create calculates the output y. We can see this in an example here. Let me zoom in. So for example, if the input is x, it is four values. That means is that internally, it has to, um, it's a linear transformation. So we have to have a weight for each each of these values so that we can just multiply it. So the weight matrix is going to have a shape of one by four because the output has one number and the input has four numbers. And then the bias has just one number, and, um, we'll see why, but just at a very high level after we combine all the multiply, all the dates with the x and just add them up, then we had to beat it by us. So this is just a formulation here. We have this beam here. Now another data we can think of linear layer is something like this. We have four small zeros in here, and these corresponds to the inputs. So for example, inside this, the serial is 0.1. Inside the second circle is 0.5 and so on. And then we want to go to one number. Yeah. This is the output. Uh, it's 0.6. Then we have these connections here, and these connections are the associated dates with these connections. So since there, uh, rates has a shape of one by four, it's it's coming from here because there are there are just four connections. Um, and then the weights have been named is very, like, W111213814. And now imagine if you had more outputs, like, one more output here, you would end up having two circles here. And then the ratio would have been, uh, two by four because now we need we will be we would have four more connections. To the second row to the second circuit. Now this is also where the name neural network is coming from because signals can be you can think of those as neurons, and then these connections be also think of as, uh, connections between neurons. So this is how a linear layer transforms some input to output, very simple formulation. And now this can be generalized into, um, into uh, more outputs. For example, if the input has more, um, more numbers, then it's expected to produce an output with more numbers also, then the data can be more complex. If you just visualize it, you would have more neurons, input neurons, and more output neurons, as a result, the VEG metrics would be, um, bigger. It's basically four times, uh, these many neurons. But this is all, uh, linear layers. It's just basically that is stored internally and will be learned during training time. And then the base matrix is used to transform input throughout.

You
21:43:49

Now because of that, you can think of linear layer as just a mathematical expression that converts x to y. So at the end of the day, linear layer is just a mathematical expression, and then it can be written down like this. For example, this neuron can be calculated by um, multiplying rates with their inputs and just add them up. And also more more Apple neurons can be computed in a similar way. Way. So linear layer is just a mathematical expression. Now linear layer is not the only layer that is available. There are a lot, uh, more layers that were developed over the years. I have listed some of them here. For example, we have convolution layers. We have activation layers. We have, um, attention layers. And many other examples. And each of these layers having different purposes and different strength. And they're designed for different reasons. So um, for example, convolution layers are designed so they work better with spatial inputs like images. So I'm not going to, um, talk about these layers in detail. There are some wonderful resources out there, um, that you can take a look. But what I wanted to share is that all these layers that are inside neural networks, it's one of these, and the linear layer and the convolution layer. And all these layers what they differ is basically how they formulate their transformation. They just formulate it differently. For example, we saw, um, linear layer here. It's it's it formulates the transformation, has a linear transformation. Now this is going to be different for other layers. So that's how these layers are different, but they all they all are similar in a beta. They just handle transform some input. Now let's go to the, um, to the next part, which is basically neural network is just a sequence of layers. We just saw that very early in this lecture. So what that means is that, for example, uh, neural network does have multiple layers. This example, linear layers. If you visualize it, it would reflect something like this. For example, the inputs come here. We have linear layer one weights here. Transforms the input to some intermediate output. Now this goes into linear two. Linear two has this rate metrics here, and then it transforms this intermediate representation into another intermediate representation and so on. And then finally, the final linear layer transforms its input to some output here, and these are the weights. So we saw that how linear layer is just a mathematical illustration. But if you think of this neural network, this is also a mathematical expression. So what that means is that you can write ticks using some, uh, mathematical expression that is, uh, calculated using x. And we can just think of that as something like this here. Um, this can be the just the the expression to to calculate y. You can see it is based on all the rates of, um, linear layers that are inside the neural network. And then it just gets some it's a bunch of multiplication and additions, and that's it. So neural network, as a whole, is also a mathematical expression that transforms into x y. Now over the years, research teams and, uh, companies have tried combining various layers and building neural network that works better for certain tasks. There was a lot of novelty and innovations into how to combine all these different layers in a unique way. That the output, which is the neural network, can learn the task very effectively. And then in 2017, we have published a paper attention is all you need, and then it introduced it introduced transformed by artificial.

You
21:46:09

Just basically a unique way of combining all these layers that you just saw that it works really well for certain tasks. So here is the paper. Attention is all you need, and it was published by. And it is just explanation, and how they came up with the architecture and how it looks like. But here in figure one, they have the the architecture of the transformer. Again, uh, feel free to read the paper to understand all these, uh, components. But what I want to share here is, at the end of the day, this architecture is also just a combination of layers that is up here. And as a whole, this can be you can think of it as a neural network. And this is basically a unique way of just, uh, putting and stacking all these available layers, just stacking them and then, um, training it. So this was the key innovation of Transformer, and it works really well for their task And initially, in this paper, the task that they were trying to solve was machine translation. That they wanted to translate text from one language to another language. So the input here is, for instance, the the text in the source language, and then it goes through all these transformations here, and then the output is basically to talk more about the output of transformers in a bit. But the output is basically eventually, it's going to be the translated text in the target language. Now I have this, uh, screenshot of this diagram here in the transformer part of the lecture. And, again, it's just a unique layout combining layers and building this neural network. But at the end, if you think of this, this is also a mathematical expression as we saw because each of these layers are some mathematical expression, and then just stacking them together they still can compute the output based on the inputs. So, um, if if you just visualize this, it can be something like this. This image is just, uh, is not real. I generated it by Chachapiti. Just wanted to show that even transformer can be written in, uh, some mathematical expression. Basically, can be written in this format. And, um, if you're interested to learn more about transformer and what are the, uh, reasons behind each component of it, There's this great, uh, illustration by Jay Alamar. It's a wonderful resource to go through and learn about, uh, the high levels and how transformer looks like. See here is the input in in the source language, and here is the output time student in the target language, which to be English here. And then it talks about all the components, encoder, decoder. ACP encoder is this part of the architecture on the left because it's encoding the input text which is in the source language. And then it has also decoders, which is this right, part of the transformer. And it's because now it starts to decode encoded input text to some, uh, output text in the target language. But again and it it goes deeper and talks about all the details

ByteByte
21:46:49

So the first layer that is very commonly used in neural networks as a building block is linear layer. And linear layer, again, as the name suggests, is just a linear transformation of the input. So for example, if the input is x, it internally has this w and b parameters, rates and bias. And these are the parameters of this linear layer, meaning that it will learn these, uh, during the training time. And then it just linearly transforms x by using this formula. And then it it generate it calculates output y. So we can see this in an example here. Let me zoom in. So example, if the input is x with these four values, what that means is that internally it has to, um, a linear transformation. So we have to have a weight for each each of these values so that we can just multiply them. So the weight matrix have a shape of one by four. Because the output has one number, and the input has four numbers. And then the bias has just one number, and, um, we'll see why, but just at a very high level, after we combine all the multiply, all the base with the x, and just add them up, then we add it with the bias. So this is just the formulation here. We have this b here. Now another way we can think of linear layer is something like this. We have four small signals here, and this corresponds to the inputs. For example, inside this, the serial is 0.1. Inside the second serial is 0.5 and so on. And then we want to go to one number here, which is the output, um, at 0.6. And then we have these connections here. And these connections are the associated weights. To these connections. So and since they are, uh, rates as a shape of one by four, it's it's coming from here because they're there are just four connections. Um, and then the rates can be named as like, the label one one one two one three one four. And now imagine if you had more outputs, like one more output here, we would end up having two circles here, and then the V shape would have been, uh, two by four. Because now we need we we would have four more connections to the second to the second circle. Now this is also where the name neural network is coming from because these signals can be we can think of those as neurons. And then these connections, um, can be also think of as, uh, connections between neurons. So this is how a linear layer transforms some input to output. Very simple formulation. And now this can be generalized into, um, into, uh, more outputs. For example, if the input has more, uh, more numbers, then it's expected to produce an output with more numbers also. Then the data can be more complex. If you just visualize it, you would have more neurons input neurons and more output neurons. As a result, the rate matrix would be, uh, bigger. It's basically four times, uh, these many neurons. But this is all, uh, linear layers. It's just basically a a weight matrix that is stored internally and will be learned during training time. And then the weight matrix is used to transform input to output. Now because of that, can think of linear layer as just a mathematical expression that converts x to y. So at the end of the day, linear layer is just a mathematical expression. And then it can be written down like this. For example, this new one can be calculated by um, multiplying rate with the inputs and just add them up. And also more more Apple neurons can be computed in a similar way. So linear layer is just a mathematical expression. Now linear layer is not the only layer that is available. There are a lot, uh, more layers that are developed over the years. I have listed some of them here. For example, we have convolution layers. We have activation layers. We have, um, attention layers. And many other examples. And each of these layers having different purposes and different strength. And they're designed for different reasons. So um, for example, convolution layers are designed so they work better with special inputs like images. So I'm not going to, um, about these layers in detail. There are some wonderful resources out there. You can take a look. But what I wanted to share is that all these layers that are inside neural networks, it's, um, one of these. It can be linear layer. It can be convolution layer. And all these layers, what they differ is basically how they formulate their transformation. They just formulate it differently. For example, it's a, um, linear layer here. It's it's formulaic transformation as a linear transformation. Now this is going to be different for all layers. So that's how these layers are different, but they are they all are similar. In beta, it just converts transform some input to output. Now let's go to the, um, um, to the next part, which is basically neural network is just a sequence of layers. We just saw that, very early in this lecture. So what that means is that, for example, uh, neural network does have multiple layers. This example, linear layers. If you visualize it, it would look like something like this. For example, the inputs from here. We have linear layer one, weights here, transforms the input to some intermediate output. Now this goes into linear two. Linear two has this weight matrix here, and then it transforms this intermediate representation into another intermediate representation. And so on. And then finally, the final linear layer transforms its input. Some output here and these other weights. So saw that how linear layer is just a mathematical expression. But if you think of this neural network, this is also a mathematical expression. So what that means is that we can write x using some, uh, mathematical expression that is calculated using x. And you can just think of that as something like this here. Um, this can be the just a expression to to calculate y. And you can see it is based on all the weights of, um, linear layers that are inside the neural network. And then you just get some it's a bunch of multiplication and additions, and that's it. So neural network as a whole is also a mathematical expression that transforms text to y. Now over the years research teams and um, companies have tried combining various layers and building neural network that works better for certain tasks. There was a lot of novelty and innovations into how to combine all these different layers in a unique way. That the output, which is the neural network, can learn the task very effectively. And then in 2017, we have published a paper named attention is all you need, and then it introduced it introduced it, transformed architecture. Transforming architecture is also a neural network, and it's just basically a unique way of combining all these layers that it just that it works really well for certain tasks. So here is the paper. Attention is all you need. And it was published by Google Brain team. And it's just explanation and, uh, how they came up with the architecture and how it looks like. But here in figure one, they have the the architecture of the transformer. Again, feel free to read the paper to understand all these, um, components. But what I want to share here is at the end of the day, this architecture is also just a combination of layers that we saw earlier. And as a whole, can be we can think of it as a neural network. And this is basically a unique way of just, uh, putting and stacking all these uh, available layers, just stacking them and then, um, training it. So this was the key innovation of Transformer, and it works really well for the task And initially in this paper, the task that they were trying to solve was machine translation. Meaning that they wanted to translate text from one language to another language. So the input here is, for instance, the the text in the source language. And then it goes to all these transformations here, and then the output is basically we'll talk more about the output of transformers in a bit. But the output is basically eventually, it's going to be the translated text in the target language. Now I have this, uh, screenshot of this dialogue here in the transformer part of the lecture. And again, it's just a unique way of combining layers and building this neural network. But at the end, if you think of this, this is also a mathematical expression as we saw Because each of these layers are some mathematical expression, and then just packing them together, we still can compute the output based on the inputs. So, um, if you just visualize this, it can be something like this. This image is just, uh, is not real. I generated it by JPGPT. I just wanted to show that image transformer can be written in, uh, some mathematical iteration. Um, basically, can be written in this format. And, um, if you're interested to learn more about transformers and how what are the, um, reasons behind each component of it, There's this great, um, illustration by Jay Almar. It's a wonderful resource to go through and learn about, uh, the highlighters and how transformer looks like. See, here is the input in the source language, and here is the output I'm still in the target language. Which seems to be English here. And then it talks about all the components, encoder, decoder. Basically encoder is um, this part of the architecture on the left. Because it's encoding the input text, which is in the source language. And then it has also decoders, which is this right, um, part of the transformer. And it's because now it starts to decode the encoded input text to some output text in the target language. But again, and it it goes deeper and talks about all the details about the components. An interesting read if you are not too familiar with the transformer architecture. So, um, this transformer was really, really powerful for machine translation. And then later, we realized that we only keep this decoder part, meaning the the right side of this, it would be really powerful for text generation. And then this is what we call decoder only transformer. So it's nothing but the just the decoder part of the original transformer architecture. So it looks like this. Again, it's just a a combination of layers as that. And some of them are, like, in a like, locked because these are just supposed to be repeated in times. Um, so we have time times in here. So what that means is that, um, this whole considered a transformer block, and we have thin block stacked right after each other. So this works very well for text generation.

ByteByte
21:52:43

And LLMs or large language models are also a text generation, uh, model. So most of the LLMs that I'm aware of are all based on decoder only transform. So what that means is that whenever we hear LLM, it means that it's just a decoder only transformer. Trained on Internet data. So, um, we'll talk more about the input and output of transformers, and we understand why this uh, architecture, how can it be adapted for text generation. Alright. We saw that decoder only transformer is really powerful for text generation. So let's understand why. And to answer that, let's first see what is the input and output of the transformer architecture. So as we saw in the previous lecture, transformer is just a a a sequence of transformer blocks, and each block has some layers. And they're just a stack, and they transform input to output. But what is the input and output? The input that these layers expect is a sequence of vectors. This is just these layers are expecting your input. Um, for example, here we have one vector, uh, we have three vectors. These three vectors, and again, each of these vectors are having these as small cells. Each cell is basically a single floating point number. And so these three go into transformers. Transformer just keeps transforming them, and then the output of the transformers is again a sequence of vectors. And as you can the size of the vectors are the same in this example, and it's in most, um, architectures, it's the same. Theoretically, they can be different, but we just keep them the same. And then the output sequence length is also the same. So we have inputs and three outputs. So this is the input and output of the, uh, decoder only transformer. Now let's also understand what is the input and output of an element for text generation. So in element, again, for texts, they are converted to tokens. So each text is, uh, the sequence of tokens. And in order to generate text, what we want an element to do is, um, we just ask some sequence of text to it. For example, here we pass these three numbers, um, corresponding to hi, how are. And then we expect the transformer yeah. Sorry. The element to output the next, um, token. And here it's nine forty four, which corresponds to you. So this is our ideal input output that we want to train or build elements. Transformers are expecting a sequence of vectors. What we want from the LLM is, uh, a sequence of numbers going to the LLM, and then the LLM just predict the next uh, token, just and also a number. Now how should we adapt this? How can we really, um, modify and add new layers to this, uh, architecture? So it becomes it's input and output becomes, uh, compatible with this. Now, um, this is how we are going to do this. For a given text, like, hi, how are first we discuss how it goes to the tokenization and gets converted to a sequence of numbers. Now this sequence of numbers goes into another layer. We call these layers embedding layers. And what embedding layer does is it converts each number into a vector. And it's also learnable, meaning that during training, this embedding layer will be learned so that each, um, ID gets converted to a sequence of vector sequence of numbers or embedding. That is meaningful or useful or effective for the model to predict the next token. So here, for instance, these three tokens, these three IDs goes into embedding layer, and then the output becomes three vectors. And this is exactly what the transformer expected as the input. So by just adding these two layers, we can easily adapt it to work with our input, um, input text. Now let's get to the output. The output of transformers is three vectors. Corresponding to each of the three input vectors. But what we want is the next token. So the typical way to make this work is we discard the first few, uh, vectors, and only keep the last vector. And now this last vector is, again, is just a vector It's just, uh, some numbers, floating point numbers. We add a linear layer on top of this. So we transform this vector into a new vector. And this new vector can be interpreted as probabilities of the next token. So its length would be equal to our vocabulary size example, if you have 50,000 tokens in our vocabulary, the output of this transformation um, by the senior layer would be a vector of size 50,000. And then the value of itself corresponds to the probability for that particular token in our vocabulary. So basically, this linear layer is just mapping this final output into probabilities of different tokens. Tokens. And again, there is just also a softmax operation which is responsible to convert these into probabilities. We skip that for simplicity. But we can we can think of these layers to convert the final output of the transformer into some probabilities for each token. And now these probabilities can be, uh, we can think of those as something like this. So for example, um, this entire thing we had here, this organization embedding layer, transformer blocks, and add additional layer here. All of these, we can think of it as the LMM architecture. So here, I have this element. I hope you are going to the element. And then after the linear transformation, the output is this vector. It five values here, five probabilities. And again, this is only for visualization. In practice, this vector would be of length, uh, 50,000 or whatever our vocabulary size is. And then these are associated to our tokens. For example, you can think of this as the probability distribution over the the vocabularies. So in this case, if, uh, I t zero corresponds to end of sentence token, it has 2% chance. For Abel, it has 11% chance and so on. And we can see the highest probability as well. Uh, with 86% chance, which also makes the most sense when the input I hope you are because well is the most likely token and also make this whole sentence meaningful. So this is how basically we go from the original decoder only transformer blocks combine it with some additional layers to make it work, uh, for text generation and, um, learning dependencies between texts. Now, um, next thing I want to talk about is if, um, if you think of all modern LRMs these days, all based on decoder only transformers. What they differ is basically some hyperparameter. In the decoder only transformer. For example, this is a screenshot I took from g p t two paper. And then they have four variations of, uh, models. And each variation has different number of parameters. Just because of the different hyperparameters to build them the architecture, to build the decoder only transformer. And some of these, um, hyperparameters are, for example, layers. Layers refers to that time span that we sign to figure, like how many transformer blocks we want to stack. For example, their smallest model, they stack 12, um, transformer blocks. And then this is the dimension of the vectors, like, the input vector size and the basically, the size of the vector that is transformer works with. And then they use seven sixty eight for the smallest model. And they just play around with these numbers to get, uh, bigger models. So when they increase number of layers, it means that they're more, uh, locks and more layers, and each layer internally has parameters. So we just included bigger models. With with more parameters. And this generally means that the

You
21:53:13

So, um, this transformer was really, really powerful for machine translation. And then later, we realized that they only keep this decoder part, meaning the the right side of this, it would be really powerful for text generation. And then this is what we call, uh, decoder only transformer. So it's nothing, but they're just a decoder part of the original transformer architecture. So it looks like this. Again, it's just a a combination of layers of stack, and some of them are, like, in a, like, blocked because these are just supposed to be repeated end times. Um, so we have time times in here. So what that means is that, um, this whole considered a transformant block, and we have n block stacked right after each other. So this works very well for generation, and LLMs or dark language models are also a text generation, uh, model. So most of the LLMs that I'm aware of are all based on decoder only transformers. So what that means is that whenever we hear element, it means that it's just the color only transformer. Trained on Internet data. So, um, in the next lecture, we'll talk more about the input and output transformers, and we understand why this, um, architecture how can it be adapted for text generation. Alright. You saw that decoder only transformer is really powerful for text generation, so let's understand why. And to answer that, let's first see what is the input and output of the transformer architecture. So as we saw in the previous lecture, transformer is just a a a sequence of transformer blocks, and each block has some layers. And they're just a stack, and they transform input to output. But what is the input and output? The input that these layers expect is a sequence of vectors. This is just how these layers are expecting your input. Um, for example, here we have one vector, uh, we have three vectors. These three vectors and, again, each of these vectors are having these small cells. Each cell is basically a single floating point number. And so these three go into transformer. Transformer just keeps transforming them, and then the outputs of the transformer is again a sequence of vectors. And as you can see, the size of the vectors are the same in this example, and it's in most uh, architectures, it's the same. Periodically, they can be different, but we just keep them the same. And then the output sequence limit is also the same, so we have three inputs and three outputs. So this is the input and output of the, uh, decoder only transform. Now let's also understand what is the input and output of an LMM for text generation. So in element, again, for text, they're 102 tokens, so it's text is, uh, a sequence of tokens. And in order to generate text, what we want an element to do is, um, we just pass some sequence of text to it. For example, here, we pass these three numbers, um, corresponding to I, uh, r, and then we expect the transformer yeah, sorry, the element to output the next, um, token. And here it's nine forty four, which corresponds to you. So this is our ideal input output that we want to train or build. Transformers are expecting a sequence of vectors. What we want from the teleem is uh, a sequence of numbers going to the LLM, and then the LLM just predict the next, uh, token. Just and also a number. Now how should we adapt this? How can we really, um, modify and add new layers to this architecture? So it becomes its input and output becomes, uh, compatible with this. Now, um, this is how we are going to do this. For a given text, like, hi, how are, we discuss how it goes to the tokenization and gets converted to a sequence of numbers. Now this sequence of numbers goes into another layer. We call these layers embedded layers. And what embedding layer does is it converts each number into a vector. And it's also learnable, meaning that during training, this embedding layer would be learned so that each, um, ID gets converted to a sequence of sequence of numbers for embedding. That is meaningful or useful or effective for the model to predict the next token. So here, for instance, these three tokens, these three IDs goes into embedding layer, and then the output becomes three vectors. This is exactly what the transformer expected as a in. So by just adding these two pairs, it can easily adapt it to work without input, uh, input text. Now let's get to the output. The output of transformer is three vectors. Responding to each of the three input vectors. But what we want is the next vector. So a typical way to make this work is we discard the first few, uh, vectors, and the only key, the last vector. And now this last vector is, again, it's just a vector. It's just some numbers, floating point numbers. We add a linear layer on top of this. We transform this vector into a new vector, and this new vector can be interpreted as probabilities of the next token. So its length would be equal to our vocabulary size. For example, if you have 50,000 tokens in our vocabulary, the output of this transformation, uh, by this linear layer would be a vector of size 50,000. And then the value of itself corresponds to the probability for that particular token. In our vocabulary. So, basically, this linear layer is just mapping this final output into a probabilities of different objects. And again, there is just a also softmax operation which is responsible hundred days into probabilities. We skip that for simplicity. But can we can think of these layers to convert the final output of the transformer into some probabilities for each token. And now these probabilities can be, uh, we can think of those as something with like this. So for example, um, this entire thing we had here, this organization embedding layer, decoding on the transformer blocks, and the add additional layer layer here. All of these, we can think of it as as the LNM architecture. So here, I have this LNM, I hope you are goes into the LNM. And then after a linear transformation, the output is this vector at five values, five probabilities. And, again, this is only for visualization. In practice, this specter would be of length. 50,000 or whatever our vocabulary sizes. And then these are associated to For example, can I think of this as the probability distribution over the vocabularies? So in this case, if the I g zero price wants to end of sentence token, it has 2% chance. Abel, it has 11% chance and so on. And you can see the highest probability as well. With 86% chance, which also makes the most sense when the input is I hope you because Bell is the most likely token and also make this whole sentence meaningful. So this is how, basically, we go from the original decoder only transformer blocks combining it with some additional layers to make it work, uh, for text generation. And, uh, learning dependencies between texts. Now, um, next thing I want to talk about is if, um, if you think of all modern LLMs these days, they're all based on the polar only transformers. What they differ is basically some hyper parameters. In the decoder only transform. For example, this is the screenshot I took from GPT two paper. And then they have four variations of models. And each variation has different number of parameters, just because of the different hyperparameters used to build them the architecture, to build the decoder only transform. And some of these, um, hyperparameters are, for example, layers. Layers refers to that time span that we saw in the figure, like, how many transformer blocks we want to stack. For example, their smallest model is at 12, uh, transformer blocks. And then this is the dimension of the vectors, like the input, uh, vector size and the basically, the size of the vector that this transformer works with. And then they use seven sixty eight for the smallest model. And they just play around with these numbers, to get, uh, bigger models. So when they increase number of layers, it means that they're more, uh, blocks and more layers, and each layer internally has parameters. So we just simply get bigger models with bit more parameters. And this generally means that the models become more powerful because when they have more model, when they have more parameters, it means that they are more, um, they have more capacity to learn, uh, more complex methods from input to output. They're also more expensive to train because if there are no parameters, we have to train them for longer. And, uh, we have to also spend more computing power to train these models. So this is GPT two, and their biggest model has around, um, you know, one, um, big, um, 1,542 environments. So it's around basically 1,500,000,000 parameters. Now this is GPT three.

You
21:53:31

So GPT three is basically, uh, the next model that OpenAI trained and released after GPT two. And what they did was they just scaled the models. So they increased the algorithm, they trained bigger and bigger models with more capacity. So for example, here, have a g t three as well and other variations, and their biggest model is g t three.

ByteByte
21:53:36

become more powerful because when they have more model when they have more parameters, it means that they are more they have more capacity to learn, uh, more complex matrices from input to output. But they are also more expensive to train because if there are more parameters, we have to train them for longer. And, uh, we have to also spend more computing power to train these models. So this is g p t two, and their biggest model has um, around, um, you know, one, um, like, um, 1,500 to 2,000,000 parameters. It's around, basically, 1,500,000,000 parameters. Now, this is GPT three. Let me make this a little bit bigger. So GPT three is basically, uh, the next model that OpenAir trained and released after GPT two. And what they did was they just scaled the models. So they increased the hyperparameters. So they trained bigger and bigger models with more capacity. So for example, here, um, have g t three small and other variations, and their biggest model is g t three

ByteByte
22:03:12

175,000,000,000 biometer. Um, so, um, this is very big, and we'll see shortly that working with these big models can be very challenging engineering wise. And it requires a lot of engineering effort so that we can put them and train them and later serve them. But here, you can see the different, um, pipe parameters that are used, um, for for instance, for the Versus model, they're using 96 layers. And then the dimension that the multichannel transformer is working with is 12,000 something. And more hyperparameters. And also included Lamma three from Facebook, but again, the same story. Um, three variations. And this, um, last variation here even bigger than g p t three. It has 405,000,000,000 parameters. And then these are some of the hyperparameters that they've been using for number of layers, model dimension, and some other parameters. So this is all I wanted to cover in the model architecture. We saw that everything is basically just based on a decoder only transformer. And they just keep changing the hyperparameters to increase or decrease the capacity of their model. And then typically increasing the number of parameters means that the model would be more capable after training. In the next lecture, we'll talk about how to train a decoder only transfer learning model. Hey, everyone. Let's focus on the model training part of elements. In the last lecture, we saw how elements looked like internally and what's their model architecture. We also saw that whatever input you pass them, like I hope you are, it will produce a pro a vector which represent the probabilities of different tokens. However, if we do not train a model, this probability is gonna be random. And the reason for that is because all the model parameters are random. So LMM has these layers and layers are keeping their weights, and those weights are random. So when we pass this, I hope you are as a a basic sequence of vectors, it goes to the element. And remember, this entire thing, as we mentioned last time, is just a mathematical expression. And since all the bits are random, that means that the output would be random. So all the probabilities that would produce by the LMM are meaningless. So we cannot really use this and rely on it to produce the next token. The purpose of my training is to use some processes and algorithm to queue these parameters that are internal to the element. So each layer would keep changing these parameters after they expose the element to the entire Internet data. Such that it can more accurately predict these probabilities. So let's understand this in more detail. So we have Internet data, which is cleaned and also converted to a sequence of IDs. Here, just for simplicity, in my example, I'm just showing them in text in video text just so we understand easier. But in practice, this text and this text that goes into the element are basically just a sequence of IDs. So how training works? Basically, we have this Internet data, and imagine this is some paragraph somewhere on the Internet, and we, um, we have something like Albert Einstein, was a German born physicist and mathematician, and so on. So the goal of training is to just expose the LLM to some part of this paragraph, and then we we force the model to predict the next token accurately. So what that means is that for any chance, we can just sample out some part of this, like Albert Einstein was a German born. And then we pass it to the algorithm. From the paragraph in our training data, we know that the next correct word is physicist. So when we pass this to the element, we get the probabilities of the next token. We also know that what is the correct token. So this vector, we can think of that that everywhere is zero except the ID of the correct token. The ID that, uh, uh, the ID that corresponds to the physicist token. So that ID is one. Let's say it's here, and is zero. And these are probabilities. And the goal is to try to update the parameters of the element so that the probabilities are as close as possible to this vector. For that, we have to define a loss function. And there are just, um, typical loss functions that are for different problems. And, basically, what the loss function does, it it measures the quality of the um, current predictions. So let's say these are the probabilities. Cross entropy loss is the common loss for this purpose that we use. Use that. It's just a formulation to compare these probabilities with the correct token. And just, uh, would give us a number. And that number, if it's high, it means that these two vectors are not similar. So the loss is high. And if the probabilities are more similar to the correct token, the the loss would be low. So this is the first step of training. We send out some text. We pass it to the element. Then we use a plus one chance, such as crossing through plus to measure the quality of the predictions. And, again, cross entropy, if you, um, search Google, or go to Wikipedia, there are, uh, formulations how cross entropy is, uh, can be calculated. This is the formula. And there's some details where is it coming from if you're interested to learn more about the theories. But, again, at a very high level, it just measures how close the probabilities are to the correct token. Now once you have this loss, it's now time to update the parameters of the element. And that's the purpose of the second part, which is optimization. Optimization is just an algorithm that uses the, um, calculated loss value then it goes and updates all the parameters of the element based on some algorithm. So it updates them such that if you pass the same input to the element, it makes, um, more accurate it predict more accurate probabilities next time. So that's that's all it happens. So now if you repeat these two steps multiple times many, many times on, uh, for different samples from the Internet data, eventually, we would have very good rates for the model, meaning that the model would end up with the rates that if we pass something like Albert Einstein was a German born, and if you look at the probabilities, the the probability for the token physicist would be very high. And that's going to be the outcome of the training process. So we continuously sample text from our training data. Pass it to the model the model, um, output probabilities. We calculate the loss compared to the correct next token. And then we apply optimizer to update the parameters of the l. And after we do this enough time on the entire Internet data, we would end up with a very good, um, next token predictive model. Meaning that whatever like, other times time, we get the probabilities And those probabilities are, um, basically, statistically very accurate in a sense that if on the Internet we see in a lot of places we see other time sign was, the probability of was would be high. So in other words, basically learns the dependency and the statistics between all different tokens from the Internet data. And then it has some implicit knowledge of the word. And that's because um, if a model can really predict a next token, um, really well, it means that it has some understanding of different domains and different areas and, um, some implicit knowledge. Imagine, for instance, we ask the we, uh, give LMM something like I like machine learning because the next token and also the, um, future tokens that it would, uh, keep iterating is probably very relevant to machine learning. So I would probably output some terms that are related to machine learning and, um, to why people like machine learning. And all of these are statistically very close to what is available on the Internet. So that's the outcome of the training. Now remember, these models can be very difficult to train. Even though the process is very straightforward, Because these elements can be very huge in terms of number of parameters, it can be, uh, engineering wise, it can be very a lot of challenges to make sure that we have the right setup to train these models. For example, I asked that what are the resources needed to train an element, um, for 105,000,000,000 parameters. And remember, this is what, uh, Lambda three, um, biggest version of Lambda three has. Has. So what is, uh, what is the memory needed? I misspelled what? With why? Um, and then the answer is, um, it requires lots of things. And in here, it says, um, why memory is needed? It's first because we need to store the model parameters during training. So all those parameters, 405,000,000 parameters needs to be stored, uh, in memory. And then during the training, again, there are some algorithms and optimizers needs to, uh, keep track of all the intermediate, um, representations and, um, activations and use those to update the weights. So there are a lot of intermediate, um, floating point value numbers that it should store. So because of all of this, and here is a rough estimation, it requires, um, like, you know, very large memory. Assuming that we use, uh, f p f p 32, meaning that we use four bytes to restore each weight. We would end up needing around 1.6 terabytes of memory. So definitely, it's not possible to train this entire on a single GPU because best GPUs we have might have around, uh, hundreds of gigabytes of memory, so it's not practical to really train this big model on a single GPU or on a single machine. And again, in its calculation, it's saying that you need at least 880 to 100 GPUs to feed the model and memory. And in practice, it requires more. Realistically, it says you use 2,000 GPUs or more. And these numbers are accurate, um, based on what happens in practice. Also here, in terms of the storage, we need huge huge storage as we train the model to just keep saving uh, the checkpoints of checkpointing the new model as we train them. So each time each checkpoint might exceed two to five terabytes of, um, the storage needed. So Aldi is basically saying that it can be very challenging to train um, very large models, very large elements. It also requires lots of distributed training and so on. Um, and then let's go to the LML iterator. This is the the technical report of LAMA three published by, uh, Meta. And what I want to show here is that if you scroll down, there are these, uh, by the way, this is a great technical report. It's very, um, very useful to read and, uh, personally learn a lot of things from it. So but what I want to show here is that, uh,

You
22:03:12

175,000,000,000 parameters. Um, So, um, this is very big, and we see shortly that working bits these big models can be very challenging engineering wise, and it requires a lot of engineering effort. So that we can load them and train them and later serve them. But here, you can see the different, um, pipe parameters that are um, for for instance, for the biggest model, they are using 96 layers, and then the dimension that they're transformer is working with is 12,000 something and more hyperparameters. And also included from Facebook, but again, the same story. Uh, three variations. And this, um, last variation here is even bigger than g p t three. It has 405,000,000,000 parameters. And then these are some of the hyperparameters that they'd be using for number of layers, model dimension, and some other, uh, parameters. So this is all I wanted to cover for in the model architecture. We saw that everything is basic just based on a decoder only transformer, and they just changing the hyperparameters to increase or decrease the capacity of the model. And then typically increasing the number of parameters means that the model would be more capable after training. In the next lecture, we'll talk about how to train the decoder on the transformer model. Hey, everyone. Let's focus on the model training of elements. In the last picture, we saw how elements look like internally and what's their model architecture. We also saw that whatever input we pass them, like, I hope you are, it would produce a a vector which represent the probabilities of different tokens. However, if we do not train the model, these probabilities would be random. The reason for that is because all the model parameters are random. So LLM has these layers, and layers are giving their weights, and those weights are random. So when you pass this by hope you are as a basically sequence of vectors, it goes to the element. And remember, this entire thing, as we mentioned last time, is just a mathematical expression. And since all the base are random, that means that the output would be random. So all the probabilities that would be produced by the element are meaningless. So we cannot really use this and rely on it to produce the next load. The purpose of modern training is to use some processes an algorithm, to tune these parameters that are internal to the element. So each layer would keep changing these parameters after we expose the element to the entire Internet data. Such that it can more accurately predict these probabilities. So let's understand this in more detail. So we have this Internet data, which is print and also converted to a sequence of IDs. Here, just for simplicity, in my example, I'm just showing them in text, in pure text, just so we understand easier. But in practice, this text and this text that goes into the element are basically just a sequence of IDs. So how training works? Basically, we have this Internet data, and imagine this is some paragraph somewhere on the Internet, and they, uh, we have something like Albert Einstein. A German born physicist and mathematician and so on. So the goal of training is to just expose the LLM to some part of this paragraph, and then the x be be forced the model to predict the next token accurate. So what that means is that, for instance, you can just send out some part of this, like Albert Einstein was a German born. And then we pass it to the field. From the paragraph in our training data, we know that the next correct word is physicist. So when we pass this to the element, we get the probabilities of the next token. We also know that what is the correct token. So this vector, we can think of that that everywhere is zero except the ID of the current token. ID that, uh, uh, the ID that corresponds to the physicist token. That ID is one. Let's say it's here. And everywhere else is zero. And these are probabilities. And the goal is to try to update the parameters of the LMM so that the probabilities are as close as possible to the spectrum. For that, we have to define a loss function. And there are just uh, typical loss functions that are used for different problems. And, basically, what the loss function does, measures the quality of the, um, current predictions. So let's say these are the probabilities. Cross entropy loss is the common loss for this purpose that we use. To use that, it's just formulation to compare these probabilities with the correct token and just, uh, would give us a number. And that number, if it's high, means that these two vectors are not similar, so the loss is high. If the probabilities are more similar to the current token, the the loss would be low. This is the first step of training. It's sentence context. We pass it to the element. Then we use a loss function such as crossing property loss to measure the quality of the predictions. Again, cross entropy, if you, um, search Google or go to Wikipedia, there are, uh, formulations, our cross entropy is, uh, and calculated. This is the formula. And there's some details where is it coming from if you're interested to learn more about the theories. But, again, at the very high level, it just measures how close the probabilities are to the particle. Now once you have this loss, it's now time to update the parameters of the element. And that's the purpose of the second part, which is optimization. Optimization is just an algorithm that uses the, um, calculated loss value then it goes and updates all the parameters of the element based on some algorithm. So it updates them such that if you pass the same input to the element, it makes, uh, more accurate predict more accurate probabilities next time. So that's that's all it happens. So now if you repeat these two steps multiple times, many, many times, and different samples from the Internet data, eventually, we would have very good rates for the model, meaning that the model would end up with the rates that if we pass some people like Albert Einstein was a German born, and if you look at the probabilities, the the probability for the token physicist would be very high. And that's going to be the outcome of the training process. So we continuously from our training data. We pass it to the model. The model, uh, output pro probabilities. They calculate the loss compared to the correct next token, and then we apply optimizer to update the parameters of the l. And after we do this enough time on the entire Internet data, we would end up with a very good, um, next token. Meaning that, once every pass, like, five times time, you get the probabilities. And those probabilities are, um, basically, statistically very accurate. In the sense that if on the Internet, we see in a lot of places, we see other time sign was, the probability of was would be high. So in other words, basically learns all the dependency and the statistics between all different tokens from the Internet data, and then it has some increasing knowledge of the world. And that's because, um, if the model can really predict the next token, um, really well, It means that it has some understanding of different domains and different areas and, um, some implicit knowledge. Imagine, for instance, we have, uh, we, uh, give LRM something like I like machine learning because the next token and also the, uh, future tokens that it would, uh, keep generating is probably very relevant to machine learning. So it would probably output some terms that are related to machine learning and, um, to why people like machine learning. And all of these are statistically very close to what is available on data. So that's the outcome of the training. Now remember, these models can be very difficult to train. Even though the process is very straightforward, Because these LLMs can be very huge in terms of number of parameters, it can be, uh, engineering wise, it can be very difficult and a lot of challenges to make sure that we have the right setup to train these models. For example, I asked that what are the resources needed to train an LLM bit, uh, 105,000,000,000 parameters. And remember, this is what, uh, uh, lambda three, uh, biggest version of lambda three has. So what is, uh, what is the memory needed? Right? This is what what? Did why. Um, and then the answer is, um, it requires lots of things. And then here it says um, why memory is needed? It's first because we need to store the model parameters during training. So all those parameters 105,000,000,000 parameters needs to be stored, uh, in memory. And then during the training, again, there's some algorithms that optimizers needs to, uh, keep track of all the intermediate, um, representations and, um, activations and use those to update the weights. So there are a lot of intermediate, um, floating point value numbers that it should store. So because of all of these, and here is a rough estimation, it requires, um, like, you know, very large memory. Assuming that we use a fifty fifty two, meaning that we use four bytes to store each weight. We would end up needing around 1.6 terabytes of memory. So definitely, it's not possible to train this entire model on a single GPU. Because the best GPUs we have might have around, uh, hundreds of gigabytes of memory. So it's not practical to really train the speed model on a single GPU or in on a single machine. And, again, in its calculation, it's saying that you need at least 880 to 100 GPUs to fit the model in memory. And in practice, it requires more. Realistically, it says you use 2,000 GPUs or more. And these numbers are accurate, um, based on what happens in practice. Also here, in terms of storage, we need huge storage. We train the model to just keep saving, uh, the checkpoints of checkpointing the new models as we train them. So each time, each checkpoint might exceed two to five terabytes of um, the storage needed. So this is basically saying that it can be very challenging to train uh, very large models, very large. It also requires lots of distributed training as well. Um, and then let's go to the LAMA three daker. This is the the technical report of LAMA three published by, uh, Meta. So And what I want to show here is that if you scroll down, there is, um, by the way, this is a great technical report. It's very, um, very useful to read and, um, learn. I personally learned a lot of things from it. It. So but what I want to show here is that, uh,

You
22:03:58

Alright. So what I want to show here is that, um, if we scroll down, is this pretraining stage, which we discussed in the last couple of lectures. Then they're talking about pretraining data, and, uh, they manually apply this cleaning and different basically here, they're saying that there are a couple of steps. The first step is the create duration and figuring of larger scale training purpose. This is we already discussed about this. Then they talk about mother large and finally, the model training. So training data, they started something. They apply some data cleaning mechanisms, and here they explain more um, safety filtering and, um, text extraction and cleaning from HTML content as well. And data protection, these are all these, how why they are important and how it works. And then after the data preparation, they talk about, um, model architecture. So they have started with Nana three, uses a standard

ByteByte
22:03:59

