ByteByte
20:49:34

And how elements are built, trained, Alright. Hello, everyone. Welcome to week one. In this week, we'll learn about LMM foundations. And learning this topic is quite important because a lot of advanced applications and use cases these days for example, agents, presenting models, writing code platforms, productivity tools, and many many other examples are all on top of LMS. So our entire focus of this week is to understand how LLMs are built, trained, and work in practice. And in particular, we'll start with an overview, then we we'll talk about the main two stages of training elements, pretraining, and post training. And then for each, we'll discuss how they can be performed and completed. And then finally, we have one, which is a hands on exercise to build an LML playground in Python. So let's grab a coffee and start. So what is an LLM? An LMM is simply an AI model that can understand and generate text. Now if this AI model is trained correctly, it can be very useful. Because we can ask real questions from the element and it can answer to our to our questions. And then we can also ask follow-up questions and the LLM can respond to our follow-up questions. So it's going to engage in a conversation, and it's going to be very useful. Now there are a lot of different companies that are, um, having their own LLMs, and they are, um, providing a chatbot service powered by those LMS, And one example popular example is Chai GPT by OpenAI. And I believe that was the first, um, chatbot that made available to public. But following that, there were many other companies having the and and offering their own, uh, chatbot services. For example, we saw Claude from Entropic. We had Gemini from Google. Grock from XAI, Meta AI from Meta. And I'm sure there are many other examples and many other companies that are offering chatbot services these days. Now all these services are offering very something very similar. There is a UI where you can enter your text and ask a question. And then the LLM would start answering your question. And there are some additional features that some of these chatbots are offering. And, um, some chatbots offer more features. Than others. But just to get an idea, I have some tabs open here uh, to go to some of these popular chatbot services and how how and see how they look in practice. So first, we'll go to, um, chat g pitting. Here's the Chargebee thing UI. And as you can see, it's a very simple UI. There is a box here where it says ask anything. And here is basically we can ask our question. Um, here, it says, Chargebee p five. If you click here, you'll see a bunch of options. It has auto instant thinking pro. And there is this legacy model, c p t four o. And before the introduction of g p t five, it was a lot more confusing here. There were a lot, um, lots of different options, including nontinking and thinking models. For example, we had four o, we had o one, we had o three, and a bunch of other models. So it's a lot more, um, clear now. For now, we'll just skip to their stick to their default model, which is, um, auto. So it decides which of these to use. But in future weeks, we'll cover other, um, options Like, we'll go over we have one full week covering thinking models, and we also talk about same compute and, uh, pro models. But for now, we'll just keep it the the auto one. And here, basically, we can, um, there are just a bunch of tools we can use. It has, um, agent mode, which searches the Internet and, um, explores lots of different resources on It has deep research. It has, um, a tool to create image. Connectors, and, uh, many more tools like a study and then a web search and so on. Again, in future weeks, we are covering many of these We have a week about agents. We have another week about, um, deep research and, um, multi multi agent systems and also image generation models. For now, we are not gonna enable any of these tools. And what I want to do is I want to enter a very

You
20:49:36

And how elements are built to train. Alright. Hello, everyone. Welcome to week one. In this week, we'll learn about LLM foundations. And learning this topic is quite important because a lot of advanced applications and use cases these days for example, agents, reasoning models, writing code platforms, productivity tools, and many many other examples are all fit on top of elements. So our entire focus of this week is to understand how elements are built trained, and work in practice. And in particular, we'll start with an overview, then we'll talk about the main two stages of training elements, pretraining and post training. And then for each, we'll discuss how they can be performed and completed. And then finally, we have project one. Which is a hands on exercise to build an LLM playground in Python. So let's grab a coffee and start. So what is an LLM? An LLM is simply an AI model that can understand and generate text. Now if this AI model is trained correctly, it can be very useful. Because it can ask real questions from the element and it can answer to us to our questions. And then we can also ask follow-up questions, and the LLM can respond to our follow-up questions. So it's going to engage in a conversation, and it's going to be very useful. Now there are a lot of different companies that are, um, having their own LLMs, and they are, um, providing a chatbot service powered by those LLMs. And one example popular example is chatGPT, by OpenAI, and I believe that was the first, um, chatbot that made available to public. But following that, there were many other companies having their own LLMs and and offering their own chatbot services. For example, we saw Claude from Entropic. We had Gemini from Google, Grag from XAI, Meta AI from Meta. And I'm sure there are many other examples and many other companies that are offering chatbot services these days. Now all these services are offering great something very similar. There is a UI where you can enter your text and ask a question, and then the LLM would start answering your question. And there's some additional features that some of these chatbots are offering, and, um, some chatbots offer more features than others. But just to get an idea, I have some tabs open here. Uh, to go to some of these popular chatbot services and how how and see how they look in practice. So first, we'll go to um, chat GPT. Here's the chat GPT UI, and as you can see, it's a very simple UI. There is a box here where it says ask anything. And here is basically we can ask our question. Um, here, it says chat g b t five. If you click here, we'll see a bunch of options. It has auto instant thinking pro, and there is this legacy model, g p t four o. And before the introduction of GPT five, it was a lot more confusing here. There were, uh, lots of different options, including nong thinking and thinking models. Example, we had four o, we had o one, we had o three, and a bunch of other models. So it's a lot more, um, clear now. For now, we'll just skip to their skip to their default model, which is photo. So it decides which of these to use. But in future, Vix will cover other, um, options Like, we'll go over we have one full week covering thinking models, and we also talk about system compute and, uh, pro models. But for now, we'll just keep it the the auto one. And here, basically, we can, um, there are just a bunch of tools we can use. It has um, agent mode, which searches the Internet and, um, explores lots of different resources and so on. It has deep research, has, um, a tool to create image, connectors, and, uh, many more tools like a study and learn, web search, and so on. Again, in future weeks, we are covering many of these We have a week about agents. We have another week about, uh, deep research and, uh, um, multi multi agent systems and also image generation models. For now, we are not gonna enable any of these tools. And what I want to do is I want to enter a very I

ByteByte
20:51:05

prompt here. So we'll see how it responds. There is Paris.

You
20:51:05

Intel Frontier. So we'll see how it respond There is Paris.

You
20:51:14

So when I type various, uh, and enter, it immediately starts to, um, generate in text.

ByteByte
20:51:19

So when I type there is Paris and enter, it immediately starts to, um, generate a text. This is the text. This is France. It's located in the North Central part of the country and so on.

ByteByte
20:51:33

Um, so this is Tatchitiki and, um, G D 5. Next, I'm going to switch to card, and my purpose is to enter the exact prompt and see how card would respond.

You
20:51:33

Um, so this is TackTracker and, um, GPT five. Cloud and see how Cloud will this one.

You
20:51:52

Example, here is basically where we can choose the which model to use. There are different models. Some are more advanced than pro. We have to pay to use those. And some are, uh, their default model. So, again, we'll just stick to their default model, and there are some tools here. Um, and then we'll I'm going to write the exact same person here, various size. And then it.

You
20:52:21

So it also started responding, and it says Paris is the capital city of France. Located in the North Central part of the country. And as you can see, this answer is different. It it's different in terms of, uh, tone. Structure, and style from what you saw in the opening in front of you. And, also, it's a little bit more detailed. Like I say, it's providing coordinates, uh, like, latitude, um, between the song. Now let's go to Gemini and ask the exact same question here. Again, here we can choose which model we want to use, and there's a bunch of tools and, uh, like, deep research. Um, getting their answers and so on. But they're only going to enter the same prompt. There's size.

ByteByte
20:52:23

For example, here is basically there we can choose the which model to use. There are different models. Some are more advanced than Chrome. We have to pay for to use those. And some are, uh, their default model. So, again, we'll just stick their default model under some tools here. Um, and then we'll I'm going to write the exact same question here. There is is. And then enter. So it also started responding, and it says Paris is the capital city of France. Located in the north and south part of the country. And as you can see, this answer is different. It it's different in terms of, uh, tone. Structure, and it started from Patriceli in the open in from JPG. And also it's a little bit detailed. Like, you can see it's providing coordinates, um, like, latitude and so on. Now let's go to Gemini and ask the exact same question here. Again, here we can choose which model we want to use, and there's a bunch of tools and, um, like, deep research um, get in-depth answers and so on. But we're only going to enter the same prompt. There's ice.

You
20:55:05

ICE is the capital and most populous city of France. Again, this answer is not exactly as what we saw earlier by other two models, but this answer is also correct, and it seems accurate. It also provide important needs. And same as story with other type of, like, and. I'm not going to enter the same prompt there. Just to save more time. But, um, it's going to be very similar. Grok You can choose the you see the tools are here, and you can choose the models here. And similarly for Metairie, um, we can use certain, um, tools and ask Metawy any question we have. Now all of these models, what they have in common is they were able to respond to my simple prompt, However, they responded differently, and some of them are in more detail and some are more have different structure and so on. So this is how elements are different. Now if I start asking more complex questions, some of these models may no longer be able to answer my, uh, question by And this is why certain evidence are typically more powerful for analytics. And it keeps changing because all these companies keep training new elements and more powerful elements. So the quality of their elements keeps changing and improving over time. And again, later, you would see how the ranking works and how can they compare various elements. But for now, I just wanted to show various elements and how they work in practice. Now the question is how these elements are built? And this is the next topic. So elements are built typically in two main stages. The stage one is pretraining, and the stage two is post training. And we're going to talk about both in very detail in in future lectures. But for now, it's at the very high level, pretraining is basically the first stage of training on LMM. And then in this stage, we train a model using some training algorithm on the Internet data. This stage is very expensive. It requires lots of compute, uh, thousands of GPUs. Chip GPUs are these hardware, um, basically compute powers that, um, allows us to train evidence. And they can be very expensive. So it's also a very lengthy process. It requires month of training. To train an LMM. And after pretraining, the stage is complete. The outcome of the pretraining is a base model. Which has a very good understanding of the Internet data. And in other words, it has implicit knowledge of the word. Because assuming that there are lots of, um, tips on the Internet about all these different domains and different, um, areas, So and since the model is exposed to all those data, it has a very good understanding of the word, and a very good implicit knowledge. Now the second stage, which is post training, is basically you continue training this base model on a different data. We call it post training data. Until we get the final model. And this is a lot less expensive. It requires less GPUs. Typically hundreds of GPUs or even less, and then it requires days of training only. And in terms of cost, this is just it is less expensive than pretraining. Pretraining typically cost hundreds of millions of dollars, but post training is a lot cheaper. So only well funded startups and very big companies can uh, perform and do pretraining just because it costs a lot. It costs millions of dollars to complete pretraining. Training. There is this interesting report from Stanford, which, uh, provides lots of statistics of um, each companies or each universities have their own LLMs and how much it costs. And they have this open here.

You
20:55:25

There are lots of different statistics. It's it's an interesting brief. Uh, like, how many models are open and there's no access to the public and so on. And then the cost some of these models and which, uh, the organizations that build these models, you can see all these companies are very big companies that can afford to train elephants. And then, um, here it shows the most of the evidence are coming from industry.

You
20:56:12

And this is also the the figure that I wanted to show. Here we have various elements and, uh, language models. Here, we can see the number of parameters, but what I want to focus on is this side, which is the cost the training cost. Basically, how much it cost to complete training of an element. So some of these models might be familiar to you. For example, here we have, uh, GPT three model. 150 175,000,000,000 parameters, and this model was trained by OpenAir. And you can see it the cost of training this model was less than a little bit less than $10,000,000, but it's still, you know, um, very expensive to train such model. You can see more models here and then g p t four and g p t four and g p t four. And if you see these models, they're, um, of training, those models are around hundreds of meetings of times, which is very expensive. And there are some other interesting statistics, but I just wanted to focus on the how costly it can be to train these algorithms.

You
20:56:51

Evidence. So back to our lecture. After we complete these two stages of training, uh, pre training, and post training, this final model is what companies typically use and deploy to, um, power their chat service. So for example, if you ask any final any question from this final model, it would answer. So here's an example. If I ask ChachiPT, this is a real example. I I ask ChagPT, um, they they prompt, like, tell me a short joke. And then the output was why was the math books at? Too many problems. So, um, just wanted to show this example as a as a way that um, this final model is what most companies use to, um, deploy and use it as a way to answer user's questions. Now this is all I wanted to talk about in this lecture. In the next lecture, we'll talk more in detail about pretraining and understand different steps of pretraining.

ByteByte
21:06:07

ICE is the capital and most populous city of France. Again, this answer is not exactly as what we saw earlier by other two models. But this answer is also correct, and it seems accurate. It also provides coordinates. And same as story with other chatbots like Grok and NetAI. I'm not going to enter the same prompt there. Just to save more time. But, um, it's going to be very similar. Grok, you can choose the see the tools are here, and you can choose the models here. And similarly for MetEye, um, we can use certain tools and ask Metairie any question we have. Now all these models, what they have in common is they are able to respond to my simple prompt. However, they responded differently. And some of them are in more detail, and some are more, uh, different structure and tone. So this is how elements are different. Um, now if I start asking more complex questions, some of these content may no longer be able to answer my, uh, question correctly. And this is why certain elements are typically more powerful than others. And it keeps changing because all these companies keep training new elements and more powerful elements. So the the quality of their elements keeps changing and improving over time. And, again, later, we will see how the ranking boards and how can they compare back as LLMs. But for now, I just wanted to show various elements and how they work in practice. Now the question is how these elements are built? And this is the next topic. So elements are built typically in two main stages. The stage one is pre training, and the stage two is post training. And we're going to talk about both in very detail in in future lectures. But for now, it's at the very high level, pretraining is basically the first stage of training the one element. And then in this stage, we train a model using some training algorithm on the Internet data. So this stage is very expensive. It requires lots of compute, um, thousands of GPUs. GPUs are these hardware, um, basically compute powers that, um, allows us to train elements. And they can be very expensive. So it's also a very lengthy process. It requires month of training to train on LMM. And after pre training stage is complete, the outcome of the is a base model. Which has a very good understanding of the Internet data. And in other words, it has implicit knowledge of the word. Because assuming that there are lots of, um, on the Internet about all these different domains and different, um, areas. So and since the model is exposed to all those data, it has a very good understanding of the word and a very good implicit knowledge. Now the second stage, which is post training, is basically we continue training this base model. On a different data. We call it post training data until we get the final model. And this is state that it's a lot less expensive. It requires less GPUs. Typically hundreds of GPUs or even less, and then it requires days of training only. And in terms of cost, usage is less expensive than pretraining. Typically costs hundreds of millions of dollars, but post training is a lot cheaper. So only well funded startups and very big companies can uh, perform and do pre training just because it a lot. It costs millions of dollars to conduct pretraining. There is this interesting report from Stanford, which um, provides us all of the statistics of, um, which companies or which universities have their own elements and how much it costs. And I have this open here. There are lots of different statistics. It's it's an interesting read. Uh, like how many models are open and uh, there's no access to the public and so on. And then the cost of some of these models and which which, uh, the organizations that build these models, you can see all of these companies are very big companies that can afford to train evidence. And then, um, here it shows the most of the evidence are coming from industry. And this is also the the figure that I wanted to show. Here we have various elements, and um, language models. Here, we can see the number of parameters, but what I want to focus on is this side, which is a cost, the training pass. Basically, how much it cost to complete training of an element. Some of these models might be familiar too. For example, here we have, uh, GPT three model. 150 175,000,000,000 parameters, and this model was trained by ONNI. And you can see it the cost of training this model was less than less than $10,000,000, but it's still, you know, um, very expensive to train such model. And you can see more models here and then DPT four and Gemini Ultra. And if you see these models, they are, um, the custom training, those models are around hundreds of millions of dollars, which is very expensive. And there are some other interesting statistics, but I just wanted to focus on the half asleep and to train these elements. So back to our lecture. After we complete these two stages of training, pre training and post training, this final model is what companies typically use and deploy to, um, to power the chatbot service. So for example, if you ask any final any question from this final model, it would answer. So here's an example. I asked Chagibati. This is a real example. I asked Chagibati, um, with a prompt like, tell me a show joke. And then the output was why was the math books at? Too many problems. So, um, just wanted to show this example as a as a way that, um, this final model is what most companies use to, um, deploy and use it as a way to answer user's questions. Now this is all I wanted to talk about in this lecture. In the next lecture, we'll talk more in detail about pretraining and understand different steps of pretraining. Hey. In the last lecture, we saw various chatbots and elements. We discussed the two main stages of training elements. The stage one is pre training and the stage two is post training. In this lecture, we are going to focus on the pre training stage. And in particular, we are going to talk about the Internet data. Does it mean to train something on Internet data? How should we collect this data, and how to prepare this data? So let's start. So data preparation has three main steps. Step one is crawling the Internet. So crawling is simply a software that starts from some CDRs or just a single base URL. And then it tries to extract its content, and I didn't identify the ad coin links within that URL, and then it goes to do that coin links. And there is a loop where it keeps doing this. It keeps downloading the content, identifying the ad coin links various URLs, and then visiting those URLs. It keeps doing this until it visits majority of the Internet. So just to better understand how web crawling works, it's I have asked to write a simple web crawler class in Python. And I also inspected your code. It's it seems alright. And before talking about the code, the purpose here is not to have a production ready web crawler software. Purpose is just to understand the logics and understand what are the key pieces in a web crawler. Because once we learn that, most of the advanced web crawlers are all built based on the same logic. So, again, any web crawler is just a software, and here we have class, uh, same type of web crawler. And what it does is it starts from a base URL, and this is given as the input to this class. And then it also keeps a a a set, a Python set. Nine time name visited. And this visited is basically keeps track of URLs that this web crawler already discovered, extracted their content, and it's done. Um, and then we have this method called crawl, and this is the main crawl, the main function the main functionality of this class. What it does is it starts from this two visit, which is basically a Python list and it adds the base URL as the first URL to start exploring it. But then this this list is basically a way to allow this software to keep adding new links as it discovers them so it can later explore. And again, this whole thing is based on a for loop or while loop. And then in the while loop here, we have as long as there are links, unexplored links, in the tool visit, meaning that there are URLs that we have not we have not explored this content yet. Um, we extract the URL. If it's previously seen or explored, we skip it. Otherwise, we restart the crawling. And here, are some libraries like requests. Again, we don't want the focus is not to learn the details of these libraries. Are just lots of different libraries we can requests to different URLs and get the output. And here we are using this request dot get send the URL to get the response back. And then from this response, there is this text. Which is the HTML content of that URL. Now again, Beautiful Soup is just one library and there are other libraries, but all these libraries are allowing us to once we have the responses, the HTML content, it allows us here to just walk uh, to go through all the URL links within that HTML content. So here basically, it says find all, uh, these tags. And then those are links. And then if it builds a full link, full URL, because some of these might be like, relative URLs. And then once we have that, it just happens into the two visit. So basically, two visit is just a list that it keeps track of all the discovered URLs that you have not explored its content, HTML content. And then there's a while loop that it continuously tries to, uh, explore unseen, uh, URLs in the to visit. And once we are done, it just it just gets added to the visit head. So very simple. So this is the URL. Again, you practice there more details or more arguments uh, like, a maximum number of times to, um, to go deep and keep identifying links and going to those links. But those are details. The purpose is just to, for now, understand the high level logic of building a web crawler. Now back to the lecture. There are two ways now we can crawl the Internet. The first option is to crawl ourselves. And this is what most big companies like Okta actually does. For example, just to validate that entropy does crawl, there is this question on on the entropy website that whether entropy crawl data from the web And the short answer, if you read this, it says, yes. We do crawl data. And this is also g p t two paper published by OpenAI. And we're going to talk, uh, in detail about different parts of this GPT two model later in this week. But for now, I just wanna scroll down and get to the training data. Here, train dataset. And if you read this here, they're saying that instead we created a new data set which analyzes the document quality. So basically, they are also having their own web crawler. So they have this software. They written that, and then they're using that to crawl the web. So this is first option. The second option is to use a public repo repository of cloud pages. Now there are different research teams and, um, organizations that regularly crawl the Internet and provide the extracted content public and make it publicly available. And one very common example of common crawl. So I Google CommonCall here and it says CommonCall is a nonprofit organization that crawls data and free provides its archives and data sets to to the public. And there's just this website for Comiccrawl. If you go there, um, it's it's how the website look like. We can go to the data. There's this latest call and, you know, instructions to get us started. And here we can choose a call. And it seems that the calls are hosted on the screen. So here, we can just choose call, options because they regularly call under different, um, times. So it's 24 crawls, 23, 22, and it goes all the way back to, I think, when since they started 2008. So any of these, once we select, you would just give us some instructions and more details about the call. Now back to the picture. Most companies prefer to write their own crawling if they are big, so it gives them more flexibility. And for fast experiments or startups or smaller companies, they prefer to start from, uh, publicly available crop pages just because it allows them to iterate faster. And these are some interesting statistics from coming across, uh, the crawling devices 2007. And then each time the crawd is approximately 2,700,000,000 web pages. And as a result of that, they're around, uh, it's around 200 to 400 terabytes of HTML text content. Each time that they crawl the Internet. And then they release a new crawl every month or, uh, two months. Alright. So this is the step one of data preparation, calling the Internet. Or in other words, it's collecting data. Now after we collect this data, it's about that that is HTML text content. Now the second step, which is also very important, is data cleaning. The raw HTML content has lots of issues. Example, here I have a screenshot. If I zoom in to see how it's running on the right, can see the HTML text content has lots of, um, irrelevant, um, information. For example, there are lots of tags, HTML, uh, tags, and markdowns and attributes, and um, many of these are not really useful to to train a model. Um, we don't at least for most use cases, we don't want an element to learn about all these, uh, tags and head HTML, all these things. What we want is the element to learn from the actual content that we see, uh, from yours. The content that we see in a browser. And those are often, um, within certain acts like h one, tags, and here we have the, you know, ping tags. Like, text, for example, the new MacBook Pro, m four features, the latest Apple silicon. So These are some of the text data that we really want to extract from this, uh, and use them for element training. So that is the the main goal of data cleaning. It's to extract useful information from the raw HTML content. But that's not the only, um, step to query the data. There are also other issues in the raw HTML content. For example, a lot of text is, uh, duplicated across the Internet. You can think of when there is an important news, um, there are lots of various websites that publish the exact same news or with a slight rebooting of the, uh, same news or original article. And we typically don't want to have lots of duplicated data in our training data. Because if we expose the LLM to, uh, duplicated data and LLM see them over and over again, at some point, it may memorize them. So we don't really want the LLM to memorize, uh, news. What we want is to learn about different words, different, um, topics and knowledges. So these are some of the things that are important to take care of during the data clinic. Again, there are other other things. For example, one important thing is to make sure that we use only useful and safe content for the sub websites that we may not want our editor to learn from. As part of data cleaning, it's important to just get rid of all those websites and those text content. Content. Now there are different companies and that they've tried various phase of cleaning um, the raw HTML content or coming across. And then after they applied those cleaning features or steps, then they get the clean text, and then they name it something. 