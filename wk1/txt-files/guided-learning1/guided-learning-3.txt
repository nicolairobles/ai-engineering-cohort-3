And training phase is basically they're different algorithms. For the training phase. And all these algorithms most mostly differ in their text splitting logic. Because building vocabulary is is very obvious. It just needs to, uh, find all the unique tokens. Next, we are going to talk about the important categories of, uh, tokenizers. Which are mostly, uh, referring to the algorithm that we can apply here. And create a vocabulary. So all these different algorithms, they can be categorized in, um, into three big categories. We have word level organizers, We have character level organizers. And we have subword level organizers. And again, for each of these, there might be various, uh, algorithms within each category. Let's just briefly talk about each. Before I explain the in detail, I just wanna point out that word data organizer and character data organizers are no longer used in building advanced algorithms, at least to the best of my knowledge. And most of recent advanced elements are all relying on a certain algorithm within the software level organization. And the reason for that is because word level and character level has some limitations, which we will talk in a bit. So let's start with word level organizers. It's very simple. As the name indicates, it scoots the text based on its, let's say, uh, white spaces. To get the actual words. So the text is speaking logic is, for instance, split the text based on its white spaces. So this sentence or any put like, perfectly fine, then it goes to whatever organization. And word error organization applies the excess splitting logic, it will end up perfectly and fine. And then using its internal vocabulary, it would just replace these tokens with their corresponding ID. So these algorithms, when we train on our Internet data, we would internally, it would apply this, uh, excess meeting, and then it would build the vocabulary. And since it's word level, the vocabulary can be very large. And the reason for that is because there are lots of unique words on the available on the Internet, both in English and also in other languages. If we include any other training data. So, um, can see here we have perfectly, and then its ID is 52000 something. And in practice, it can be hundreds of thousands of unicorns if we run word embeddable organizer on the Internet. So this is Wordablet organizer. Now let's switch to Art level organizer. Again, as the name suggests, is split the text based on their characters. So for example, and you put that perfectly fine when it goes into hard level tokenizer. It becomes something like this, p e r f e c t, and so on. And then each token gets replaced by their ID. And you can see, the ID here are very small. And this is because in character recognizer, when we apply it, when we when we train it on the Internet data, we don't usually have, um, that many characters. Uh, for example, if it's trained on English, only data would end up perhaps with lowercase, upper case, um, characters and some punctuations and, you know, things like like that and question mark and so on. So the vocabulary length would not be the vocabulary data is not going to be huge. It's in fact going to be very small, and it would have, uh, very little number of, uh, tokens. And that's why you see most of these numbers are small here. Now this is Cartier Before I talk about software development, I want to, um, talk about the limitations. As we saw earlier, when we run board level tokenizer and train it, we will end up with a huge vocabulary. And that is the new version of word recognizer.

ByteByte
21:21:09

When the vocabulary is huge, it means that you have to maintain and manage lots of different tokens And this can be very expensive when we train a model because during the training, we have to uh, keep this table and learn some association, uh, with each token or or in fact its IDs. So it's going to be very expensive to learn when our, um, vocabulary is very large. Now, curriculum organizer, its limitation is exactly opposite of that. Um, it does not really suffer from having a huge vocabulary because the vocabulary is small. But what it suffers from is the long sequence of numbers. So and the reason for that is because of its, um, the splitting logic. You can see here it would end up after we convert perfectly to a sequence of numbers, we would have too many numbers here. And this is not also ideal, and this is also can be expensive because now the during the training, the model need to learn, um, the dependency and um, relation between all these tokens. And if the sequence is too long, it's only to be costly. So these are the limitations, and this is why in the first place, software that recognizers were intended and prefer. So now let's switch to subawarded organizers. So key intuition behind sublevel organization is that they come up with some speaking logic And after we apply that splitting logic to the text, it split the text into smaller units. Such that each unit is larger than characters. Also they are smaller than words. So this way, it tries to somehow balance and 600 between word level organizers and character level organizers. For example, for an input, like, perfectly fine. After it goes to a support level organizers, it can become something like perfectly and fine. And then each token would get replaced by their IDs. So as you can see, the uh, the word the tokens are smaller than word level token For example, we don't have perfectly as a token. We have perfect andy. So the word perfectly would be split that into smaller units. Now there are different algorithms that are under software data organizers. And one very popular algorithm is byte current coding or BPE. And that is what a lot of LLMs have been using to as a organizer. And I'm going to briefly go over it so we get an intuition and high level understanding of how an algorithm like works. A key idea behind the algorithm is that it starts with characters. And it tries to iteratively merge frequent pairs of tokens and create new tokens. This way, it has full control over the vocabulary size. Let's see a quick example. Um, I I searched online and I found this visualization. Seems to be in Korean. I don't understand Korean, but what I want to show is this diagram. And, basically, the way the algorithm tries to form tokens and create its vocabulary is it starts from the training data. It counts all the unique words. And then it gets to the characters. And then for each character, it creates a new token. So for example, Edge has a new token, u has a new and then it just sort those in the vocabulary. And then from here and this 10 is the frequency. Of, uh, basically number of times that this word was appear in the training data. And then from here, it tries to iteratively merge these tokens. And at this level, it tries to iteratively merge them and then create new tokens. And the way that it it merges two tokens or picks tokens to merge is based on their frequency. So it tries to find the most frequent pair and then just merge them. For example, after here, we have, um, it creates a new token named UG, and that simply a lot of UGs in the

ByteByte
21:21:55

training data. If if you see here, we have 10 times. We have also u g here, five times. And we have UG here five times. And because of that, now it creates a new token named UG because it believes that u g can be a popular, um, token in in the training data. And it can it might have some meanings associated to it, and it creates u g. And then it continues doing that until it reaches a certain number of tokens. In the vocabulary. So it has control over the vocabulary size. Sometimes they keep doing this until the vocabulary reaches for instance, uh, 50,000, uh, vocabularies. So this is byte error encoding, and there is a very interesting, um, tutorial published by Hugging Face, byte pair encoding organization. And then here it explains in detail the training algorithm and how it starts forming the tokens and merging and so on. So it can be an interesting read. And then, um, as I mentioned, to the best of my knowledge, all of Amazon seller names these days are based on some variations of

You
21:21:55

So in organizations, we are dealing with two phases. The first phase is the training phase, then the second phase, which you have it here, is the inference phase. In the training phase, the idea is to expose the tokenizer to this long sequence of text that we've already cleaned, and then be let it learn from it. And understand the statistics of different words and what are the unique words. The purpose of the training phase is to, um, apply these two um, squares that we have here. These are just two, um, algorithms or or or logics. And then as a result of that, the tokenizer would end up with the vocabulary. So let's just talk about each of these in detail. The first step the first part is text splitting. So what happens here is that once we this long sequence of is provided, this applies some logic. It's it's simply a logic applied to this long sequence of text to speed. Basically, the the goal here is to split the into smaller units. And depending what on our logic, it can be, um, implemented in different ways. For example, here, we have an example, uh, machine learning ML is a subject of artificial intelligence. And then after applying text and splitting, we may end up with something like this. It's a list of smaller chunks or units of text. For example, we have machine learning and then we have, like, smaller units and sub build artificial, and and finally, we have that here. So this is Texas specific. Now imagine they apply this logic on the entire clean data data, to get a very huge list of, um, smaller units of text. The next part is building the vocabulary. Now given this huge list, in this step, we create a vocabulary of all the unique units of text. That we have in this list. And then we also assign an ID. So it's it's very simple. Building vocabulary is nothing but finding all the unique, um, units and refer to this, uh, each of these smaller units of text as tokens. So the building vocabulary is simply responsible for for finding all the unit tokens and just listing them in a table and also assigning an ID token. Example, we can have a vocabulary like this. And then for tokens, we have pay, about, after all, also, and so on. And these are some of the tokens that we've seen in this list. After we escalated the the initial the original text. And then these are the IDs, which starts from zero, and they just increment it, uh, one by one. And here, it can be seen that at the end, we have around 270,000, uh, 131 uh, tokens. And depending on our training data, this long sequence of text, it may end up with different vocabularies. It's different than it just depends on how many unique, um, tokens you would end up seeing after we split the next step. So this is the training phase. This is we just run this one time on our training data to build this vocabulary. And then from there, the tokenizer is ready to be used. Basically, the tokenizer, it's just internally, the storage is vocabulary. And then anytime they pass a text to it, can convert it to a sequence of numbers. And the way that it does is, first, it runs this, uh, excess meeting logic It applies it to tell me a job, and then it splits tell me a job to, uh, a sequence of tokens. And then it would refer to its internal vocabulary and replace each token, which is corresponding ID. For example, tell me a joke might become tell me a joke after the Texas meeting. And then after tokens are replaced by their IDs, we may end up with something like this. So this is how a tokenizer can go from any text to a sequence of numbers. In addition to that, it can also go back to an original text given a sequence of numbers, and this is also a very simple logic. So let me pass sequence of numbers like this to the tokenizer, internally, the tokenizer can also store an inverse table. So instead of mapping from tokens to IDs, it can have a mapping from IDs to tokens. Once this is provided to the tokenizer and we want to detourcanize it, meaning that you want to go from numbers to text, it just simply goes to those, uh, rows in the in the inverse data, and find the corresponding token. And then it it applies the inverse of text as to the tokens to output the original text, in this case, So these are the two main phases of, um, tokenization. And training phase is basically they're different. For the training phase, and all these algorithms mostly differ in their text splitting logic. Because building vocabulary is is very obvious. It just needs to, uh, find all the unique tokens. Next, we are going to talk about the important categories of, uh, tokenizers. Which are mostly, uh, referring to the algorithm that we can apply here. And create the vocabulary. So all these different algorithms, they can be categorized in uh, into three big categories. We have word level tokenizers. We have character level tokenizers. And you have software level tokenizers. And, again, for each of these, there might be various algorithms in any category. Let's just briefly talk about each. Before I explain the in detail, uh, I just wanna point out that word event organizer and target event organizers are no longer used in building type of elements, at least to the best of my knowledge. And most of recent advanced elements are all relying on a certain algorithm, within the software level organization. And the reason for that is because word level and character level has some limitations, which we would talk in a bit. So let's assert a word level organizer. It's very simple. As the name indicates, it splits the text based on its, let's say, um, white spaces to get the actual words. So the text is splitting logic is, for instance, split the text based on its white spaces. So the sentence or any put, like, perfectly fine, and it goes to whatever organization. And whatever organization applies the taxes within budget, it would end up with perfectly and fine. And then using its internal vocabulary, it would just replace these tokens with their corresponding ID. So these algorithms then be trained on our Internet data, it would internally, it would apply this, um, excess bleeding, and then it would build a vocabulary. And since it's wordable, the vocabulary can be very large. And the reason for that is because there are lots of unique words on the available on the Internet, both in English and also in other language if you include them in our training data. So, um, can see here we have perfectly, and then its ID is 52000 something. In practice, it can be, uh, hundreds of thousands of unicorns. If we run WordNet organizer on an Internet. So this is Word development organizer. Now let's switch to part level organizer. Again, as the name suggests, it's it splits the text based on the characters. So for example, and it would, like, perfectly fine, and it goes into character recognition. It becomes something like this, p e r f e c t, and so on. And then each token gets replaced by your ID. And you can see, the IDs here are very small, and this is because in private organizer, then we apply when we when we train it on the Internet data, we don't usually have, um, that many characters. For example, if it's trained on English only data, it will end up it has to lowercase, upper case, um, characters, and some punctuations and, you know, things like dots and question marks and so on. So the vocabulary length would not be the vocabulary, general, is not going to be huge. Fact going to be very small, and they would have, uh, very little number of, uh, tokens. That's why you see most of these numbers are small. Now this is our current level before I talk about software development, tokenization. I want to, um, talk about the limitations. As we saw earlier, when we run word error tokenizer and training, it would end up with a huge vocabulary, and that is the limitation of board level When the vocabulary is huge, it means that you have to maintain and manage lots of different tokens this can be very expensive when we train a model because during the training, we have to uh, keep this stable and learn some association, uh, with each token or or, in fact, it's IDs. So it's going to be very expensive to learn when our, um, vocabulary is very large. Now car to the organizer, its limitation is exactly opposite of it. Um, it does not really suffer from having a huge vocabulary because the vocabulary is small. But what it suffers from is a long sequence of numbers. So and the reason for that is because of its, um, the splitting logic. You can see here, it would end up after a 100 perfectly fine to a sequence of numbers. You would have too many numbers And this is not also ideal, and this is also can be expensive because now the during the training, the model need to learn, um, the dependency and the, um, relation between all these tokens. And if the sequence is too long, it's going to be costly. So these are the limitations, and this is why in the first place, software development organizers were intended and preferred. So now let's switch to software developers. So key intuition behind software development organization is that they come up with some splitting logic And after we apply that splitting logic to the text, they split the text into smaller units. Such that each unit is larger than characters, and also they are smaller than words. So this way, it tries to somehow balance and sits somewhere between word level organizers and character level organizers. Example, for an input, like, perfectly fine, after it goes to software level organizers, it can become something like perfectly and fine, and then each token would get replaced by their IDs. So as you can see, the, um, the word the tokens are smaller than word of the token. For example, you don't have perfectly as a token. We have perfect and lee. So the word perfectly would be split into smaller units. Now there are different algorithms that are under software developers. And one very popular algorithm is white pair encoding or BPE. And that is what a lot of elements have been using to as a organizer. And I'm going to briefly go over it so we get an intuition and high level understanding of how an algorithm like PPE works. The key idea behind the algorithm is that that it starts with partners, and it tries to iteratively merge frequent pairs of tokens. And create new tokens. So this way it has full control over the vocabulary size. Let's see a quick example. Um, I I searched online, and I found this visualization. This seems to be Korean. I don't understand Korean, but what I want to show is this diagram. And, basically, the way the algorithm tries to form tokens and create this vocabulary is it starts from the training data, it counts all the unicorns, and then it gets to the characters. And then for each character, it creates a new token. So for example, Catch has a new token, you has a new token, and then it stores those in the vocabulary. And then from here, and this 10 is the, um, frequency of basically, number of times that this word was appeared in the training data. And then from here, it tries to iteratively merge these tokens at this level, it's basically characters. It tries to iteratively merge them and then create new tokens. And the way that it merges two tokens or picks tokens to merge is based on their frequency. So it tries to find the most frequent pair and then just merge them. For example, after here, we have um, it creates a new token named 10 times. We have also UG here five times. We have UG here five times. And because of that, now it creates a new token named UG. Because it believes that u g can be a popular, um, token in in the training data. And it can it might have some meanings associated with it, and then it creates u g. And then it continues doing that until it reaches a certain number of tokens. In the vocabulary. So it has control over the vocabulary size. Sometimes they keep doing this until the vocabulary reaches for instance, um, 50,000, uh, tokens. So this is byte pair encoding, and there is a very interesting, uh, tutorial uh, published by Huddlface, byte pair encoding organization. In here, it explains in detail the training algorithm and how it starts forming the tokens and rating as well. It can be an interesting thing. And then, um, as I mentioned, to the best of my knowledge, all of these days are based on some variations of

ByteByte
21:22:41

some other recognizers, and a lot of them are based on VP algorithm. For example, in, uh, HTTP two pager again. And then, um, again, we're going to talk about different parts of it, like model architecture and so on. But for now, I want to show you the tokenizer that they've been using. So here is the input representation. And here, they're talking about byte frame coding or BDE. And then they're just explaining it how they are using VTE and, um, impact some variations of it as their tokenizer. And also searched LAMA three, which is meta's element, and it's relatively new. And even this paper was using BPO as its tokenizer. Um, here, they're saying that the tokenizer is a BPE model. So, again, most of the recent elements are based on, uh, some variations of BP or some algorithm on your sublevel organizer. Now, um, after just to get a comparison of what the web organizer

You
21:25:45

software tokenizers, and a lot of them are based on DP algorithm. For example, in, uh, PCBT two paper again, And then, um, again, we're going to talk about different parts of it, like, model architecture and so on. But for now, I want to show you the tokenizer that they'll be used. So here is a input representation. Here, they're talking about white pair encoding for VTE. And then they're just explaining that how they're using VPE and, uh, in fact, some variations of it as their tokenizer. And I also searched LAMA three, which is meta's LNM. And it's relatively new. And even this paper was using PP as its tokenizer, Here, they're saying that the tokenizer is a BPE model. So, again, most of the recent evidence are based on, um, some variations of e p or some algorithm on your software. Now, um, after just to get a comparison of what the web tokenizer packet tokenizers and some of the web tokenizers, This can be one possible vocabulary after we apply each of these. For example, if we try if we use some character level organizer, this can be a vocabulary. What I want to show here is that the vocabulary is very small in terms of, uh, number of tokens. It has This is an example, but in this case, we have, uh, 105 tokens. And then you can see the tokens are, like, various like, a small unit of text. Which is short characters, like a d, lowercase, uppercase, punctuations, and space, and so on. Now if we deploy word error recognizers to our training data, we may end up with a bit of vocabulary like this. And as you can see here, we have, uh, a lot more vocabularies. It's 207,000 something. Again, this is an example, but they are closer to reality then you can see the performance or any individual word that was ever seen on the Internet. And the reason that it's very, very, um, it can this vocabulary can be very, very large is because there are a lot of, um, on the Internet, basically, it's very likely to have lots of random words. I mean, for example, if I search Google, something like okay. Apparently, we have some, um, music here. It it is on it. And you can see this name. So since Internet data means all these text data, and then if we want if basically text by white spaces, we wouldn't have all these, uh, smaller units. And then at some point, we would have this, um, this, um, this sequence of factors as a word. So that's why in practice, we would have a vocabulary, which can be very, very large. And also, um, not too efficient because, uh, maybe, um, appeared in some few places, but it's not a meaningful word. So it just doesn't uh, it's not too efficient to have its own token. And then we have software that that vocabulary, which is basically sits in between word development and character development. And you can see focus can be for important frequent words in English. We have them as their own token like the home and so on. And then it tries to also, um, identify the smaller units that are also very frequent, for instance, in English. So that's why we have I n g, e d, Abel, and so on as their own token. So that's for instance, when you have a verb like walking, it can be split into walk and, uh, I n g. So it's going to split basically text into smaller units. So these are just some examples, and here we have around 50,000. But in practice in more recent, um, advanced tokenized software development tokenizers, this vocabulary can be bigger. Like, 100,000 tokens or even 200,000 tokens. But again, word level organizers, especially if you include, um, extra complicated languages, can become very, very, very large. So that's that. And then the next thing I want to show is an interesting, um, visualization of organizers. And there's this website, um, deep organizer. Then I have this open here. So if just a visualization to get a better understanding of how the text look like after we apply tokenization. And what are the IDs and so on. So here, we can choose some, um, popular organizers. So these are basically pre trained organizers on some Internet data. And already has its internal vocabulary. So it's ready to be used for inference. And then these are the names of those tokenizers. So let me choose this one for instance. And here, I can write something like, I love machine learning. And we can see that this has five um, tokens. And then after applying the token, after applying the screening logic, it slowly distakes into I um, space glove, space machine, space learning, and, um, this punctuation. So, um, basically, this is how the algorithm, the BPI came up with. So the reason that we have a space law is because just because after applying the

ByteByte
21:26:33

and sublevel tokenizers, this can be one possible vocabulary after we apply each of these. For example, if we try if we use some character level organizer, this can be the vocabulary. And what I want to show here is that the vocabulary is very small in terms of uh, number of tokens it has. This is an example, but in this case, you have, um, 105 tokens. And then you can see the tokens are, like, various like, a small unit of text, which short characters with a b, lowercase, uppercase, punctuations, and the space and so on. Now if you apply word level organizers to our training data, you may end up with a vocabulary like And as you can see here, we have, uh, a lot more vocabularies. It's, um, 170,000 something. Again, this is an example, but they are closer to reality And then you can see the tokens are any individual word that was ever seen on the Internet. And the reason that it's very, very, um, it can this vocabulary can be very, very large is because very low of, um, on the Internet, basically, it's very likely to have have, um, lots of random words. I mean, for example, if I search Google, something like okay. Apparently, I have some, um, musics here. It it is, uh, You can say this name. So since Internet data means all these text data, and then if we run if we speak text or in white spaces, we would end up with all these, uh, smaller units. And then at some point, we would have this, um, this, um, this sequence of factors as a word. That's why in practice, we would have a vocabulary, which can be very, very large. And also, um, not too efficient because, uh, maybe, um, appeared in some few places, but it's not a meaningful word. So it just doesn't, uh, it's not too efficient to have its own token. Um, and then we have subworded vocabulary, which is basically sits in between word level and character level. And you can see tokens can be for important frequent words in English. We have them as their token, like the of home and so on. And then it tries to also, um, identify the smaller units that are also very frequent, for instance in English. So that's why we have I n g, e d, Abel, and so on as their own token. That's for instance, when you have a verb like walking, it can be splitting to walk and, um, I n g. So it's going to split basically text into smaller units. So these are just some examples and here we have around 50,000. But in practice in more recent, uh, advanced vocabulary, subword data, organizers, this vocabulary can be bigger. Like, 100,000 tokens or even 200,000 tokens. But again, word level tokenizers, especially if you include text from other languages, can become very, very, very large. Um, so that's that. The the next thing I want to show is an interesting, um, visualization of organizers. And there's this website, um, tick tokenizer. And then I have this one here. So this is just a visualization to get a better understanding of how the text look like after we have, like, organization and what are the IDs and so on. So here we can choose some popular tokenizers. So these are basically pre trained tokenizers on some infinite data, and it already has its internal vocabulary. So it's ready to be used for inference. And then these are the names of those organizers. So let me choose this one for instance. And here I can write something like I love machine learning. And we can see that this has five, uh, tokens. Then after applying the token, after applying the screening logic, it specifies this text into I um, space love, space machine, space learning, and, um, this punctuation. So, um, basically, this is how the the algorithm, the BP algorithm, came up with token. So the reason that we have the space love is because just because after applying the algorithm to later and data, this becomes its own token. And then these are the IDs. So, um, in the dictionary, I is 40, love is, uh, 3,021 and so on, and exclamation point is zero. So it's an interesting, uh, visualization. We can just clear and validate. We can remove space ahead and see if it's still a word or not, or we can do things like, um, I love walking and walking is this on token in the it's perfectly fine. Let's try this. It's perfectly fine. So here you can see perfectly is becoming three tokens. And Eloi, as we had in our example, has its own token. So what that means is, um, I think I have a misspelling here. Perfectly. Alright. Now perfectly is, um, its own token. What is interesting, if you try something very random like, um, it is, uh, and something like this, can see a splitting logic splits it into various smaller units. And just that's just simply because we don't have any tokens associated with these words.

You
21:26:35

algorithm to the Internet data, this becomes its own token. And then these are the IDs. So, um, its vocabulary by case 40, love is, um, 3,021 and so on. And exclamation point is zero. So it's interesting. Visualization. We can just play around with it. We can remove spaces and see if it's still a word or not. Or we can do things like, um, I love walking. And working is its own token in this example. It's perfectly fine. Let's try this. It's perfectly fine. So here we can see perfectly is becoming three tokens. LRO, as we had in our example, has its own token. So what that means is, um, I think I have them, missus Kelly. Perfectly. Alright. Now perfectly is, uh, its own token. It is interesting if you try something very random, like, um, it is, uh, and something like this. Can see the splitting logic splits it into very smaller units. And that's just simply because we don't have any tokens associated with these words.
