# Lesson 4 of 22
# Project 1: Build an LLM Playground

## Overview

An introductory project to explore how prompts, tokenization, and decoding settings work in practice, building the foundation for effective use of large language models.

## Learning Objectives

- Tokenization of raw text into discrete tokens
- Basics of GPT-2 and Transformer architectures
- Loading pre-trained LLMs with Hugging Face
- Decoding strategies for text generation
- Completion vs. instruction-tuned models

## Estimated Time Commitment

This project typically takes around 90 minutes, depending on how much time you spend experimenting.

## Instructions

Open the notebook on GitHub using the provided link below. At the top of the notebook, click the Colab badge to run it in Google Colab (no installation required). To run locally instead, clone the repository, open the notebook in Jupyter or VS Code, and follow the instructions to setup your environment. Then follow these steps:

1. Start at the top of the notebook and run every cell in order.
2. When you see a comment such as `YOUR CODE HERE` inside a code cell, add the missing lines where the comment appears.
3. Experiment. Change decoding settings one at a time (temperature, `top_k`, `top_p`), try a few different prompts, and inspect how tokenization affects outputs.

## When You Get Stuck

- Post your question in the Q/A channel and get guidance from the instructor or your peers.
- If you can't resolve the issue, make a note of where you stopped and continue with the next sections. The complete solution will be reviewed during the weekly Project Deep Dive session.

## When You Cannot Complete the Project

- Skim through the notebook to familiarize yourself with the main ideas.
- Join the Project Deep Dive session for a full walkthrough and explanation.
- If you cannot attend live, watch the session recording to catch up at your own pace.

## Start Here

[Full project notebook and instructions](https://github.com/bytebyteai/ai-engineering-cohort-3/tree/main/project_1)
